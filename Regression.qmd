---
title: "Регрессионный анализ в R"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = F, warning = F)
```

## Основы линейной регрессии

На предыдущих занятиях мы изучили основные типы и структуры данных, овладели основами создания простых функций на R, научились импортировать и экспортировать данные в разных форматах. Мы уже умеем проводить различные трансформации данных, визуализировать данные разного типа и проводить одномерный и двумерный анализ на категориальных данных. Мы даже справились со сложной задачей анализа данных с помощью метода главных компонент и анализа соответствий.

Мы продолжаем изучать многомерные методы, и следующий на очереди - регрессионный анализ и его отдельные разновидности.

В обобщенном виде уравнение регрессионного анализа может быть представлено как:

$$Y=X\beta+\epsilon,$$

где $Y$ - вектор значений зависимой переменной, $X$ - матрица значений независимых переменных (предикторов), $\beta$ - вектор коэффициентов регрессии, используемых для подгонки к известным значениям зависимой переменной, $\epsilon$ - ошибки модели (остатки, разница между реальными и предсказанными значениями).

Преимуществами и причинами популярности регрессионного анализа являются следующие:

-   регрессионные модели могут включать множество предикторов одновременно, что позволяет оценивать вклад каждого при условии контроля над остальными параметрами;
-   существует большое количество разновидностей регрессионного анализа, могут использоваться различные типы предикторов и зависимых переменных;
-   довольно легко интерпретировать результаты;
-   достаточно просты в применении и не слишком сложны с математической точки зрения.

Чаще всего используются следующие виды регрессионного анализа:

-   линейная регрессия (зависимая переменная числовая)
-   логистическая регрессия (зависимая переменная бинарная)
-   порядковая регрессия (зависимая переменная - упорядоченная факторная)
-   мультиномиальная регрессия (зависимая переменная категориальная)

### Загрузка данных

В качестве практического примера мы будем использовать учебный набор данных о взаимосвязи между объемами продаж и затратами на рекламу. Это небольшой набор, который поможет понять основные идеи метода и его различные реализации.

Как всегда, мы начинаем с загрузки данных.

{{< iconify arcticons 1dm size=42px >}}[Скачать данные](https://github.com/domelia/rcourse/blob/main/Advertising.csv)

```{r}
library(readr)
Advertising = read_csv("Advertising.csv")
```

После загрузки данных в R, первым делом нужно посмотреть сами данные, их структуру. Поскольку мы использовали функцию `read_csv()`, данные были загружены в формате tibble (tbl_df). Это практически датафрейм, вернее его усовершенствованная версия, в которой данные обрабатываются быстрее, и не происходит некоторых неприятных трансформаций (например, не меняются типы и имена данных).

Мы видим, что у нас всего 200 наблюдений и 4 переменных типа `double` (числовой формат, в котором происходит более точное округление десятичных знаков до 16 знаков после запятой - с 64-битной, то есть двойной точностью).

В нашем наборе переменная **Sales** (Продажи) будет являться зависимой переменной, и мы будем пытаться выявить взаимосвязи между продажами и тремя другими - независимыми переменными: TV, Radio, и Newspaper, обозначающими, соответственно, затраты на рекламу на телевидении, радио и в газетах.

### Предварительная визуализация данных

После рассмотрения структуры данных, следующий шаг - это визуализация. Поскольку у нас только количественные (не категориальные) переменные, лучший способ их представить - это диаграммы рассеяния, которые мы можем сделать для каждого индивидуального предиктора.

Например, для рекламы на телевидении

```{r}
plot(Sales ~ TV, data = Advertising, col = "dodgerblue", pch = 20, cex = 1.5,
     main = "Продажи vs Реклама на телевидении")
```

**Самостоятельная работа**: сделайте аналогичные графики для других переменных.

Чтобы сделать все графики сразу, можно воспользоваться функцией `pairs()`.

```{r}
pairs(Advertising)
```

Часто нам интересно посмотреть взаимосвязи только между зависимой переменной и предиктором, а функция `pairs()` выдает много лишнего.

Функция `featurePlot()` из библиотеки `caret` [(Classification And REgression Training)](https://topepo.github.io/caret/), подходит для этой цели гораздо лучше.

```{r}
library(caret)
featurePlot(x = Advertising[ , c("TV", "Radio", "Newspaper")], y = Advertising$Sales)
```

Мы видим, что есть явный рост продаж по мере увеличения рекламы на радио и телевидении, тогда как связь с рекламой в газетах не так очевидна.

### Простая линейная регрессия и функция lm()

Давайте построим простую линейную модель для продаж, в которой в качестве предиктора будут выступать затраты на телевизионную рекламу.

```{r}
mod_1 = lm(Sales ~ TV, data = Advertising)
```

#### Общие результаты и тестирование гипотез

Функция `summary()` позволяет вывести на экран информацию о модели, полученную с помощью функции `lm()`, которая может быть полезной для тестирования гипотез, касающихся предикторов и оценки значимости регрессионных коэффициентов.

```{r}
summary(mod_1)
```

Давайте разбираться!

Вывод начинается с повторения регрессионного уравнения под заголовком `Call`: `lm(formula = Sales ~ TV, data = Advertising)`.

Затем в модели приводится распределение остатков. Остатки должны иметь нормальное распределение с абсолютными значениями минимума и максимума, также как и квартилями очень близкими друг к другу, что предполагает их примерно одинаковое расстояние от центра распределения.

В нашем случае это правило выполняется.

Следующая часть вывода содержит таблицу с коэффициентами.

Чтобы понять, что они означают и каким образом получаются, приведем формулу регрессии, но для случая с одной переменной:

$$y_i=\alpha+\beta x_i+\varepsilon_i,$$

где $y$ - зависимая переменная, $i$ - единица анализа, $\alpha$ - интерцепт (константа), $\beta$ - коэффициент регрессии, $x$ - независимая переменная и $\varepsilon$ - ошибка.

Когда мы работаем с выборочными данными, формула изменяется, так как вместо истинных значений у нас будут оценки:

$$\hat{y}=\hat{\alpha}+\hat{\beta}x$$

Оценка для интерцепта ($\alpha$) - значение $y$ когда $x = 0$. В геометрическом смысле это точка пересечения регрессионной прямой с осью $OY$. Иногда интерцепт может иметь смысл и подлежит интерпретации, но часто он может принимать несуществующие значения, выходящие за рамки возможных значений переменных, не описывается и не интерпретируется (допустим, мы пытаемся выявить зависимость веса от роста, получается, что интерцепт нам покажет, чем равен вес, когда рост равен нулю, что не имеет смысла).

В нашем примере интерцепт - это среднее значение продаж при нулевых затратах на рекламу.

Далее следуют коэффициенты для предикторов. Для каждого предиктора коэффициент обозначает ожидаемое изменение в зависимой переменной при изменении предиктора на одну единицу.

Геометрический смысл beta-beкоэффициента: это угол наклона регрессионной прямой:

![](https://assets.coursehero.com/study-guides/lumen/images/pima-concepts-statistics/linear-regression-3-of-4/m3_examining_relationships_topic_3_2_fit_line_algebra11.gif)

Чтобы найти коэффициенты $\alpha$ и $\beta$ нам нужно понять, как они вычисляются. Начнем с коэффициентов $\beta$:

$$\hat{\beta}=\frac{cov(x,y)}{var(x)}$$

Чтобы узнать коэффициент $\beta$ для рекламы на телевидении, нам необходимо найти ковариацию между рекламой и продажами и дисперсию затрат на рекламу:

```{r}
cov_x_y<-cov(Advertising$TV, Advertising$Sales)
var_x<-var(Advertising$TV)
beta_x<-cov_x_y/var_x
beta_x
```

Коэффициент показывает, на сколько увеличатся продажи, при увеличении затрат на рекламу на единицу.

Посчитав коэффициент $\beta$, мы можем перейти к $\alpha$:

$$\hat{\alpha}=\bar{y} - \hat{\beta}\bar{x}$$

Для того, чтобы вычислить его вручную, нам необходимо знать средние значения $\bar{y}$ и $\bar{x}$.

```{r}
y_bar<-mean(Advertising$Sales)
x_bar<-mean(Advertising$TV)
intercept<-y_bar-beta_x*x_bar
intercept
```

Все сходится!

Теперь мы можем посчитать предсказанные значения по продажам на основе нашей модели:

```{r}
yhat <- intercept + beta_x * Advertising$TV
head(yhat) 
```

Когда мы проводим регрессионный анализ, один из главных вопросов, которые мы себе задаем - можно ли на основе знаний о переменной $x$ понять, как ведет себя $y$. Говоря формальным языком, мы исследуем эту взаимосвязь, оценивая остаточные значения, ассоциированные с коэффициентами $\alpha$ и $\beta$, на основе тестирования гипотез о том, отличаются ли данные коэффициенты от нуля.

Например:

-   $H_0: \beta=0$
-   $H_1: \beta \neq 0$

Иными словами, если коэффициент $\beta$ равен нулю, то переменная $x$ никак не объясняет $y$, так как $0 \times x=0$.

Поскольку мы основываемся на допущении о том, что остатки имеют нормальное распределение, это нам позволяет рассчитать t-статистику для коэффициентов и проверить их статистическую значимость.

Хотя R все делает автоматически, давайте разберемся, как это происходит.

Посчитаем разницу между реальными и предсказанными значениями по продажам:

```{r}
res <- Advertising$Sales - yhat
head(res)
```

Далее мы должны посчитать разброс наблюдений вокруг регрессионной прямой, которую мы только что воспроизвели, а также стандартную ошибку остатков, которая используется для оценки ошибок коэффициентов и их статистической значимости.

Чтобы найти стандартную ошибку остатков, нам требуется:

-   найти сумму квадратов отклонений остатков ($RSS$)
-   найти количество степеней свободы ($df=N-2$)

```{r}
res.sqr <- res^2
RSS <- sum(res.sqr, na.rm=T)
df <- length(Advertising$Sales) - 2
df
RSE <- sqrt(RSS / df)
RSE
```

Собственно говоря, это мы и видим в выводе: `Residual standard error: 3.259 on 198 degrees of freedom`.

Зная стандартную ошибку остатков, мы можем вычислить стандартные ошибки для наших коэффициентов. Для этого, мы должны сначала вычислить сумму квадратов отклонений по независимой переменной (затраты на рекламу ТВ):

```{r}
TSSx <- sum((Advertising$TV - x_bar)^2)
TSSx
```

Чтобы найти стандартную ошибку коэффициента $\beta$, нужно стандартную ошибку остатков разделить на квадратный корень из суммы квадратов отклонений по переменной $x$:

$$SE_{\beta}=\frac{RSE}{\sqrt{TSS_x}}$$

```{r}
SEB <- RSE / sqrt(TSSx)
SEB
```

Для интерцепта алгоритм несколько отличается:

$$SE_{\alpha}=RSE*\sqrt{\frac{1}{N}+\frac{\bar{x^2}}{TSS_x}}$$

```{r}
SEA <- RSE * sqrt((1 / 200)+(x_bar^2 / TSSx))
SEA
```

Зная стандартные ошибки, мы можем теперь посчитать соответствующие t-статистики, чтобы оценить, отличаются ли наши коэффициенты от нуля. Для этого нужно значения коэффициентов разделить на их стандартные ошибки.

Для коэффициента по переменной телевизионной рекламы:

```{r}
t.B <- beta_x / SEB
t.B
```

Для интерцепта:

```{r}
t.A <- intercept / SEA
t.A
```

Эти значения измеряются в стандартных отклонениях и показывают, насколько далеко наши коэффициенты находятся от нуля. Значения 17,6 и 15,4 очень большие, следовательно, наши коэффициенты статистически значимы, что и подтверждают соответствующие p-значения из вывода: `15.36   <2e-16 ***` и `17.67   <2e-16 ***`.

#### Показатели качества модели

Какие еще важные показатели мы должны принять во внимание, когда мы анализируем результаты регрессионного анализа?

Обратимся к оставшейся части вывода.

Основной мерой, показывающей, насколько хорошо регрессионная модель объясняет данные, является коэффициент детерминации - $R^2$. Для того, чтобы найти $R^2$, нужны следующие промежуточные вычисления о некоторых компонентах дисперсии зависимой переменной:

-   сумме квадратов остатков ($RSS$)
-   общей сумме квадратов отклонений от среднего ($TSS$)
-   сумме квадратов отклонений, объясненной моделью ($ESS$)

```{r}
TSS <- sum((Advertising$Sales - y_bar)^2)
TSS
ESS <- TSS - RSS
ESS
r.sqr <- ESS / TSS
r.sqr
```

Таким образом, 61,2% дисперсии зависимой переменной объясняется регрессионной моделью.

Несмотря на то, что показатель $R^2$ является довольно информативным, у него есть один существенный недостаток: он имеет свойство неоправданно возрастать, при включении дополнительных переменных в анализ, даже если они не оказывают существенного влияния на зависимую переменную. Иными словами, чем более комплексной будет модель, тем выше будет $R^2$, что не очень хорошо.

Поэтому вместо обычного $R^2$ в качестве более точной оценки качества модели используется скорректированный показатель - `adjusted`$R^2$. Проблему множественных предикторов этот показатель решает, путем внесения «наказаний» (пенальти) за включение в модель дополнительных переменных. Чтобы найти скорректированный $R^2$ используется формула:

$$1-\frac{(1-R^{2})(n-1)}{n-k-1},$$

где $k$ - количество предикторов в модели, не считая интерцепта (A).

```{r}
r.sqr.adj<-1-(((1 - r.sqr) * (200 - 1)) / (200 - 1 - 1))
r.sqr.adj
```

Поскольку у в модели один предиктор, значение уменьшилось незначительно.

У нас остался нерассмотренным только один показатель из вывода - F-статистика. F-критерий является «глобальным» тестом, показывающим, насколько лучше наша модель базовой модели - такой, в которую включен только один интерцепт.

Еще одна интерпретация: этот тест показывает, что в нашей модели есть хотя бы один значимый предиктор.

В нашем выводе `F-statistic: 312.1 on 1 and 198 DF,  p-value: < 2.2e-16`, что указывает на то, что модель с предиктором существенно лучше базовой модели объясняет зависимую переменную.

#### Сравнение нескольких моделей

Допустим, мы хотим создать несколько двумерных моделей и сравнить их. Это возможно с помощью функции `mtable()` из пакета `memisc` (Management of Survey Data and Presentation of Analysis Results - управление данными исследований и презентация результатов анализа). Для демонстрации создадим три модели, иллюстрирующие взаимосвязь между продажами и каждым типом рекламы.

Первая модель у нас уже есть, создадим две других:

```{r}
mod_2 = lm(Sales ~ Radio, data = Advertising)
mod_3 = lm(Sales ~ Newspaper, data = Advertising)
```

Благодаря функции 'mtable()' мы можем создать таблицу, в которой сведем всю важную информацию по всем трем моделям:

```{r}
library(memisc)
mtable<-mtable(mod_1, mod_2, mod_3)
mtable
```

Видим, что хотя во всех моделях интерцепты и коэффициенты предикторов являются значимыми, показатель $R^2$ максимально высок в модели, где в качестве объясняющей переменной используется показатель затрат на рекламу на телевидении.

### Множественная регрессия

Рассмотрим случай, когда количество предикторов больше одного, то есть наша модель является моделью уже не простой, а множественной регрессии.

Формула для нескольких предикторов приобретает вид:

$$\hat{y} = \hat{\beta_0} + \hat{\beta_1}x_1 + \hat{\beta_2}x_2 ...+... \hat{\beta_n}x_n$$

Синтаксис в R аналогичен тому, что мы использовали для двумерной регрессии. Создадим модель, в которую включим сразу все независимые переменные.

```{r}
mod_4 = lm(Sales ~ TV + Radio + Newspaper, data = Advertising)
#mod_4 = lm(Sales ~ ., data = Advertising) можно использовать и такой синтаксис
```

Кроме функции `summary()` красивую таблицу с результатами можно создать с помощью функции `tab_model()` из библиотеки `sjPlot`

```{r}
sjPlot::tab_model(mod_4)
```

Результаты показывают, что значимыми являются только коэффициенты для радио- и телерекламы, тогда как реклама в газетах не является значимым фактором, определяющим продажи.

Скорректированный коэффициент детерминации (`Adjusted R-squared`), показывает, что эта модель гораздо лучше, чем любая модель с одним предиктором, и объясняет 89,6% дисперсии.

Дополнительно, в целях сравнения, давайте создадим более простую модель, без газет.

```{r}
mod_5 = lm(Sales ~ TV + Radio, data = Advertising)
sjPlot::tab_model(mod_5)
```

Как видим, коэффициент детерминации не изменился (что неудивительно, ведь у удаленной переменной коэффициент регрессии равнялся нулю).

#### Сравнение моделей с помощью дисперсионного анализа

Чтобы сравнить, какая модель работает лучше, можно применить функцию `anova()`, запускающую дисперсионный анализ. В нашем случае, мы будем сравнивать модель со всеми предикторами **mod_1** с сокращенной моделью **mod_0**. Наша задача будет заключаться в том, чтобы понять, какую роль играет переменная газетной рекламы в аддитивной модели.

```{r}
anova(mod_4, mod_5)
```

Мы видим, что разница между моделями в одну степень свободы (1 параметр - как раз наша переменная о рекламе в газетах).

::: {.alert .alert-info role="alert"}
**Число степеней свободы (df)** − важный показатель регрессионного анализа, используемый в формулах метрик, показывающих качество модели:

-   Res.Df - число степеней свободы, рассчитываемых для остатков (разности между предсказанными и реальными значениями).
-   Res.Df - количество наблюдений - количество оцениваемых параметров.
-   Model 1: Sales \~ TV + Radio
-   df= 197= 200-3 (2 предиктора + константа)
-   Model 2: Sales \~ TV + Radio + Newspaper
-   df= 196= 200-4 (2 предиктора + константа)
:::

Результаты дисперсионного анализа показывают, что качество модели не поменялось, и значит мы можем использовать более лаконичную (сокращенную) модель.

#### Предсказание значений зависимой переменной для новых данных

Обычно у регрессионного анализа две основные задачи - **объяснение** взаимосвязи между переменными и **предсказание** новых (неизвестных) значений зависимой переменной на основе модели. Для осуществления прогноза чаще всего используется функция `predict()`, обладающая большой гибкостью (может применяться с различными методами моделирования и типами данных).

Если эту функцию использовать к модели, созданной на основе функции `lm()`, то она будет рассчитывать предсказанные значения для каждого наблюдения.

Давайте посмотрим первые десять.

```{r}
head(predict(mod_5), n = 10)
```

Отметим, что эффект функции `predict()`будет зависеть от того, какие данные даются на входе. Наша модель относится к классу`lm`, поэтому `predict()` запускает функцию`predict.lm()` Если нам нужно что-то другое, можно посмотреть подробности с помощью `?predict.lm()`.

Мы также можем сгенерировать новые данные, и попробовать посчитать зависимую переменную на них.

Давайте создадим новый набор с идентичными именами переменных.

```{r}
new_obs = data.frame(TV = 150, Radio = 40, Newspaper = 1)
```

Теперь мы можем использовать `predict()`, чтобы посчитать оценки и доверительные интервалы для новых данных.

Если указать только модель и источник данный, `R` выдаст точечную оценку, то есть "предсказанное значение" $\hat{y}$.

```{r}
predict(mod_5, newdata = new_obs)
```

Если указать дополнительно аргумент`interval` со значением `"confidence"`, `R` покажет также 95% доверительные интервалы для среднего значения по данному наблюдению.

```{r}
predict(mod_1, newdata = new_obs, interval = "confidence")
```

Кроме того, мы можем изменить уровень и выбрать не доверительные интервалы, а предсказательные интервалы (доверительные интервалы прогноза). В чем отличие?

Предсказательные интервалы показывают, в каком диапазоне значений будет находиться будущее наблюдение, тогда как доверительные интервалы показывают вероятный диапазон, в котором будет находится какой-либо статистический параметр, например, среднее в генеральной совокупности.

Поскольку предсказательные интервалы рассчитываются в ситуации большей неопределенности, то они обычно шире, чем доверительные интервалы.

```{r}
predict(mod_1, newdata = new_obs, interval = "prediction", level = 0.95)
```

#### Диагностика модели и оценка влияния наблюдений на результаты

В `R` доступны несколько функций, позволяющих оценить, насколько полученная модель хорошо воспроизводит исходные данные, и как различные наблюдения вносят вклад в предсказательные способности этой модели:

-   `resid()` выдает остаток (разность между предсказанным и реальным значением)
-   `hatvalues()` показывает `leverage` - отклонение в значениях по независимым переменным по каждому наблюдению. Данный показатель важен для понимания, как экстремальные значения по независимым переменным могут повлиять на результаты анализа.

::: {callout-info}
Что такое `hat` - значения? `hat` - *по-английски* «шляпа», а также диакритический знак «циркумфлекс» ($\hat{ }$), с помощью которого обозначаются значения зависимой переменной, предсказанные с помощью регрессионной модели.

Эти предсказанные значения обозначаются как $\hat{y}$ и рассчитываются по формуле:

$$\hat{y}=Xb$$

Для коэффициентов линейной регрессии используется следующая формула:

$$b = (X^{'}X)^{-1}X^{'}y$$

Следовательно, мы можем переписать уравнение для предсказанных значений как:

$$\hat{y}=X(X^{'}X)^{-1}X^{'}y$$

Таким образом, предсказанные значения могут быть получены путем умножения $n \times 1$ вектора $y$, содержащего наблюдаемые значения на $n \times n$ матрицы $H$:

$$H=X(X^{'}X)^{-1}X^{'}$$

Или, более лаконично:

$$\hat{y}=Hy$$

Матрица $H$ часто называется `hat-matrix` - «матрица в шляпе», а ее диагональные значения как раз и являются значениями левериджа.
:::

-   `rstudent()` стьюдентизированные остатки по каждому наблюдению (остаток в регрессионной модели деленный на ее скорректированную стандартную ошибку)
-   `cooks.distance()` рассчитывает важность каждого наблюдения

```{r}
ddf <- data.frame(residuals=residuals(mod_5), rstandard=rstandard(mod_5), rstudent=rstudent(mod_5), leverage=hatvalues(mod_5), cookd=cooks.distance(mod_5))
```

Как мы можем это использовать?

Например, мы можем отобрать наблюдения, чьи стандартизированные остатки отклоняются более, чем на 2 стандартных отклонения в обе стороны:

```{r}
library(dplyr)
filter(ddf, abs(rstandard) > 2 | abs(rstudent) > 2)
```

> {{< iconify arcticons brain-it-on size=42px >}} **Задание**: проанализируйте в таблице исходных данных наблюдения с указанными номерами. Какие выводы можно сделать?

Второй важный момент: анализ показателей `leverage` и `Cook's distance`.

Замечательная вещь по поводу левериджа заключается в том, что его значения помогают выявить экстремальные значения $x$, которые могут влиять на результаты регрессионного анализа. Каким образом? Мы должны понять, какое значение левериджа нужно признать большим, то есть соответствующим значениям $x$, расположенным максимально далеко от средних значений по всем другим наблюдениям. Общим является правило, согласно которому, любое наблюдение, чье значение левериджа в три раза превышает среднее значение, является нетипичным / странным / достойным внимания:

$$\bar{h}=\frac{\sum_{i=1}^{n}h_{ii}}{n}=\frac{p}{n}$$

Иными словами, если:

$$h_{ii} >3\left( \dfrac{p}{n}\right),$$

то мы должны обратить внимание на это наблюдение. Сумма всех значений левериджа равняется количеству параметров модели: `r sum(ddf$leverage)` - два предиктора + интерцепт (константа).

```{r}
hat_max = 3*3/200
filter(ddf, leverage>hat_max)
```

Что мы видим? Мы видим, что по модели 5 у нас нет таких наблюдений, чей леверидж превышал бы максимально возможное значение.

Мы также можем отсортировать наблюдения по расстоянию Кука, чтобы понять, какие наблюдения являются наиболее влиятельными:

$$D_i=\frac{(y_i-\hat{y}_i)^2}{(k+1) \times MSE}\left[ \frac{h_{ii}}{(1-h_{ii})^2}\right],$$

где $MSE$ - среднеквадратическая ошибка регрессии, а $h_{ii}$ - значения левериджа.

```{r}
arrange(ddf, desc(cookd))[1:6,]
```

Рекомендуется исключать из анализа наблюдения, расстояние Кука для которых превышает 1. В нашем анализе таких нет, но вот наблюдения 131 и 6 являются все-таки подозрительными, как имеющие наиболее расстояние Кука и самые большие остатки.

Аналогичную информацию можно получить с помощью специальных графиков:

```{r}
par(mfrow = c(2, 2))
plot(mod_5)
```

Что показывают графики?

**1. Residuals vs Fitted** (Остатки vs предсказанные значения)

Этот график показывает, есть ли в остатках регресии какие-либо нелинейные паттерны. Такое может случиться, если между предикторными переменными и зависимой переменной имеются нелинейные взаимосвязи, соответственно если эта нелинейность возникает на графике, значит модель плохо воспроизводит эти отношения. Если мы видим, что остатки равномерно распределены вокруг линии предсказанных значений без каких-либо серьезных колебаний, это хороший знак, значит у нас в модели таких нелинейных взаимосвязей нет. На нашем графике есть еле заметный «прогиб», но четким паттерном его назвать вряд ли возможно.

**2. Normal Q-Q residuals**

Данный график показывает, что остатки нормально распределены (то есть маленьких остатков много и их среднее значение приближается к нулю, в больших остатков мало). У нас с нормальностью остатков практически все в порядке, если не считать постоянно выбивающееся наблюдение 131.

**3. Scale-Location**

Данный график позволяет протестировать допущение о гомогенности дисперсии остатков (гомоскедастичности). Если мы видим, что остатки распределены вдоль линии равномерно, и их форма не напоминает «фен», то все хорошо.

**4. Residuals vs Leverage**

Ну и, наконец, последний график визуализирует самые влиятельные наблюдения - одновременно через леверидж и расстояние Кука. Сомнительные наблюдения на всех графиках обозначены цифрами.

Библиотека `olsrr` (Tools for Building OLS Regression Models) также содержит несколько полезных функций, которые могут помочь в выявлении таких наблюдений.

```{r}
library(olsrr)
ols_plot_cooksd_bar(mod_1)
```

```{r}
ols_plot_cooksd_chart(mod_1)
```

```{r}
ols_plot_dfbetas(mod_1)
```

Что дальше? Мы выяснили, что некоторые наблюдения являются нетипичными, что может приводить к искаженным вычислениям. Но мы можем попробовать удалить переменные, которые вызвали наибольшее количество вопросов, и сравнить результаты.

```{r}
Advertising2<-Advertising[-c(6,131),]
mod_6 = lm(Sales ~ TV + Radio, data = Advertising2)
mtable<-mtable(mod_5, mod_6)
mtable
```

После удаления экстремальных наблюдений, качество модели улучшилось (скорректированный $R^2=91.5%$), хотя общие выводы аналогичны.

#### Мультиколлинеарность

Еще один сложный термин))) Что такое мультиколлинеарность? Мультиколлинеарность случается тогда, когда один предиктор может предсказывать другой. Иными словами, мы хотели бы, чтобы предикторы хорошо предсказывали поведение зависимой переменной, но не друг друга, и если такое случается, то это и называется мультиколлинеарностью. Хотя слишком высокая мультиколлинеарность является редкостью, проверка на нее является одной из стандартных процедур регрессионного анализа. Отметим, что проблема мультиколлинеарности является важной, когда мы исследуем важность предикторов, пытаемся на основе интерпретации коэффициентов регрессии обнаружить значимые закономерности (например, доказать, что повышение уровня образования может привести к значительному увеличению доходов или что по мере развития ассоциаций между гражданами увеличивается уровень институционального доверия). Если же первостепенной задачей моделирования является предсказание (как бывает во многих задачах машинного обучения), то проблема мультиколлинеарности не является релевантной, и ее можно проигнорировать.

Как мы можем проверить, если в нашей модели чрезмерная мультиколлинеарность?

Самый простой способ - посмотреть на коэффициенты корреляции между предикторами:

```{r}
Advertising %>%
  dplyr::select(TV, Radio, Newspaper) %>%
  cor()
```

Наши независимые переменные связаны друг с другом довольно слабо. Специальной мерой, позволяющей проверить мультиколлинеарность, является $VIF$- variance inflation factor, показывающая увеличение в дисперсии коэффициентов после включения дополнительной переменной:

```{r}
car::vif(mod_6)
```

VIF \< 3 обозначает слабую корреляцию между переменными (идеальные условия). Чаще всего в литературе приводится пороговое значение $VIF=5$, и только переменные $VIF<5$ должны быть включены в модель.

У нас в модели с мультиколлинеарностью все в порядке.

### Линейная регрессия с категориальными предикторами

Напомним, что **категориальные переменные** (также известные, как качественные, или факторные переменные) - это такие переменные, которые позволяют разделить наблюдения на группы. Их особенностями является ограниченное количество значений (уровней). Типичными являются примеры с полом (два уровня - мужчины и женщины) или национальностью, социальным статусом или уровнем образования (например, лица с общим средним, средним профессиональным и высшим образованием).

Обычно регрессионный анализ проводится с количественными переменными, и когда исследователь желает включить в модель категориальную переменную, необходимы некоторые шаги, чтобы сделать результаты более интерпретируемыми.

В частности, категориальные переменные перекодируются в набор так называемых «dummy» (фиктивных) переменных, в результате создается **матрица контрастов**. Современные программы, в том числе и R, «умеют» это делать автоматически.

> {{< iconify arcticons anz size=42px >}} **Пример**: воспользуемся набором данных `Salaries` из пакета `car`, в котором содержатся данные о зарплате ассистентов, ассоциированных профессоров и профессоров в одном из американских колледжей (данные - за 2008-2009 учебный год). Данные были собраны администрацией для того, чтобы отслеживать различия между зарплатой, получаемой преподавателями мужчинами и женщинами,

Загрузим данные:

```{r}
library(car)
data("Salaries")
head(Salaries, 3)
```

#### Категориальные переменные с двумя уровнями

Вспомним, что в регрессионном уравнении для того, чтобы предсказать переменную $y$ на основе независимой переменной $x$, нужно суммировать все основные компоненты:

$$y = b_0 + b_1*x$$

При этом:

-   $b_0$ и $b_1$ являются регрессионными коэффициентами, представляющими константу (интерцепт) и угол наклона регрессионной прямой (`slope`).

Допустим, мы хотим проанализировать различия в заработной плате у мужчин и женщин.

На основе переменной пола, мы можем создать новую фиктивную переменную, которая будет принимать значения:

-   1 если преподаватель мужчина
-   0 если преподаватель женщина

и использовать эту переменную в регрессионном уравнении. При этом интерпретация коэффициентов и самого уравнения будет следующей:

-   $b_0$ средняя зарплата у женщин,
-   $b_0 + b_1$ средняя зарплата у мужчин,
-   $b_1$ различия в среднем между зарплатой мужчин и женщин.

Создадим модель:

```{r}
mod_7  <- lm(salary ~ sex, data = Salaries)
summary(mod_7)$coef
```

Исходя из выведенной информации, средняя зарплата у преподавателей женщин - 101002 долларов (за 9 месяцев), тогда как у мужчин `101002 + 14088 = 115090`. Полученное p-значение для фиктивной переменной `sexMale` очень значимое, что указывает на то, что имеются статистические обоснования наличия различий в зарплате по полу.

Функция `contrasts()`позволяет посмотреть код, который использовался для создания фиктивных переменных:

```{r}
contrasts(Salaries$sex)
```

При такой кодировке женщины являются референтной группой, с которой сравниваются мужчины, и в целом, любая подобная кодировка является условной, ее результаты будут влиять только на интерпретацию коэффициентов регрессии.

Если нас такая кодировка не устраивает, мы можем использовать функцию `relevel()` для смены уровней:

```{r}
Salaries <- Salaries %>%
  mutate(sex = relevel(sex, ref = "Male"))
```

После перекодировки результаты регрессионного анализа будут следующими:

```{r}
mod_8 <- lm(salary ~ sex, data = Salaries)
summary(mod_7)$coef
```

Поскольку мы теперь сравниваем зарплату женщин с зарплатой мужчин, коэффициент переменной `sexFemale` негативный, что означает более низкий уровень зарплат у женщин, по сравнению с мужчинами.

Коэффициент $b_0$ равено 115090 (средняя зарплата у мужчин), тогда как коэффициент $b_1$ - -14088, показывает, на сколько, в среднем, ниже зарплата у женщин. Соответственно, 115090 - 14088 = 101002 - средняя зарплата женщин.

#### Категориальная переменная с более чем двумя уровнями

Что делать, если в качественной переменной, которую мы хотим использовать, более двух уровней? Наиболее типичным является подход, когда такая категориальная переменная трансформируется в `n-1` бинарных переменных, каждая из которых имеет по два уровня. И эти `n-1` новых переменных содержат ту же информацию, что исходная переменная. В результате такой кодировки создается таблица контрастов.

Например, в нашем наборе есть переменная `rank`, которая имеет три уровня: `AsstProf`, `AssocProf` и `Prof`. Мы можем создать две фиктивных переменных - `AssocProf` и `Prof`:

-   если `rank = AssocProf`, тогда в новом столбце `AssocProf` преподавателями, являющими ассоциированными профессорами, будет присвоено значение 1, а профессорам - 0.
-   если `rank = Prof`, тогда в новом столбце `Prof` все профессора получат значение 1, а ассоциированные профессора - 0.\
-   что же с ассистентами? В обоих новых столбцах они получат значение 0.

Такого рода кодировка в R осуществляется автоматически. С помощью функции `model.matrix()` мы можем посмотреть, как такая матрица контрастов может выглядеть:

```{r}
res <- model.matrix(~rank, data = Salaries)
head(res[, -1])
```

В практике регрессионного анализа есть различные способы кодирования категориальных переменных (создания контрастов). По умолчанию в R первый уровень используется в качестве референтного, а остальные интерпретируются уже по отношению к этому уровню.

::: {callout-tip}
Пример, который мы только что рассмотрели, показывает, что дисперсионный анализ - ANOVA (analyse of variance) является специальным случаем линейной модели, в которой предикторами являются категориальные переменные. И поскольку R это тоже «понимает», мы можем извлечь из модели результаты дисперсионного анализа (предпочтительнее использовать функцию `Anova()` из пакета `car` (car означает Companion to Applied Regression - компаньон для прикладных задач регрессионного анализа).
:::

Создадим модель, в которой мы будем предсказывать зарплату от всех других переменных в наборе (знак плюс означает, что мы будем рассматривать только главные эффекты, без интеракций):

```{r}
mod_9<- lm(salary ~ yrs.service + rank + discipline + sex,
             data = Salaries)
Anova(mod_9)
```

После того, как мы приняли во внимание другие переменные (стаж - yrs.service, должность - rank, область знаний - discipline), стало понятно, что фактор пола уже не имеет значения и не вносит вклада в вариабельность заработной платы. Значимыми становятся должность и область знания.

Чтобы вывести более подробные результаты анализа, лучше воспользоваться функцией `summary()`:

```{r}
summary(mod_9)
```

Результаты показывают, что зарплата ассоциированного профессора в среднем на 14560.40 долларов выше, чем у ассистента, при прочих равных условиях, а у профессора - выше на 49159.64 долларов. Интересно, что зарплата значительно варьирует от специализации: на прикладных кафедрах (applied departments) наблюдается в среднем на 13473.38 долее высокая зарплата, по сравнению с теоретическими дисциплинами (theoretical departments).

#### Интеракции

Интеракции происходят тогда, когда эффект одного из предикторов зависит от другой переменной в модели.

Чтобы продемонстрировать эффект интеракции, рассмотрим взаимосвязь между должностью и областью знаний в примере про зарплату преподавателей:

$$
\begin{split}
y_i &=\beta_0 + \beta_1*(rank) + \beta_2*(discipline) + \beta_3*(rank*discipline) +\\ & \beta_4*(yrs.service) + \beta_5*(sex) + \varepsilon_i
\end{split}
$$

```{r}
mod_9 <- lm(salary ~ yrs.service + sex + rank * discipline, data = Salaries)
summary(mod_9)
```

::: {callout-tip}
Отметим, что хотя в формуле мы указали только интеракцию, в выводе содержатся также сведения и об индивидуальных эффектах. R включает эту информацию автоматически.
:::

Интерпретируя результаты отметим, что эффект от взаимосвязи не значим, и единственным значимым предиктором в модели остается должность: значительная прибавка в зарплате отмечается только у профессоров, тогда как дисциплинарная принадлежность значима только на уровне статистической тенденции ($p=0,086$).

#### Отбор переменных для модели

Прежде чем перейти к моделированию, аналитик проводит тщательную работу по отбору переменных. Обычно, этому предшествует теоретический анализ, который позволит определить, какие показатели, важные для целевой переменной, необходимо включить в исследование, а затем - в модель.

Однако, когда эксперимент уже проведен, наступает время проверки статистических гипотез. Очевидно, что не всегда все включаемые в модель параметры, в конце концов оказываются значимыми.

Какие алгоритмы мы можем использовать для определения финальной, самой лучшей модели из возможных?

Отбор переменных (variable selection) - это процесс выбора наиболее значимых переменных для включения в регрессионную модель. Методы отбора помогают улучшить производительность модели и избежать чрезмерной подгонки.

В рамках данного занятия мы рассмотрим следующие методы отбора:

-   анализ всех возможных моделей / лучшей модели, определяемой на основе оценке качества модели
-   пошаговые алгоритмы

Для работы мы будем использовать пакет `olsrr`:

```{r eval=FALSE}
install.packages("olsrr")
library(olsrr)
```

###### Анализ всех возможных моделей

Прежде чем мы рассмотрим методы пошагового отбора, давайте вкратце рассмотрим регрессию по всем/лучшим подмножествам. Поскольку они оценивают все возможные комбинации переменных, эти методы требуют больших вычислительных затрат и могут вывести систему из строя, если использовать их с большим набором переменных.

Метод `All subset regression` (все возможные варианты) представляет результаты по всем возможным комбинациям предикторов. Если у нас есть $k$ потенциальных независимых переменных, не считая константы, то количество отдельных моделей, которые потребуется проанализировать, составит - $2^k$. Например, если у нас 10 предикторов, то количество моделей - $2^10$ - `r 2^10`, а если переменных 20 - то количество комбинаций превышает миллион.

```{r}
model <- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
ols_step_all_possible(model)
```

###### Подборка лучших моделей (Best Subset Regression)

Данный метод позволяет отобрать модели, которые являются лучшими по обобщенным критериям модели, например, имеет наибольший $R^2$ или меньшие $MSE$ (средняя квадратичная ошибка) или $AIC$ (информационный критерий Акаике).

```{r}
model <- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
ols_step_best_subset(model)
```

###### Пошаговый отбор (Stepwise Selection)

**Пошаговая регрессия** - это метод подбора регрессионных моделей, который предполагает итерационный отбор независимых переменных для использования в модели. Он может быть реализован с помощью прямого отбора, обратного исключения или комбинации обоих методов.

Метод **прямого отбора** начинается с модели без предикторов и постепенно добавляет каждую новую переменную, проверяя ее статистическую значимость, а метод **обратного исключения**, напротив,ю начинается с полной модели и затем по очереди удаляет наименее статистически значимые переменные.

> {{< iconify arcticons anz size=42px >}} **Пример**: Для иллюстрации возможностей пошагового отбора воспользуемся данными из области недвижимости (HousingData).Набор включает данные по 506 объектам недвижимости, оцененных по 14 показателям.

Переменные:

-   CRIM : уровень преступности
-   ZN : proportion of residential land zoned for lots over 25,000 sq.ft.
-   INDUS : доля промышленных предприятий среди нежилых объектов
-   CHAS : дамми переменная, показывает расположение относительно главной реки
-   NOX : концентрация нитрита озота
-   RM : среднее количество комнат
-   AGE : доля зданий, построенных до 1940
-   DIS : взвешенное расстояние до пяти значимых бостонских деловых центров
-   RAD : индекс доступности хайвея
-   TAX : налоги
-   PTRATIO : соотношение между учителями и преподавателями (обеспеченность учителями)
-   B : доля чернокожено населения
-   LSTAT : доля населения с низкими доходами
-   MEDV : медианная стоимость в 1000 долларов

{{< iconify arcticons 1dm size=42px >}}[Скачать данные](https://www.kaggle.com/datasets/altavish/boston-housing-dataset/data)

Загрузим данные и создадим общую модель:

```{r}
HousingData<-read.csv("HousingData.csv")
model <- lm(MEDV ~ ., data = HousingData)
summary(model)
```

Метод ступенчатого включения (начинаем с нулевой модели и постепенно добавляем предикторы)

```{r}
ols_step_forward_p(model)
```

Метод пошагового исключения:

```{r}
ols_step_backward_p(model)
```

Принудительное включение в модель по имени переменной:

```{r}
ols_step_forward_p(model, include = c("AGE", "LSTAT"))
```

Принудительное включение по индексу:

```{r}
ols_step_forward_p(model, include = c(5, 7))
```

Выбор на основе коэффициента детерминации:

```{r}
ols_step_forward_adj_r2(model)
```

Визуализация модели:

```{r}
k <- ols_step_forward_adj_r2(model)
plot(k)
```

###### Иерархический отбор

Когда для отбора переменных используются p-значения, возможно использования иерархического отбора. Этот метод предполагает, что поиск значимых переменных ограничен следующей переменной. Если какая-то переменная не отбирается по причине не подходящего p-значения, то и ни одна последующая переменная не рассматривается для включения.

```{r}
ols_step_forward_p(model, 0.1, hierarchical = TRUE)
```

Пошаговая регрессия может оказаться хорошей идеей, особенно, когда количество предикторов велико и нужно отобрать только самые значимые. Между тем, исследователи отмечают большое количество «подводных камней» и статистических проблем, которые могут возникнут в процессе применения регрессионного анализа, таких как переобученность данных, смещенные оценки, ошибки I рода (Harrell, 2015). Кроме того, в процессе применения пошаговых методов возникает опасная иллюзия итого, что компьютер автоматически отбирает правильные переменные, на самом деле это происходит без связи с теоретическими основаниями и гипотезами исследования. Более того, модель, которая была отобрана на основе какого-то критерия, на самом деле может оказаться нестабильной и малоинформативной. Во многих случаях правильным было бы опираться на теорию и предыдущие исследования, тогда как методы отбора могут рассматриваться в качестве поисковых техник.

## Логистическая регрессия

Логистическая регрессия применяется в том случае, если наша зависимая перменная имеет вид 0-1, то есть является дихотомической и имеет значения 1 и 0. По сути, такой регрессионный анализ решает задачу классификации, то есть определения принадлежности к одному из двух классов ("победит" или "проиграет", примут на работу или нет и т.д.).

> {{< iconify arcticons anz size=42px >}} **Пример**: В качестве примера, мы будем рассматривать данные о приеме в высшие учебные заведения. В частности, нас будет интересовать, как результаты выпускных экзаменов GRE (Graduate Record Exam scores) и средние оценки GPA (grade point average), а также престиж учебного заведения связаня с допуском в высшее учебное заведение. Зависимой является переменная admit/don’t admit, которая уже закодирована в формате 0-1.

```{r}
data <- read.csv("https://stats.idre.ucla.edu/stat/data/binary.csv")
head(data)

```

Модель логистической регрессии имеет вид:

$$
\log\left(\frac{p(x)}{1 - p(x)}\right) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots  + \beta_p x_p.
$$

Откуда, путем перестановки, мы можем вывести вероятность принадлежности к группе 1:

$$
p(x) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots  + \beta_p x_p)}} = \sigma(\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots  + \beta_p x_p)
$$

Обычно основное уравнение представляется в виде сигмоиды (логистической функции):

$$
\sigma(x) = \frac{e^x}{1 + e^x} = \frac{1}{1 + e^{-x}}
$$

![](https://static.javatpoint.com/tutorial/machine-learning/images/linear-regression-vs-logistic-regression.png)

Подгонка модели осуществляется путем максимизации функции правдоподобия, что практически никогда не происходит вручную, и мы предоставим возможность это сделать`R`

Начнем с того, что переведем ранг заведения в факторную переменную:

```{r}
data$rank <- factor(data$rank)
```

```{r}
model_glm <- glm(admit ~ gre + gpa + rank, data = data, family = "binomial")
```

Результаты логистической регрессии очень похожи на то, что мы видели в линейной регрессии, только вместо `lm()` мы используем `glm()`. Другая особенность - в атрибуте `family = "binomial"`, что означает, что у нас будет зависимая переменная, состоящая из двух классов. Если использовать `glm()` с `family = "gaussian"` то получится обычная линейная регрессия.

Давайте посмотрим на общие результаты

```{r}
summary(model_glm)
```

Вывод в целом напоминает то, что мы видели в линейной регрессии. Видим, что все наши зависимые переменные являются значимыми. Переменные ранга имеют отрицательные коэффициенты, так как сравниваются со значением 1 - группа учебных заведений с наиболее высокими позициями.

Однако, стоит помнить, что коэффициенты в логистической регрессии не простые, они представляют собой логарифм шансов. Что это значит?

Давайте посмотрим на таблицу с нашей зависимой переменной:

```{r}
table(data$admit)
```

Всего допущено 127 человек из 400, то есть вероятность допуска составит: 127/400=0.3175, а отношение шансов (допуска к недопуску): 0.3175/(1-0.1375)=0.3681159. Логарифм данного выражения составит:

```{r}
log(0.3175/(1-0.3175))
```

Это именно то, что мы бы получили, если бы создали модель только с одним интерцептом:

```{r}
model_null<-glm(admit ~ 1, data = data, family = "binomial")
summary(model_null)
```

Вернемся к коэффициентам нашей большой модели:

-   изменения в одну единицу по переменной `gre`, логарифм шансов допуска увеличится на 0.002.
-   изменение на одну единицу в `gpa`, логарифм шансов допуска увеличится на 0.804.

Индикаторные переменные для ранга имеют слегка иную интерпретацию. Например, посещая школу, входящую во вторую группу по престижности, по сравнению с группой 1 изменяет логарифм шансов на -0.675.

Внизу таблицы с коэффициентами располагаются индексы подгонки (AIC).

Чтобы перевести коэффициенты в обычное отношение шансов, применяется экспоненциальная функция. Можно соединить это действие с вычислением доверительных интервалов:

```{r}
exp(cbind(OR = coef(model_glm), confint(model_glm)))
```

Как интерпретировать отношение шансов?

Для количественных переменных меняется мало что:

-   изменение на одну единицу gre на 0,2% увеличивает шансы быть принятыми
-   изменение среднего балла на единицу увеличивает шансы на 123%
-   а вот учеба в школе ранга 2 снижает шансы на 50%, ранга 3 - на 73.8%, ранга 4 - на 78,8%.

::: {callout-tip}
Общая формула перевода отношения шансов в проценты: (OR-1) \* 100
:::

Следующий этап - посмотреть, как работает функция `predict()` вместе с `glm()`:

```{r}
head(predict(model_glm))
```

По умолчанию `predict.glm()` использует `type = "link"`.

```{r}
head(predict(model_glm, type = "link"))
```

Это означает, что `R` возвращает:

$$
\hat{\beta}_0 + \hat{\beta}_1 x_1 + \hat{\beta}_2 x_2 + \cdots  + \hat{\beta}_p x_p
$$ для каждого наблюдения.

Важно понимать, что это **не** предсказанные вероятности, и для того, чтобы их получить:

$$
\hat{p}(x) = \hat{P}(Y = 1 \mid X = x)
$$

мы должны написать `type = "response"`

```{r}
head(predict(model_glm, type = "response"))
```

Соответственно, это вероятности, но **не** результаты классификации. Для того, чтобы их получить, мы должны сравнить вероятности с пороговым значением.

```{r}
model_glm_pred = ifelse(predict(model_glm, type = "response") > 0.5, 1, 0)
head(model_glm_pred)
```

Что мы сделали?

$$
\hat{C}(x) = 
\begin{cases} 
      1 & \hat{f}(x) > 0 \\
      0 & \hat{f}(x) \leq 0 
\end{cases}
$$

где

$$
\hat{f}(x) =\hat{\beta}_0 + \hat{\beta}_1 x_1 + \hat{\beta}_2 x_2 + \cdots  + \hat{\beta}_p x_p.
$$

Тот код, который мы запустили, делает следующее:

$$
\hat{C}(x) = 
\begin{cases} 
      1 & \hat{p}(x) > 0.5 \\
      0 & \hat{p}(x) \leq 0.5 
\end{cases}
$$

где

$$
\hat{p}(x) = \hat{P}(Y = 1 \mid X = x).
$$

Посчитав классификации, мы можем также посчитать метрики для ошибок.

```{r, message = FALSE, warning = FALSE}
tab = table(predicted = model_glm_pred, actual = data$admit)
tab
library(caret)
confusionMatrix = confusionMatrix(tab, positive = "1")
c(confusionMatrix$overall["Accuracy"], 
  confusionMatrix$byClass["Sensitivity"], 
  confusionMatrix$byClass["Specificity"])
```

![](https://i.stack.imgur.com/NzSnD.jpg)

Мы можем также предсказать результаты допуска в вуз для новых данных.

Попробуем их сгенерировать на основе исходных данных:

```{r}
newdata1 <- with(data, data.frame(gre = mean(gre), gpa = mean(gpa), rank = factor(1:4)))
```

Предскажем результаты зачисления:

```{r}
newdata1$rankP <- predict(model_glm, newdata = newdata1, type = "response")
newdata1
```

Видим, что при средних оценках, учащиеся, обучавшиеся в престижных школах имеют большую вероятность поступить, чем те, кто училися не в очень престижных заведениях.

Мы также можем захотеть узнать, насколько хорошо наша модель соответствует действительности. Это может быть особенно полезно при сравнении конкурирующих моделей.

Аналогом $R^2$ для логистической регрессии является $R^2 Макфаддена$:

```{r}
with(summary(model_glm), 1 - deviance/null.deviance)
```

Если не хочется вычислять вручную, можно воспользоваться готовой функцией `pR2`:

```{r message=FALSE, warning=FALSE}
#install.packages('pscl')
library(pscl)

pR2(model_glm)['McFadden']

```

Еще одной популярной псевдомерой является $R^2 Nagelkerke$:

```{r message=FALSE, warning=FALSE}
#install.packages('fmsb')
library(fmsb)

NagelkerkeR2(model_glm)
```

## Мультиномиальная логистическая регрессия

Что делать, если наша зависимая переменная имеет не две, а более категорий? Для этого случая больше подходит мультиномиальная логистическая регрессия.

$$
P(Y = k \mid { X = x}) = \frac{e^{\beta_{0k} + \beta_{1k} x_1 + \cdots +  + \beta_{pk} x_p}}{\sum_{g = 1}^{G} e^{\beta_{0g} + \beta_{1g} x_1 + \cdots + \beta_{pg} x_p}}
$$

Мы не будем погружаться в технические детали, но попробуем реализовать этот подход на практике. мы воспользуемся знакомым нам набором данных `iris`.

Чтобы выполнить мультиномиальный регрессионный анализ нам потребуется функция `multinom` из библиотеки `nnet`, где используется синтаксис, похожий на `lm()` и `glm()`. Лучше добавить `trace = FALSE`, чтобы не выводилась информация об оптимизационных процессах во время обучения.

```{r}
library(nnet)
model_multi = multinom(Species ~ ., data = iris)
summary(model_multi)
```

Заметим, что на выходе у нас коэффициенты только для двух классов, так же как и в обычной регрессии у нас есть только коэффициент для одного класса.

## Непараметрическая регрессия. Метод k-ближайших соседей

Все методы, которые мы рассматривали до этого момента, являются параметрическими. Это можно представить в виде обобщающей формулы.

$$
f(x) = \mathbb{E}[Y \mid X = x]
$$

Например, типичная форма для множественной линейной регрессии:

$$
f(x) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_p x_p
$$

Задача аналитика в этом случае заключается в оценке параметров модели и предсказания на их основе.

Непараметрические методы основываются на самих данных, а не на параметрах. В этом случае используется понятие локальности.

Рассуждения при этом примерно такие: чему будет равняться y, если x равен...?

$$
\hat{f}(x) = \text{average}(\{ y_i : x_i = x \})
$$

Поскольку не всегда это требование выполняется, то условия чуть-чуть меняются:

$$
\hat{f}(x) = \text{average}( \{ y_i : x_i \text{ equal to (or very close to) x} \} )
$$

Одним из конкретных примером использования непараметрического подхода является метод ближайших соседей:

$$
\hat{f}_k(x) = \frac{1}{k} \sum_{i \in \mathcal{N}_k(x, \mathcal{D})} y_i
$$

![](https://bookdown.org/f100441618/bookdown-regresion/www/KNN.jpg)

### KNN в `R`

Посмотрим, как работает этот метод на данных набора `HousingData`:

```{r}
library(FNN)
library(MASS)
data(Boston)
```

Создаем тренировочную и тестируемые выборки:

```{r}
set.seed(42)
boston_idx = sample(1:nrow(Boston), size = 250)
trn_boston = Boston[boston_idx, ]
tst_boston  = Boston[-boston_idx, ]
```

```{r}
X_trn_boston = trn_boston["lstat"]
X_tst_boston = tst_boston["lstat"]
y_trn_boston = trn_boston["medv"]
y_tst_boston = tst_boston["medv"]
```

Создадим дополнительный набор для переменной `lstat` по которым мы будем предсказывать `medv` для создания графики.

```{r}
X_trn_boston_min = min(X_trn_boston)
X_trn_boston_max = max(X_trn_boston)
lstat_grid = data.frame(lstat = seq(X_trn_boston_min, X_trn_boston_max, 
                                    by = 0.01))
```

Чтобы применить метод KNN в качестве разновидности регрессионного анализа, нам понадобится функция `knn.reg()` из библиотеки `FNN`.

Ее общая архитектура следующая:

```{r, eval = FALSE}
knn.reg(train = ?, test = ?, y = ?, k = ?)
```

Данные

-   `train`: предикторы (тренировочные данные)
-   `test`: предикторы на тестовых данных, $x$, по которым мы хотели бы сделать предсказания
-   `y`: зависимая переменная (на тренировочных данных)
-   `k`: количество "соседей"

Результат:

-   вывод функции `knn.reg()` представляет собой $\hat{f}_k(x)$

```{r}
pred_001 = knn.reg(train = X_trn_boston, test = lstat_grid, y = y_trn_boston, k = 1)
pred_005 = knn.reg(train = X_trn_boston, test = lstat_grid, y = y_trn_boston, k = 5)
pred_010 = knn.reg(train = X_trn_boston, test = lstat_grid, y = y_trn_boston, k = 10)
pred_050 = knn.reg(train = X_trn_boston, test = lstat_grid, y = y_trn_boston, k = 50)
pred_100 = knn.reg(train = X_trn_boston, test = lstat_grid, y = y_trn_boston, k = 100)
pred_250 = knn.reg(train = X_trn_boston, test = lstat_grid, y = y_trn_boston, k = 250)
```

Мы сделали предсказания на основе `lstat`, для различных значений `k`. Отметим, что `250` это общее количество наблюдений в тренировочном датасете.

```{r, fig.height = 8, fig.width = 6, echo = FALSE}
par(mfrow = c(3, 2))

plot(medv ~ lstat, data = trn_boston, cex = .8, col = "dodgerblue", main = "k = 1")
lines(lstat_grid$lstat, pred_001$pred, col = "darkorange", lwd = 0.25)

plot(medv ~ lstat, data = trn_boston, cex = .8, col = "dodgerblue", main = "k = 5")
lines(lstat_grid$lstat, pred_005$pred, col = "darkorange", lwd = 0.75)

plot(medv ~ lstat, data = trn_boston, cex = .8, col = "dodgerblue", main = "k = 10")
lines(lstat_grid$lstat, pred_010$pred, col = "darkorange", lwd = 1)

plot(medv ~ lstat, data = trn_boston, cex = .8, col = "dodgerblue", main = "k = 25")
lines(lstat_grid$lstat, pred_050$pred, col = "darkorange", lwd = 1.5)

plot(medv ~ lstat, data = trn_boston, cex = .8, col = "dodgerblue", main = "k = 50")
lines(lstat_grid$lstat, pred_100$pred, col = "darkorange", lwd = 2)

plot(medv ~ lstat, data = trn_boston, cex = .8, col = "dodgerblue", main = "k = 250")
lines(lstat_grid$lstat, pred_250$pred, col = "darkorange", lwd = 2)
```

-   Оранжевые "кривые" представляют собой $\hat{f}_k(x)$ где $x$ это значения, которые мы определили через `lstat_grid`.

мы видим, что `k = 1` приводит к большой переобученности, так как `k = 1` это очень комплексная, вариативная модель. В свою очередь, `k = 250` страдает недообученностью данных, так как `k = 250` это очень простой пример с маленькой дисперсией, то есть по сути, всегда будет предсказываться одно и то же значение.

### Выбор параметра $k$

Дилемма: - низкое значение `k` = слишком сложная модель - высокое значение `k` = слишком жесткая модель.

Где золотая середина?

-мы хотим минимизировать $\hat{f}_k$:

$$
\text{EPE}\left(Y, \hat{f}_k(X)\right) = 
\mathbb{E}_{X, Y, \mathcal{D}} \left[  (Y - \hat{f}_k(X))^2 \right]
$$

Проведем тестирование на ошибку RMSE:

```{r}
rmse = function(actual, predicted) {
  sqrt(mean((actual - predicted) ^ 2))
}
```

```{r}
# создадим вспомогательную функцию, чтобы "вытащить" предсказанные значения
make_knn_pred = function(k = 1, training, predicting) {
  pred = FNN::knn.reg(train = training["lstat"], 
                      test = predicting["lstat"], 
                      y = training$medv, k = k)$pred
  act  = predicting$medv
  rmse(predicted = pred, actual = act)
}
```

```{r}
# определяем возможные значения k
k = c(1, 5, 10, 25, 50, 250)
```

```{r}
# Получаем train RMSEs
knn_trn_rmse = sapply(k, make_knn_pred, 
                      training = trn_boston, 
                      predicting = trn_boston)
# Получаем test RMSEs
knn_tst_rmse = sapply(k, make_knn_pred, 
                      training = trn_boston, 
                      predicting = tst_boston)

# Определяем лучшее значение k
best_k = k[which.min(knn_tst_rmse)]

# Найдем значения для переобученности, недообученности и для лучшего случая
fit_status = ifelse(k < best_k, "Over", ifelse(k == best_k, "Best", "Under"))
```

```{r}
# Суммируем результаты
knn_results = data.frame(
  k,
  round(knn_trn_rmse, 2),
  round(knn_tst_rmse, 2),
  fit_status
)
colnames(knn_results) = c("k", "Train RMSE", "Test RMSE", "Fit?")

# Отобразим результаты
knitr::kable(knn_results, escape = FALSE, booktabs = TRUE)
```

Вопрос на засыпку: почему при k=1 ошибка на тренировочной выборке не равна 0?

### Сравнение с линейной регрессией

Если у нас линейная зависимость: - lm() работает хорошо - knn "подгоняет автоматически"

Если связь независимая: - lm() работает плохо - или работает лучше при определенных условиях - knn "делает все автоматически"

```{r echo = FALSE, fig.height = 5, fig.width = 10}
line_reg_fun = function(x) {
  x
}

quad_reg_fun = function(x) {
  x ^ 2
}

sine_reg_fun = function(x) {
  sin(x)
}

get_sim_data = function(f, sample_size = 100, sd = 1) {
  x = runif(n = sample_size, min = -5, max = 5)
  y = rnorm(n = sample_size, mean = f(x), sd = sd)
  data.frame(x, y)
}

set.seed(42)
line_data = get_sim_data(f = line_reg_fun)
quad_data = get_sim_data(f = quad_reg_fun, sd = 2)
sine_data = get_sim_data(f = sine_reg_fun, sd = 0.5)

x_grid = data.frame(x = seq(-5, 5, by = 0.01))

par(mfrow = c(1, 3))
plot(y ~ x, data = line_data, pch = 1, col = "darkgrey")
grid()
knn_pred = FNN::knn.reg(train = line_data$x, test = x_grid, y = line_data$y, k = 10)$pred
fit = lm(y ~ x, data = line_data)
lines(x_grid$x, line_reg_fun(x_grid$x), lwd = 2)
lines(x_grid$x, knn_pred, col = "darkorange", lwd = 2)
abline(fit, col = "dodgerblue", lwd = 2, lty = 3)

plot(y ~ x, data = quad_data, pch = 1, col = "darkgrey")
grid()
knn_pred = FNN::knn.reg(train = quad_data$x, test = x_grid, y = quad_data$y, k = 10)$pred
fit = lm(y ~ x, data = quad_data)
lines(x_grid$x, quad_reg_fun(x_grid$x), lwd = 2)
lines(x_grid$x, knn_pred, col = "darkorange", lwd = 2)
abline(fit, col = "dodgerblue", lwd = 2, lty = 3)

plot(y ~ x, data = sine_data, pch = 1, col = "darkgrey")
grid()
knn_pred = FNN::knn.reg(train = sine_data$x, test = x_grid, y = sine_data$y, k = 10)$pred
fit = lm(y ~ x, data = sine_data)
lines(x_grid$x, sine_reg_fun(x_grid$x), lwd = 2)
lines(x_grid$x, knn_pred, col = "darkorange", lwd = 2)
abline(fit, col = "dodgerblue", lwd = 2, lty = 3)

```

Те же шаги, но быстрее, можно осуществить с помощью библиотеки `caret`:

```{r}
model_knn_caret <- train(
  medv ~ .,
  data = trn_boston,
  method = 'knn',
  preProcess = c("center", "scale"), tuneLength = 20 #этот параметр позволяет рассчитать разное количество соседей
)

model_knn_caret

```

```{r}
knnPredict <- predict(model_knn_caret, newdata = tst_boston)
rsq_knn_cv <- cor(knnPredict, tst_boston$medv) ^ 2
rsq_knn_cv

```

## Самостоятельная работа

1.  Провести регрессионный анализ данных об успеваемости студентов и определяющих их фактора.

Независимые переменные:

-   Hours Studied: Общее количество часов, потраченных на учебу каждым студентом.
-   Previous Scores - предшествующие результаты: Баллы, полученные студентами на предыдущих экзаменах.
-   Extracurricular Activities - Внеклассная деятельность: Участвует ли студент во внеклассных мероприятиях (да или нет).
-   Sleep Hours - Часы сна: Среднее количество часов сна студента в сутки.
-   Sample Question Papers Practiced: Количество пробных экзаменационных работ, которые студент практиковал.

Целевая переменная:

-   Performance Index: Показатель общей успеваемости каждого студента. Индекс успеваемости представляет собой академическую успеваемость студента и округляется до ближайшего целого числа. Индекс варьируется от 10 до 100, при этом более высокие значения свидетельствуют о более высокой успеваемости.

{{< iconify arcticons 1dm size=42px >}}[Скачать данные](https://github.com/domelia/rcourse/blob/e53e6c681fb30693f8c0803cfa1bff9141ac50a2/Student_Performance.csv)

2.  Провести анализ методом логистической регрессии на данных по климату. В качестве зависимой переменной будет выступать вопрос про оценку опасности проживания вблизи ледников (вопрос 19) , а в качестве объясняющих - пол, возраст и переменная проживания в определенном районе (type).
