---
title: "Основы многомерного анализа: снижение размерности"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Общие цели занятия и возможности библиотеки`factoextra`

**Многомерный анализ данных** представляет собой сложную, но очень интересную задачу, которая часто встречается в деятельности аналитика данных. Задачи анализа взаимосвязей между переменными, их структурирование в виде кластеров, факторов, компонент и представление в визуально интерпретируемом виде часто встречаются, как в фундаментальных, так и прикладных исследованиях.

Многомерный анализ основан на принципе многомерной статистики, который предполагает наблюдение и анализ более чем одной переменной за один раз. Обычно многомерный анализ используется для решения ситуаций, когда в ходе научного эксперимента выполняются несколько измерений, и взаимосвязь между этими измерениями и их структурами имеет важное значение.

Многомерный анализ охватывает большой репертуар методов, включая факторный анализ, кластерный анализ, регрессионный анализ, метод главных компонент и другие, поэтому эта обширная тема, в которой мы изучим только некоторые направления анализа.

Многомерный анализ включает в себя:

-   модели, основанные на многомерном нормальном распределении и генерализованные модели, а также распределений
-   измерение и анализ взаимосвязей между отдельными переменными
-   расчеты вероятностей на многомерных данных
-   Изучение структур и паттернов в данных

Для подобных целей на языке R разработано множество библиотек, но определенными преимуществами обладает одна из них - `factoextra`. Ее цель заключается в извлечении результатов и визуализации **эксплораторного** многомерного анализа данных, включая:

**Метод главных компонент - Principal Component Analysis (PCA)** - анализирующий количественные (непрерывные) переменные путем сокращения размерности данных без потери важной информации (например, представление множества шкал психологических тестов в виде нескольких компонент или представление множества показателей социально-экономического развития регионов в виде нескольких интегральных показателей).

**Корреспондентский анализ (анализ соответствий) - Correspondence Analysis (CA)**, выступающий расширением метода главных компонент для анализа больших таблиц сопряженности, образуемых на основе двух качественных переменных (категориальных данных)(например, продукты и их характеристики, метод хорошо подходит для вопросов с множественным выбором).

**Множественный корреспондентский анализ, множественный анализ соответствий - Multiple Correspondence Analysis (MCA)**, являющийся адаптацией метода CA к таблице данных, содержащих более двух категориальных переменных (например, взаимосвязь между занятостью, национальностью и полом).

**Множественный факторный анализ - Multiple Factor Analysis (MFA)** - предназначен для работы с данными, где переменные организованы в группы, блоки или наборы (качественных и / или количественных) переменныех. MFA позволяет обнаружить общие структуры, присутствующие во всех или некоторых наборах данных. Выполняется в два этапа. На первом этапе на каждом наборе данных проводится анализ главных компонент, а затем исходные данные в каждом наборе «нормализуются» путем деления каждого элемента на квадратный корень из собственного значения, полученного из МГК на данном наборе. Затем, нормализованные данные объединяются в единую матрицу, над которой проводится общий анализ главных компонент, а индивидуальные наборы данных проектируются на результаты этого анализа, чтобы найти общие зависимости и расхождения [@abdi2007multiple]

**Иерархический множественный факторный анализ - Hierarchical Multiple Factor Analysis (HMFA)**: расширение метода MFA, подходящий для ситуации, где данные организованы в иерархические структуры.

**Факторный анализ смешанных данных - Factor Analysis of Mixed Data (FAMD)** - особый случай MFA, предназначенный для анализа данных, содержащих как количественные, так и качественные переменные.

::: {.alert .alert-info role="alert"}
**Важно!** Сам анализ проводится с помощью других библиотек, библиотека factoextra используется для извлечения результатов и визуализации.
:::

Существует большое количество пакетов, в которых можно сделать анализ главных компонент, например, `FactoMineR`, `ade4`, `stats`, `ca`, `MASS` и `ExPosition`.

Результаты будут представлены по-разному, в зависимости от того, какие функции будут использованы. Помочь в интерпретации и визуализации полученных результатов многомерного анализа, например, кластерного анализа или результатов снижения размерности, нам как раз и поможет библиотека `factoextra`.

В рамках данного занятия мы будем использовать библиотеку `FactoMineR` [@le2008factominer] для расчетов, а библиотеку `factoextra` для извлечения и анализа результатов.

С какими методами может работать библиотека `factoextra`:

![](https://cran.r-project.org/web/packages/factoextra/readme/tools/factoextra-r-package.png)

### Как сделать правильный выбор?

Выбор метода для анализа будет зависеть от формата данных и их структуры:

Так, например, если у нас количественные данные, то нам подойдет метод главных компонент, а если качественные, то в зависимости от их количества - анализ соответствий - простой или множественный.

В случае, если данные разной природы, то нам лучше обратиться к смешанным методам - иерархическому множественному факторному анализу или факторному анализу для смешанных данных. . ![](https://cran.r-project.org/web/packages/factoextra/readme/tools/multivariate-analysis-factoextra.png)

### Установка и загрузка библиотек

Устанавливаем библиотеки:

```{r, eval=FALSE, message=FALSE, warning=FALSE}
install.packages("factoextra")
install.packages("FactoMineR")
```

Загружаем:

```{r}
library("factoextra")
library("FactoMineR")
```

## Метод главных компонент (Principal component analysis)

Метод главных компонент (PCA) позволяет суммировать и визуализировать информацию, представленную в наборе данных, где некоторые индивиды / наблюдения описываются через большое количество взаимосвязанных друг с другом количественных переменных. Каждая переменная может быть представлена в качестве отдельного измерения. Если переменных больше трех, то визуализация результатов становится затруднительным.

Метод главных компонент извлекает информацию из многомерной таблицы данных $n \times k$ и представляет ее в виде новых переменных, чье количество меньше количества исходных переменных.

Это новые переменные и называются **главные компоненты**, они являются линейной комбинацией исходных переменных. Количество главных компонент всегда меньше или равно количеству исходных переменных.

Информация набора данных соответствует дисперсии переменных, которые в него входят.

Цель МГК заключается в идентификации главных компонент, описывающих максимум дисперсии переменных. Снижение размерности заключается в определении ведущих направлений (векторов), или главных компонент, показывающих изменение данных.

Метод МГК предполагает, что эти главные направления описывают большую часть дисперсии, поэтому являются более важными.

::: {#fig-proj}
![](https://i.stack.imgur.com/Q7HIP.gif) Геометрический смысл нахождения главной компоненты.
:::

Другими словами, МГК сокращает размерность данных, путем выделения двух или трех самых важных компонент, которые мы может визуализировать графически и минимальной потерей информации.

Таким образом, главными задачами метода главных компонент являются:

-   выявление скрытой структуры в наборе данных,
-   снизить размерность данных, путем удаления статистического шума и избыточных переменных
-   идентифицировать взаимосвязанные переменные

На практике этот метод реализуется двумя возможными подходами:

-   путем разложения исходной матрицы взаимосвязей (ковариаций или корреляций) на произведение матриц собственных значений и собственных векторов
-   путем сингулярного разложения данной матрицы.

### Собственные значения и собственные вектора

Допустим наши данные представлены в матрице $\mathbf X$ размера $n \times p$, где $n$ – количество наблюдений, а $p$ – количество признаков. Наши данные должны быть центрированы, то есть в каждом столбце из каждого элемента должно быть вычтено среднее значение, так чтобы среднее значение по каждому признаку равнялось нулю.

Тогда $p \times p$ матрица ковариаций $\mathbf C$ может быть представлена как:

$$\mathbf C = \mathbf X^\top \mathbf X/(n-1)$$

Поскольку эта матрица симметричная, она может быть диагонализирована, то есть представлена в виде произведения трех матриц, в результате чего находится соответствующая диагональной матрицы собственных значений:

$$\mathbf C = \mathbf V \mathbf L \mathbf V^\top,$$

где $\mathbf V$ - это матрица собственных векторов,а $\mathbf L$ - это диагональная матрица, на диагонали которой находятся собственные значения $λ_i$, упорядоченные по убыванию:

$$\mathbf L = \begin{bmatrix}
\lambda_1 &  & & \\ 
 & \lambda_2 & & \\ 
 & & \lambda_{...} & \\
 & & & \lambda_p  \\
\end{bmatrix}$$

Собственные вектора называются **главными осями** или **главными направлениями**.

Проекции данных на главные оси собственно и называются **главными компонентами**, представляющими собой новые, трансформированные переменные.

Главная компонента $j$ находится в $j$ столбце матрицы $\mathbf {XV}$.

Координаты $i-ой$ точки данных в новом пространстве главных компонент заданы $i-й$ строкой $\mathbf XV$.

Для нахождения собственных значений решается характеристическое уравнение:

$$|\mathbf C-\mathbf \Lambda \mathbf I|\vec{x}=0,$$

предполагающее вычисление детерминанта $|\mathbf C-\mathbf \Lambda \mathbf I|$ и корней уравнения степени $n$, в результате которого находятся $\lambda_1, \lambda_2, ... \lambda_p$.

Далее, путем подстановки собственных значений в исходное уравнение, для каждого $\lambda$ находится собственный вектор.

Фундаментальным для метода главных компонент является уравнение:

$$\mathbf C=\mathbf A \mathbf {A}',$$

где $\mathbf A$ – матрица нагрузок, а $\mathbf {A}'$ - транспонированная ей матрица.

Нагрузки получаются путем умножения собственных векторов на квадрат из собственных значений:

$$\mathbf A= \mathbf V \sqrt{\mathbf \Lambda}$$

Таким образом, главную компоненту можно представить как линейную комбинацию исходных данных и компонентных нагрузок.

Пример с первой компонентой:

$$PC_k = a_{k1}X_1 + a_{k2}X_2 + … + a_{kp}X_p,$$

где a$_ij$ является нагрузкой переменной $x_j$ по компоненте $PC_i$, $x_j$ – $j-ая$ переменная матрицы признаков $\mathbf X$.

### Сингулярное разложение

Когда мы осуществляем сингулярное разложение матрицы взаимосвязей, у нас получается несколько иная картина:

$$\mathbf X = \mathbf U \mathbf S \mathbf V^\top,$$

где $\mathbf U$ - являются унитарной матрицей (столбцы которой называются левыми сингулярными векторами), $\mathbf S$ - диагональная матрица сингулярных значений $s_i$, а столбцы матрицы $\mathbf V$ называются правыми сингулярными векторами.

Правые сингулярные вектора – это и есть собственные вектора $A^\top A$, то есть между методами есть тесная взаимосвязь, и $\lambda_i = s_i^2/(n-1)$.

> {{< iconify arcticons anz size=42px >}} **Пример**: Рассмотрим пример с данными соревнований по десятиборью, в которые входят следующие дисциплины:

-   "X100m" - бег на 100 метров
-   "Long.jump" - прыжки в длину
-   "Shot.put" - толкание ядра
-   "High.jump" - прыжки в высоту
-   "X400m" - бег на 400 метров
-   "X110m.hurdle" - бег с препятсвиями
-   "Discus" - метание диска
-   "Pole.vault" - прыжок с шестом
-   "Javeline" - метание копья
-   "X1500m" - без на 1500 метров

![](https://sportsmatik.com/uploads/matik-sports-corner/matik-know-how/decathlon_1499492047_83444.jpg)

```{r}
data(decathlon2)
head(decathlon2)
```

![](images/PCA/pic3.png)

Итак, наши данные описывают выполнение атлетами испытаний в двух типах спортивных соревнований (Desctar и OlympicG). Набор содержит информацию о 27 спортсменах и 13 переменных.

**Активные спортсмены** (голубой цвел, строки 1:23) : Данные, которые будут использованы для проведения анализа методом главных компонент. **Дополнительные спортсмены** (синий цвет, строки 24:27) : Координаты по этим спортсменам будут использованы для предсказания параметров с помощью МГК по информации полученной по активным спортсменам / переменным **Активные переменные** (розовый цвет, столбцы 1:10) : переменные, используемые в МГК **Дополнительные переменные**: используются для предсказания, включая:

**Дополнительные количественные переменные** (красные): Столбцы 11 и 12 соответствуют рангу и баллам.

**Дополнительные качественные переменные** (зеленые): данные по соревнованиям (категориальная переменная).

Начнем с разделения наших данных на активные и дополнительные части:

```{r}
decathlon2.active <- decathlon2[1:23, 1:10]
head(decathlon2.active[, 1:6], 4)
```

### Стандартизация данных

Когда проводится анализ главных компонент, переменные часто стандартизируются. Эта процедура особенно рекомендуется тогда, когда переменные измеряются в разных единицах (килограммы, сантиметры и пр.).

Главная цель стандартизации - сделать переменные сопоставимыми. Обычно стандартизация происходит таким образом, чтобы переменная имела 1) стандартное отклонение равным 1, и 2) среднее значение 0.

Типичная формула для стандартизации:

$$\frac{x_i−mean(x)}{sd(x)}$$

Где $mean(x)$ это среднее значение, а $sd(x)$ стандартное отклонение (SD).

Вы уже знакомы с функцией `scale()`, которая может быть использована для стандартизации.

Однако, стандартизация может быть осуществлена сразу в процессе анализа (это действие по умолчанию).

Проведем анализ главных компонент на активных данных по декатлону:

```{r}
res.pca <- PCA(decathlon2.active, graph = FALSE)
```

На выходе функции мы получаем следующие результаты:

```{r}
print(res.pca)
```

### Визуализация и интерпретация

Мы будем использовать библиотеку `factoextra`, чтобы разобраться с результатами анализа.

Полезные функции:

-   `get_eigenvalue(res.pca)`: - извлекает собственные значения / дисперсию главных компонент
-   `fviz_eig(res.pca)`: визуализация собственных значений
-   `get_pca_ind(res.pca)`, `get_pca_var(res.pca)` - извлекает результаты для наблюдений и переменных
-   `fviz_pca_ind(res.pca)`, `fviz_pca_var(res.pca)` - визуализирует результаты по наблюдениям и переменным
-   `fviz_pca_biplot(res.pca)` - создает двойной график (биплот) по индивидам и переменным

### Собственные значения / Дисперсия

Собственные значения показывают, какую долю дисперсии переменных представляет каждая компонента. Собственные значения больше для первых компонент и меньше - для последующих.

Собственные значения могут быть использованы для определения количества главных компонент, которые достойны рассмотрения.

Для получения информации о собственных значениях и дисперсии можно использовать следующий код:

```{r}
eig.val <- get_eigenvalue(res.pca)
eig.val
```

Сумма всех собственных значений равняется 10, то есть общему количеству переменных.

Доля дисперсии, объясняемой каждым собственным значением, представлена во второй колонке.

Например, если 4.124 разделить на 10 получится 0.4124, или около 41.24% изменчивости, объясненной первой компонентой.

Кумулятивный процент представлен в последнем столбце

Видим, что четыре первых компоненты объясняют 80% дисперсии.

Собственные значения могут быть использованы для определения количества компонент. По правилу Кайзера собственное значение \> 1 обозначает, что эта главная компонента описывает дисперсию хотя бы одной переменной.

Мы можем также ограничить число компонент теми, которые описывают какую-то определенную долю дисперсии (например, 70%).

Единого правила не существует!

В нашем анализе первые три компоненты объясняют 72% дисперсии, это достаточно приемлемый результат.

Альтернативный метод заключается в рассмотрении диаграммы рассеяния собственных значений, упорядоченных по убыванию (правило локтя или каменистой осыпи).

Отбирается количество компонент выше точки, фиксирующей спад собственных значений, которые становятся очень близки друг другу (Jollife 2002, Peres-Neto, Jackson, and Somers (2005)).

Попробуем сделать такой график:

```{r}
fviz_eig(res.pca, addlabels = TRUE, ylim = c(0, 50))
```

### Результаты. Работаем с переменными

Самый простой способ извлечь информацию о переменных, это воспользоваться функцией `get_pca_var()`.

Эта функция предоставляет список матриц, содержащих результаты для активных переменных (координаты, корреляцию между переменными и осями, квадрат косинуса и вклады).

```{r}
var <- get_pca_var(res.pca)
var
```

Рассмотрим эти параметры подробнее

-   var\$coord: координаты, используемые для создания графика (проекции, нагрузки)
-   var\$cos2: квадрат косинуса - качество представленности переменных в факторном пространстве Рассчитывается как квадрат координат: var.cos2 = var.coord \* var.coord.
-   var\$contrib: вклады - показывает вклад переменной в главную компоненту (в процентах): (var.cos2 \* 100) / (total cos2 of the component). Чем важнее переменная для этой компоненты, тем выше у нее вклад.

Мы можем создать графики, основываясь либо на: 1) их качестве (cos2) или 2) на их вкладах в главные компоненты

Представим результаты:

```{r, eval=FALSE}
# Координаты
head(var$coord)
# Cos2: качество анализа
head(var$cos2)
# Вклады в компоненты
head(var$contrib)

```

Рассмотрим, как визуализировать результаты анализа по отдельным наблюдениям и переменным и как сделать выводы о взаимосвязах между ними.

### Корреляционный круг

Корреляция между переменной и главной компонентой используется в качестве координаты переменной на графике.

Представление переменных зависит от графика наблюдений, наблюдения представлены своими проекциями, тогда как переменные - корреляциями [@abdi2010principal].

```{r}
# Координаты переменных
head(var$coord, 4)
```

Представим переменные в виде графика:

```{r}
fviz_pca_var(res.pca, col.var = "black")
```

Такой график показывает взаимосвязи между всеми переменными. Он может быть интерпретирован следующим образом:

-   переменные с положительной корреляцией сгруппированы вместе;
-   переменные, имеющие отрицательную корреляцию, находятся на разных сторонах графика (квадрантах);
-   расстояние между переменными и началом координат показывает качество переменных в факторном пространстве;
-   переменные, расположенные как можно дальше от начала координат, представлены лучше.

### Качество представленности переменных

Качество представленности переменных в факторном пространстве называется `cos2` (квадрат косинуса, квадрат координаты).

Их можно «добыть» следующим образом:

```{r}
head(var$cos2, 4)
```

Мы можем визуализировать этот показатель с помощью очень интересного графика из библиотеки `corrplot` (не забываем устанавливать):

```{r}
library("corrplot")
corrplot(var$cos2, is.corr=FALSE)
```

Можно сделать и столбчатую диаграмму (по первым двум компонентам:

```{r}
fviz_cos2(res.pca, choice = "var", axes = 1:2)
```

Отметим, что: - высокие значения cos2 обозначают, что переменная хорошо описывается главной компонентой. В этом случае переменная располагается ближе к окружности на графике. - низкие значения cos2 обозначают, что переменные на очень хорошо представлены полученной компонентной структурой. В этом случае переменная - ближе к центру.

Сумма всех квадратов косинуса по всем компонентам равна 1 - то есть 100% дисперсии.

Сделаем цветной график, так, чтобы: - переменные с низкими значениями cos2 будут окрашены в голубой цвет - переменные со средними значениями cos2 будут иметь оранжевый цвет - переменные с высокими значениями cos2 будут иметь красный цвет

```{r}
fviz_pca_var(res.pca, col.var = "cos2",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), 
             repel = TRUE # Avoid text overlapping
             )
```

Можно представить не с помощью цвета, а с помощью прозрачности:

```{r}
fviz_pca_var(res.pca, alpha.var = "cos2")
```

### Оценка вклада переменной в компоненту

Вклад переменной оценку дисперсии данной главной компоненты представлен в процентном соотношении.

Переменные, которые коррелируют и с первой компонентой PC1 (Dim.1) и со второй компонентой (PC2, Dim.2) являются самыми главными в описании измечивости данных в наборе.

Переменные, которые не коррелируют ни с одной компонентой или коррелируют с последними компонентами не являются важными и могут быть исключены из анализа.

Посмотрим вклад переменных:

```{r}
head(var$contrib, 4)
```

Чем выше вклад, тем больше переменная "вкладывает" в компоненту.

Воспользуемся функцией `corrplot`, чтобы это визуализировать:

```{r}
library("corrplot")
corrplot(var$contrib, is.corr=FALSE)
```

Функция `fviz_contrib()` может быть использована для того, чтобы построить столбчатую диаграмму по данному показателю:

```{r}
# Вклад переменных в PC1
fviz_contrib(res.pca, choice = "var", axes = 1, top = 10)
# Вклад переменных в PC2
fviz_contrib(res.pca, choice = "var", axes = 2, top = 10)
```

Общий вклад переменных в обе компоненты:

```{r}
fviz_contrib(res.pca, choice = "var", axes = 1:2, top = 10)
```

Красная линия показывает некоторый средний вклад. Если бы вклад всех переменных был бы одинаковый, то каждая вносила бы по 1/10 = 10%. Соответственно, переменная, превышающая данный уровень, может считаться более важной, а менее - незначимой.

Самые важные переменные в нашем анализе - X100m, Long.jump (прыжок в длину) и Pole.vault (прыжок с шестом).

Вклад переменных может быть визуализирован следующим образом:

```{r}
fviz_pca_var(res.pca, col.var = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07")
             )
```

### Анализ по группам

Мы можем также провести анализ по отдельным группам и использовать данные о группировке в визулизации.

Так как у нас нет группирующей переменной, давайте ее создадим.

Мы разобьем переменные на три кластера, используя метод `kmeans`, и затем информацию о кластере включим в график в качестве группирующей переменной.

```{r}
# Сначала - кластерный анализ, создаем 3 кластера на основе координат

set.seed(123)
res.km <- kmeans(var$coord, centers = 3, nstart = 25)
grp <- as.factor(res.km$cluster)

# Распределяем переменные по кластерам

fviz_pca_var(res.pca, col.var = grp, 
             palette = c("#0073C2FF", "#EFC000FF", "#868686FF"),
             legend.title = "Cluster")
```

### Описание отдельных измерений

В предыдущем разделе мы проанализировали переменные с позиции их вклада в главные компоненты.

Возникает вопросы, что же эти компоненты из себя представляют.

Функция`dimdesc()` может быть использована для анализа отдельных измерений:

```{r}
res.desc <- dimdesc(res.pca, axes = c(1,2), proba = 0.05)
# Описание измерения 1
res.desc$Dim.1
```

В результатах выше, \$quanti означает результаты по количественным переменным, переменные отсортированы по значению p-value.

### Анализ отдельных наблюдений

Результаты по отдельным наблюдениям могут быть получены с помощью аналогичной функции `get_pca_ind()`. Так же, как и `get_pca_var()`, функция `get_pca_ind()` позволяет получить список матриц с результатами по наблюдениям (координаты, корреляции с осями, квадрат косинуса и вклады)

```{r}
ind <- get_pca_ind(res.pca)
ind
```

Посмотрим результаты по отдельным показателям

```{r}
# Координаты наблюдений
head(ind$coord)
# Качество
head(ind$cos2)
# Вклады
head(ind$contrib)
```

### Графики качества и вкладов

Функция `fviz_pca_ind()` помогает получить график по наблюдениям (спортсменам):

```{r}
fviz_pca_ind(res.pca)
```

Как и в случае с переменными, мы можем раскрасить наблюдения в зависимости от значений cos2:

```{r}
fviz_pca_ind(res.pca, col.ind = "cos2", 
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE # Avoid text overlapping (slow if many points)
             )
```

Отметим, что наблюдения с похожими значениями на графике располагаются близко друг к другу.

Столбчатый график по наблюдениям:

```{r}

fviz_cos2(res.pca, choice = "ind")
```

Визуализация вклада спортсменов в первые две компоненты:

```{r}
fviz_contrib(res.pca, choice = "ind", axes = 1:2)
```

### Анализ по группам

Чтобы показать разбивку по группам, давайте воспользуемся известным нам набором по ирисам.

Переменная вида “Species” будет использована в качестве группирующей переменной:

```{r}
iris.pca <- PCA(iris[,-5], graph = FALSE)
```

Создаем график:

```{r}
fviz_pca_ind(iris.pca,
             geom.ind = "point", # показываем только точки, не метки
             col.ind = iris$Species, # группирующая переменная
             palette = c("#00AFBB", "#E7B800", "#FC4E07"),
             addEllipses = TRUE, # эллипсы концентрации
             legend.title = "Группы"
             )
```

### Создаем биплот

Это, конечно, самый интересный момент. Чтобы сделать простой биплот по переменным и наблюдениям, можно воспользоваться следующим кодом:

```{r}
fviz_pca_biplot(res.pca, repel = TRUE,
                col.var = "#2E9FDF", # Цвет переменных
                col.ind = "#696969"  # Цвет наблюдений
                )
```

Отметим, что такой график целесообразен, если у нас мало переменных и наблюдений, в противном случае он может быть нечитаемым.

Отметим также, что координаты переменных и наблюдений рассчитываются не в одном пространстве. Иными словами, в биплоте для нас больше важны направления переменных, чем абсолютные значения расстояний на графике.

Грубо говоря, смысл биплота заключается в следующем:

-   наблюдение, которое находится с той же стороны, что и переменная, имеет большие значения именно по данной переменной;
-   наблюдение, которое находится с другой стороны от переменной, имеет по ней маленькие значения.

Сделаем график по ирисам:

```{r}
fviz_pca_biplot(iris.pca, 
                col.ind = iris$Species, palette = "jco", 
                addEllipses = TRUE, label = "var",
                col.var = "black", repel = TRUE,
                legend.title = "Species") 
                
```

В следующем графике мы хотим раскрасить как наблюдения, так и переменные по группам.

Чтобы разобраться с цветами, мы воспользуемся вспомогательными функциями `fill_palette()` и `color_palette()` \[ggpubr\].

```{r}
fviz_pca_biplot(iris.pca, 
                # Наблюдения по группам
                geom.ind = "point",
                pointshape = 21,
                pointsize = 2.5,
                fill.ind = iris$Species,
                col.ind = "black",
                # Цвет переменных по группам
                col.var = factor(c("sepal", "sepal", "petal", "petal")),
                
                legend.title = list(fill = "Species", color = "Clusters"),
                repel = TRUE        # чтобы не было пересечений
             )+
  ggpubr::fill_palette("jco")+      # цвет для наблюдений
  ggpubr::color_palette("npg")      # цвет для переменных
```

В следующем графике наблюдения раскрашены по группам, а переменные - по вкладу в главные компоненты (градиентная заливка).

```{r}
fviz_pca_biplot(iris.pca, 
                # Individuals
                geom.ind = "point",
                fill.ind = iris$Species, col.ind = "black",
                pointshape = 21, pointsize = 2,
                palette = "jco",
                addEllipses = TRUE,
                # Variables
                alpha.var ="contrib", col.var = "contrib",
                gradient.cols = "RdYlBu",
                
                legend.title = list(fill = "Species", color = "Contrib",
                                    alpha = "Contrib")
                )
```

### Дополнительные элементы

Помните, мы разделили наш набор на две части - активные и дополнительные переменные?

У нас есть две количественные дополнительные переменные (`quanti.sup`, колонки 11:12), и одна дополнительная качественная переменная (`quali.sup`, колонка 13) и четыре дополнительных спортсмена (`ind.sup`, строки 24:27).

Дополнительные элементы не используются в основном анализе, а их координаты предсказываются исходя из информации, полученной в результате анализа, полученного на активных данных.

Как такой анализ может быть проведен?

```{r}
res.pca <- PCA(decathlon2, ind.sup = 24:27, 
               quanti.sup = 11:12, quali.sup = 13, graph=FALSE)

```

### Количественные переменные

Результаты предсказания (координаты, корреляции и квадрат косинуса) для дополнительных переменных:

```{r}
res.pca$quanti.sup
```

Визуализация переменных (и активных, и дополнительных)

```{r}
fviz_pca_var(res.pca)
```

### Данные по наблюдениям

Предсказанные результаты по наблюдениям:

```{r}
res.pca$ind.sup
```

Визуализация наблюдений (активных и дополнительных). Мы можем представить также качественные переменные (`quali.sup`), а их координаты можно извлечь через `res.pca$quali.supp$coord`.

```{r}
p <- fviz_pca_ind(res.pca, col.ind.sup = "blue", repel = TRUE)
p <- fviz_add(p, res.pca$quali.sup$coord, color = "red")
p
```

### Данные по качественным переменным

Дополнительные качественные переменные могут также быть использованы для группировки. Это может помочь с интерпретацией данных.

Результаты по качественным переменным:

```{r}
res.pca$quali
```

Чтобы использовать эту переменную в качестве группирующей, нужно использовать аргумент `habillage` (по-французски это "одевание"), в который вносится индекс этой переменной.

```{r}
fviz_pca_ind(res.pca, habillage = 13,
             addEllipses =TRUE, ellipse.type = "confidence",
             palette = "jco", repel = TRUE) 


```

### Фильтр результатов

Если у нас много наблюдений или переменных, то можно визуализировать только некоторые из них через аргументы `select.ind` и `select.var`.

Можно сделать сортировку по имени, по косинусу или вкладу.

Примеры:

```{r}
# Отбор переменных с cos2 >= 0.6
fviz_pca_var(res.pca, select.var = list(cos2 = 0.6))
# Top 5 активных переменных с самым высоким cos2
fviz_pca_var(res.pca, select.var= list(cos2 = 5))
# Отбор по именам
name <- list(name = c("Long.jump", "High.jump", "X100m"))
fviz_pca_var(res.pca, select.var = name)
# top 5 самых важных наблюдений и переменных
fviz_pca_biplot(res.pca, select.ind = list(contrib = 5), 
               select.var = list(contrib = 5),
               ggtheme = theme_minimal())

```

## Анализ соответствий

**Анализ соответствий**, по-английски - Correspondence analysis (CA), яляется расширением метода главных компонент, предназначенным для исследования взаимосвязей между качественными (категориальными) переменными. Подобно методу главных компонент, он позволяет обобщить информацию и визуализировать данные в виде двумерных графиков.

В рамках данного занятия мы будем рассматривать пример данных, представленных в виде таблице сопряженности. Координаты, полученные в ходе анализа соответствий будут использоваться для того, чтобы в графическом виде представить ассоциации между строковых и столбцовых элементов.

Когда мы анализируем двумерную таблицу сопряженности, мы обычно стремимся сопоставить отдельные строки каким-либо столбцам. Например, выявляя различия между различыми типами поведения по полу, мы хотели бы выяснить, какие из них являются "мужскими", а какие "женскими".

Анализ соответствий дает нам такую возможность, поскольку он представляет геометрический подход к визуализации строк и столбцов двумерной таблицы в качестве точек в пространстве меньшей размерности (меньшей - значит, что в нем меньше измерений, чем количество строк или столбцов), так, что расположение точек, соответствующих строкам или столбцам, будет нам показывать их взаимосвязи. Главная цель такого анализа - в наглядном виде представить глобальное видение этих взаимосвязей, удобное для интерпретации.

Мы будем анализировать набор данных, который называется `housetasks`, включенный в библиотеку `factoextra`. Не трудно догадаться, в нем содержатся данные о ведении домашнего хозяйства.

```{r}
data(housetasks)
housetasks
```

Набор данных представляет собой таблицу частот, где в строках содержится описание 13 различных домашних дел и их распределение в супружеской чете:

-   "Laundry" - стирка
-   "Main_meal" - приготовление еды (основное блюдо)
-   "Dinner" - ужин
-   "Breakfeast" - завтрак
-   "Tidying" - уборка дома
-   "Dishes" - мытье посуды
-   "Shopping" - покупки
-   "Official" - работа с документами
-   "Driving" - вождение
-   "Finances" - финансы
-   "Insurance" - страховка
-   "Repairs" - ремонтные работы
-   "Holidays" - подготовка к праздникам

Строки - разная работа, значения в таблице - частоты, полученные в зависимости от выполнения дел:

-   wife - только женой
-   alternatively - совместно (по очереди)
-   husband - только мужем
-   jointly - совместно (вместе)

![](images/PCA/pic4.png)

### Графическое представление таблицы и тест хи-квадрат

Наша таблица не очень большая, и значит мы легко можем проанализировать профили строк и столбцов.

Очевидно,что стирка, приготовление основного блюда и ужин чаще всего - это женские занятия.

Ремонт и вождение автомобили - преимущественно мужские, а вот к праздникам супруги готовятся чаще вместе.

Давайте представим эти результаты графически с помощью библиотеки `gplots`и функции `balloonplot()`:

```{r}
library("gplots")
# 1. Переконвертируем данные в таблицу
dt <- as.table(as.matrix(housetasks))
# 2. Сделаем красивый график
balloonplot(t(dt), main ="housetasks", xlab ="", ylab="",
            label = FALSE, show.margins = FALSE)

```

Чтобы проверить данные на наличие взаимосвязи или независимости между двумя признаками, можно обратиться к тесту хи-квадрат, который показывает отклонение наблюдаемых частот от ожидаемых (равномерно распределенных):

```{r}
chisq <- chisq.test(housetasks)
chisq

```

В нашем примере мы видим, что переменные, представленные в строках и столбцах статистически значимо взаимосвязаны (p-value = r chisq\$p.value).

### Как провести анализ соответствий в R

Формула очень напоминает метод главных компонент:

```{r}
library("FactoMineR")
res.ca <- CA(housetasks, graph = FALSE)
```

Вывод функции CA() включает следующие результаты:

```{r}
print(res.ca)
```

### Визуализация и интепретация

Функции, которые используются в `factoextra` для извлечения и визуализации данных, очень напоминают те, которые использовались нами в методе главных компонент:

-   get_eigenvalue(res.ca): извлекает собственные значения / долю дисперсии, описываемые по каждому измерению (оси)
-   fviz_eig(res.ca): визуализация собственных значений
-   get_ca_row(res.ca), get_ca_col(res.ca): извлечение результатов отдельно по строкам и столбцам
-   fviz_ca_row(res.ca), fviz_ca_col(res.ca): визуализация результатов отдельно по строкам и столбцам
-   fviz_ca_biplot(res.ca): биплот

### Собственные значения / дисперсия

Вспомним, что мы рассматриваем собственные значения, чтобы определиться с количеством измерений (компонент, осей), которые мы будем рассматривать. Собственные значения и соответствующие им доли дисперсии могут быть извлечены с помощью функции `get_eigenvalue()`. У первой оси собственное значение будет максимальным и оно будет уменьшаться для последующих измерений.

```{r}
eig.val <- get_eigenvalue(res.ca)
eig.val
```

Собственные значения описывают количество информации, обобщаемой по каждой оси. Измрения упорядочены по убыванию и приводятся в соответствии с долями дисперсии.

Так же, как и в МГК, собственные значения могут быть использованы для определения оптимального количества осей для анализа.

Хотя единого "рецепта" на этот случай не существует, хороший анализ характеризуется большой долей дисперсии, описываемой через небольшое количество измерений.

В нашем анализе первые два измерения объясняют 88.6% дисперсии, и это достаточно неплохой результат.

Альтернативный метод - посмотреть на график рассеяния, построенный для упорядоченных собственных значений.

```{r}
fviz_screeplot(res.ca, addlabels = TRUE, ylim = c(0, 50))

```

Точка, после которой возникает спад (так называемый “локоть”) означает оптимальную размерность.

Мы можем также посчитать среднее собственное значение и оставить те измерения (оси), собственные значения по которым выше:

У нас 13 строк и 4 столбца, значит, если бы данные были бы случайно распределены по категориям, то ожидаемое собственное значение по каждому измерению равнялось бы: 1/(nrow(housetasks)-1) = 1/12 = 8.33%.

Точно такую же процедуру нужно провести и по столбцам: 1/(ncol(housetasks)-1) = 1/3 = 33.33%

В соответствии с рекомендациями (M. T. Bendixen 1995):

Любая ось, вносящая больше вклада, чем максимальное из этих двух процентов, должна рассматриваться в качестве важной и включаться в интерпретацию результатов анализа.

У нас максимальное значение - 33,3%

```{r}
fviz_screeplot(res.ca) +
 geom_hline(yintercept=33.33, linetype=2, color="red")
```

Как хорошо видно на графике, мы должны оставить два измерения. Отметим, что оставшиеся измерения на самом деле нам мало помогут, поскольку их вклад в объяснение изменчивости данных очень мал.

Измерения 1 и 2 объясняют около 48.7% и 39.9% общей инерции (аналог дисперсии для анализа соответствий). В совокупности это составляет 88.6%.

### График для строк

Аналогичным образом получим и визуализируем результаты для строк.

```{r}
row <- get_ca_row(res.ca)
row
```

Компоненты, содержащиеся в функции `get_ca_row()` могут быть использованы для построения графика:

-   row\$coord: координаты по каждому измерению (1, 2 and 3). Используются для построения диаграммы рассеяния.
-   row\$cos2: качесто представленности строк.
-   var\$contrib: вклад строк в процентах в определение измерений

::: {.alert .alert-info role="alert"}
Отметим, что можно визуализировать данные по строкам либо 1) по качеству их представленности в факторной структуре (cos2) или 2) по их вкладу в определение направлений.
:::

Как это сделать?

```{r}
# Координаты
head(row$coord)
# Cos2
head(row$cos2)
# Вклады в измерения
head(row$contrib)
```

### Координаты строк

Визуализируем только строки:

```{r}
fviz_ca_row(res.ca, repel = TRUE)
```

### Качество репрезентации строк

Результаты показывают, что таблица сопряженности была успешно представлена в пространстве меньшей размерности с помощью анализа соответствий, и мы смогли успешно описать 88.6% дисперции (инерции) данных.

Тем не менее, не все точки одинаково хорошо расположились вдоль двух осей (измерений).

Вспомним, что качество представленности переменной (строки) измеряется через показатель (cos2 - квадрат косинуса), что аналогично квадрату корреляции (или квадрату координаты по направлению).

Показатель cos2 измеряет степень ассоциации между строками / столбцами и конкретной осью.

Как этот показатель извлечь?

```{r}
head(row$cos2, 4)

```

Значения cos2 изменяются в диапазоне между 0 и 1. Сумма всех cos2 для всех по всем измерениям равна 1. Чем больше сумма квадратов косинусов для отдельной строки по всем измерениям, тем лучше информация, которую она представляет, описывается полученной моделью.

Давайте эту информацию представим графически:

```{r}
fviz_ca_row(res.ca, col.row = "cos2",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), 
             repel = TRUE)
```

### Альтернативный способ представления значений cos2

Мы можем визуализировать качество представленности строк с помощью библиотеки `corrplot`:

```{r}
library("corrplot")
corrplot(row$cos2, is.corr=FALSE)
```

Еще один вариант - столбчатая диаграмма:

```{r}
fviz_cos2(res.ca, choice = "row", axes = 1:2)
```

### Вклад строк в измерения

Вклад строк (в %) в определение измерений может быть извлечена путем:

```{r}
head(row$contrib)
```

Соответственно, строки с большими значениями вносят больший вклад в определение измерений и являются более важными.

Можем сделать аналогичный корплот:

```{r}
library("corrplot")
corrplot(row$contrib, is.corr=FALSE)    
```

Функция `fviz_contrib()` полезна для анализа вклада по отдельным измерениям:

```{r}
# Вклад строк в измерение 1
fviz_contrib(res.ca, choice = "row", axes = 1, top = 10)
# Вклад строк в измерение 2
fviz_contrib(res.ca, choice = "row", axes = 2, top = 10)

```

Общий вклад в два измерения

```{r}
fviz_contrib(res.ca, choice = "row", axes = 1:2, top = 10)
```

Отметим, что такие дела, как ремонт, стирка, приготовление еды и вождение являются самыми важными по первому измерению.

Тогда как выходные и ремонт - для второго измерения.

Визуализируем это:

```{r}
fviz_ca_row(res.ca, col.row = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), 
             repel = TRUE)
```

Данный график может нам дать некоторые идеи по поводу содержания осей. Очевидно, что первая ось - собственно делит наши занятия на мужские и женские - по критерию физического труда (интеллектуальной активности) - рутинного труда. Второе измерение скорее охватывает аспекты совместной деятельности или одиночной (водить машину вдвоем довольно затруднительно.

### Представление переменных по столбцам

Чтобы извлечь результаты по вариантам ответа, представленным в столбцах, понадобится функция `get_ca_col()`:

```{r}
col <- get_ca_col(res.ca)
col
```

Чтобы обратиться к отдельным компонентам вывода, нужно набрать:

```{r}
# Координаты
head(col$coord)
# Качество репрезентации
head(col$cos2)
# Вклады
head(col$contrib)
```

### Графики качества и вклада

Воспользуемся аналогичными функциями визуализации, чтобы провести анализ качества и вклада по столбцам:

```{r}
fviz_ca_col(res.ca, col.col = "cos2", 
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE)

```

```{r}
fviz_cos2(res.ca, choice = "col", axes = 1:2)
```

Судя по нашим результатам, вариант ответа "по очереди" (Alternating) представлен первыми двумя измерениями не самым наилучшим образом.

Чтобы визуализировать вклад столбцов в первые два фактора, можно использовать следующий код:

```{r}
fviz_contrib(res.ca, choice = "col", axes = 1:2)
```

Результаты - аналогичные. График еще раз подтвердил, что наибольший вклад вносят три варианта ответа - "муж", "жена" и "совместные действия".

### Делаем биплот

Аналогично тому, как мы делали биплот в ходе анализа методом главных компонент, можно визуализировать результаты анализа соответствий.

Стандартным является симметричный биплот, в котором строки (синие точки) и столбцы (красные треугольники) представлены в одном пространстве на основе главных координат, показывающих их профили. В этом случае, только расстояние между точками одной группы может быть реально проинтерпретировано, тогда как расстояния между строками и столбцами интерпретировать напрямую некорректно, можно делать только самые общие выводы и обобщения.

```{r}
fviz_ca_biplot(res.ca, repel = TRUE)
```

### Биплот, учитывающий вклад переменных

В стандартном симметричном биплоте, который мы только что сделали, довольно трудно понять, какие точки (переменные) вносят больший вклад в полученную модель.

Майкл Гринэкр (Michael Greenacre) предложил новый тип шкалирования (масштабирования), получивший название "биплот вкладов" (contribution biplot), который позволяет визуализировать эти моменты (M. Greenacre 2013). При таком отображении, точки, которые вносят маленький вклад и являющиеся не очень важными, располагаются ближе к центру координат.

Такой график можно получить, задав аргумент `map = “rowgreen”` или `map = “colgreen”`.

```{r}
fviz_ca_biplot(res.ca, map ="colgreen", arrow = c(TRUE, FALSE),
               repel = TRUE)
```

В принципе, графики - аналогичны предыдущему, в них лишь ставится больший акцент на значимости переменных.

### Описание измерений

Чтобы понять, какие строки или столбцы в большей степени связаны с главными измерениями, можно подробно исследовать информацию по каждому из них, что удобно сделать с помощью функции `dimdesc()`(FactoMineR). Строки и столбцы в выводе отсортировываются по их координатам.

```{r}
res.desc <- dimdesc(res.ca, axes = c(1,2))
```

### Пример анализа по климатическому исследованию

Проведем анализ соответствий на основе данных таблицы сопряженности между регионом исследования и вопросом V14, описывающем наиболее частые природные явления, происходящие в зимнее время.

Подключим необходимые пакеты для загрузки и обработки данных, загрузим наши данные:

```{r message=FALSE, warning=FALSE}
library(haven)
library(dplyr)
library(questionr)
df<-read_sav("База_КлимРиск_2023.sav")
```

Поскольку вопрос V14 является вопросом с множественным выбором, создадим набор:

```{r}
V14<-df %>% 
  select(contains("V14"))
```

По набору и переменной региона сделаем таблицу сопряженности, укажем `freq=FALSE`, чтобы у нас получилась таблица с абсолютными значениями, а не с процентами (хотя результаты были бы очень похожими):

```{r}
table_V14_region<-cross.multi.table(V14, df$Region, true.codes=list("да"), freq=FALSE)
```

Удалим лишний столбец (по Монголии) и строки, в которых содержатся переменные с вариантами ответа «Другое»:

```{r}
table_V14_region<-table_V14_region[1:10, 1:3]
```

Создадим подписи, обозначив имена столбцов (регионы) и строк (природные явления):

```{r}
colnames(table_V14_region)<-c("АК", "РА", "РТ")
rownames(table_V14_region)<-c("Перепад.темп", "Аном.холод", "Оттеп.", "Гололед", "Снегопад", "Лавины", "Пасм.дни", "Реч.лед", "Зажоры", "Ветра, метели")
```

Проведем корреспондентский анализ:

```{r}
res.ca <- CA(table_V14_region, graph = FALSE)
```

Посмотрим, сколько измерений у нас получилось:

```{r}
get_eigenvalue(res.ca)
```

Видим, что наши данные описываются всего двумя измерениями, первое из которых описывает 88,8% дисперсии, второе - 11,2%.

```{r}
row <- get_ca_row(res.ca)
row
```

Посмотрим показатель квадрата косинуса:

```{r}
row$cos2
```

Почти все строки максимально хорошо описываются первым измерением, кроме гололедных явлений, по второму измерению максимальные оценки, напротив у голодеда и процессов, связанных с образованием речного льда.

Посмотрим вклады:

```{r}
row$contrib
```

В первое измерение наибольший вклад вносили перепады температуры, оппепели и аномальные холода, тогда как для второго измерения важными были переменные, связанные с гололедом, аномальными холодами и сдвигами в периодах образования речного льда.

Сделаем биплот:

```{r}
fviz_ca_biplot(res.ca, repel = TRUE)
```

## Самостоятельная работа

1.  Проанализировать данные психосемантического исследования по религии.

{{< iconify arcticons 1dm size=42px >}}[Скачать данные](https://github.com/domelia/rcourse/blob/main/religions.sav)

2.  Сделать анализ главных компонент. Вывести результаты по собственным значениям и дисперсии (вместе с графиками), по координатам (корреляциям) и вкладам - по дескрипторам и ролям
3.  Сделать визуализацию - отдельно по дескрипторам, по ролям и биплот.
4.  Сделать анализ соответствий в базе данных по климату по переменной V16 (вопрос с множественным выбором по природным явлениям, происходящим в летний период).

## Источники

::: {#refs}
:::
