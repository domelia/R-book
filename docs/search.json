[
  {
    "objectID": "Regression.html",
    "href": "Regression.html",
    "title": "12  Регрессионный анализ в R",
    "section": "",
    "text": "12.1 Основы линейной регрессии\nНа предыдущих занятиях мы изучили основные типы и структуры данных, овладели основами создания простых функций на R, научились импортировать и экспортировать данные в разных форматах. Мы уже умеем проводить различные трансформации данных, визуализировать данные разного типа и проводить одномерный и двумерный анализ на категориальных данных. Мы даже справились со сложной задачей анализа данных с помощью метода главных компонент и анализа соответствий.\nМы продолжаем изучать многомерные методы, и следующий на очереди - регрессионный анализ и его отдельные разновидности.\nВ обобщенном виде уравнение регрессионного анализа может быть представлено как:\n\\[Y=X\\beta+\\epsilon,\\]\nгде \\(Y\\) - вектор значений зависимой переменной, \\(X\\) - матрица значений независимых переменных (предикторов), \\(\\beta\\) - вектор коэффициентов регрессии, используемых для подгонки к известным значениям зависимой переменной, \\(\\epsilon\\) - ошибки модели (остатки, разница между реальными и предсказанными значениями).\nПреимуществами и причинами популярности регрессионного анализа являются следующие:\nЧаще всего используются следующие виды регрессионного анализа:",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Регрессионный анализ в R</span>"
    ]
  },
  {
    "objectID": "Regression.html#основы-линейной-регрессии",
    "href": "Regression.html#основы-линейной-регрессии",
    "title": "12  Регрессионный анализ в R",
    "section": "",
    "text": "регрессионные модели могут включать множество предикторов одновременно, что позволяет оценивать вклад каждого при условии контроля над остальными параметрами;\nсуществует большое количество разновидностей регрессионного анализа, могут использоваться различные типы предикторов и зависимых переменных;\nдовольно легко интерпретировать результаты;\nдостаточно просты в применении и не слишком сложны с математической точки зрения.\n\n\n\nлинейная регрессия (зависимая переменная числовая)\nлогистическая регрессия (зависимая переменная бинарная)\nпорядковая регрессия (зависимая переменная - упорядоченная факторная)\nмультиномиальная регрессия (зависимая переменная категориальная)\n\n\n12.1.1 Загрузка данных\nВ качестве практического примера мы будем использовать учебный набор данных о взаимосвязи между объемами продаж и затратами на рекламу. Это небольшой набор, который поможет понять основные идеи метода и его различные реализации.\nКак всегда, мы начинаем с загрузки данных.\nСкачать данные\n\nlibrary(readr)\nAdvertising = read_csv(\"Advertising.csv\")\n\nПосле загрузки данных в R, первым делом нужно посмотреть сами данные, их структуру. Поскольку мы использовали функцию read_csv(), данные были загружены в формате tibble (tbl_df). Это практически датафрейм, вернее его усовершенствованная версия, в которой данные обрабатываются быстрее, и не происходит некоторых неприятных трансформаций (например, не меняются типы и имена данных).\nМы видим, что у нас всего 200 наблюдений и 4 переменных типа double (числовой формат, в котором происходит более точное округление десятичных знаков до 16 знаков после запятой - с 64-битной, то есть двойной точностью).\nВ нашем наборе переменная Sales (Продажи) будет являться зависимой переменной, и мы будем пытаться выявить взаимосвязи между продажами и тремя другими - независимыми переменными: TV, Radio, и Newspaper, обозначающими, соответственно, затраты на рекламу на телевидении, радио и в газетах.\n\n\n12.1.2 Предварительная визуализация данных\nПосле рассмотрения структуры данных, следующий шаг - это визуализация. Поскольку у нас только количественные (не категориальные) переменные, лучший способ их представить - это диаграммы рассеяния, которые мы можем сделать для каждого индивидуального предиктора.\nНапример, для рекламы на телевидении\n\nplot(Sales ~ TV, data = Advertising, col = \"dodgerblue\", pch = 20, cex = 1.5,\n     main = \"Продажи vs Реклама на телевидении\")\n\n\n\n\n\n\n\n\nСамостоятельная работа: сделайте аналогичные графики для других переменных.\nЧтобы сделать все графики сразу, можно воспользоваться функцией pairs().\n\npairs(Advertising)\n\n\n\n\n\n\n\n\nЧасто нам интересно посмотреть взаимосвязи только между зависимой переменной и предиктором, а функция pairs() выдает много лишнего.\nФункция featurePlot() из библиотеки caret (Classification And REgression Training), подходит для этой цели гораздо лучше.\n\nlibrary(caret)\nfeaturePlot(x = Advertising[ , c(\"TV\", \"Radio\", \"Newspaper\")], y = Advertising$Sales)\n\n\n\n\n\n\n\n\nМы видим, что есть явный рост продаж по мере увеличения рекламы на радио и телевидении, тогда как связь с рекламой в газетах не так очевидна.\n\n\n12.1.3 Простая линейная регрессия и функция lm()\nДавайте построим простую линейную модель для продаж, в которой в качестве предиктора будут выступать затраты на телевизионную рекламу.\n\nmod_1 = lm(Sales ~ TV, data = Advertising)\n\n\n12.1.3.1 Общие результаты и тестирование гипотез\nФункция summary() позволяет вывести на экран информацию о модели, полученную с помощью функции lm(), которая может быть полезной для тестирования гипотез, касающихся предикторов и оценки значимости регрессионных коэффициентов.\n\nsummary(mod_1)\n\n\nCall:\nlm(formula = Sales ~ TV, data = Advertising)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.3860 -1.9545 -0.1913  2.0671  7.2124 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 7.032594   0.457843   15.36   &lt;2e-16 ***\nTV          0.047537   0.002691   17.67   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.259 on 198 degrees of freedom\nMultiple R-squared:  0.6119,    Adjusted R-squared:  0.6099 \nF-statistic: 312.1 on 1 and 198 DF,  p-value: &lt; 2.2e-16\n\n\nДавайте разбираться!\nВывод начинается с повторения регрессионного уравнения под заголовком Call: lm(formula = Sales ~ TV, data = Advertising).\nЗатем в модели приводится распределение остатков. Остатки должны иметь нормальное распределение с абсолютными значениями минимума и максимума, также как и квартилями очень близкими друг к другу, что предполагает их примерно одинаковое расстояние от центра распределения.\nВ нашем случае это правило выполняется.\nСледующая часть вывода содержит таблицу с коэффициентами.\nЧтобы понять, что они означают и каким образом получаются, приведем формулу регрессии, но для случая с одной переменной:\n\\[y_i=\\alpha+\\beta x_i+\\varepsilon_i,\\]\nгде \\(y\\) - зависимая переменная, \\(i\\) - единица анализа, \\(\\alpha\\) - интерцепт (константа), \\(\\beta\\) - коэффициент регрессии, \\(x\\) - независимая переменная и \\(\\varepsilon\\) - ошибка.\nКогда мы работаем с выборочными данными, формула изменяется, так как вместо истинных значений у нас будут оценки:\n\\[\\hat{y}=\\hat{\\alpha}+\\hat{\\beta}x\\]\nОценка для интерцепта (\\(\\alpha\\)) - значение \\(y\\) когда \\(x = 0\\). В геометрическом смысле это точка пересечения регрессионной прямой с осью \\(OY\\). Иногда интерцепт может иметь смысл и подлежит интерпретации, но часто он может принимать несуществующие значения, выходящие за рамки возможных значений переменных, не описывается и не интерпретируется (допустим, мы пытаемся выявить зависимость веса от роста, получается, что интерцепт нам покажет, чем равен вес, когда рост равен нулю, что не имеет смысла).\nВ нашем примере интерцепт - это среднее значение продаж при нулевых затратах на рекламу.\nДалее следуют коэффициенты для предикторов. Для каждого предиктора коэффициент обозначает ожидаемое изменение в зависимой переменной при изменении предиктора на одну единицу.\nГеометрический смысл beta-beкоэффициента: это угол наклона регрессионной прямой:\n\nЧтобы найти коэффициенты \\(\\alpha\\) и \\(\\beta\\) нам нужно понять, как они вычисляются. Начнем с коэффициентов \\(\\beta\\):\n\\[\\hat{\\beta}=\\frac{cov(x,y)}{var(x)}\\]\nЧтобы узнать коэффициент \\(\\beta\\) для рекламы на телевидении, нам необходимо найти ковариацию между рекламой и продажами и дисперсию затрат на рекламу:\n\ncov_x_y&lt;-cov(Advertising$TV, Advertising$Sales)\nvar_x&lt;-var(Advertising$TV)\nbeta_x&lt;-cov_x_y/var_x\nbeta_x\n\n[1] 0.04753664\n\n\nКоэффициент показывает, на сколько увеличатся продажи, при увеличении затрат на рекламу на единицу.\nПосчитав коэффициент \\(\\beta\\), мы можем перейти к \\(\\alpha\\):\n\\[\\hat{\\alpha}=\\bar{y} - \\hat{\\beta}\\bar{x}\\]\nДля того, чтобы вычислить его вручную, нам необходимо знать средние значения \\(\\bar{y}\\) и \\(\\bar{x}\\).\n\ny_bar&lt;-mean(Advertising$Sales)\nx_bar&lt;-mean(Advertising$TV)\nintercept&lt;-y_bar-beta_x*x_bar\nintercept\n\n[1] 7.032594\n\n\nВсе сходится!\nТеперь мы можем посчитать предсказанные значения по продажам на основе нашей модели:\n\nyhat &lt;- intercept + beta_x * Advertising$TV\nhead(yhat) \n\n[1] 17.970775  9.147974  7.850224 14.234395 15.627218  7.446162\n\n\nКогда мы проводим регрессионный анализ, один из главных вопросов, которые мы себе задаем - можно ли на основе знаний о переменной \\(x\\) понять, как ведет себя \\(y\\). Говоря формальным языком, мы исследуем эту взаимосвязь, оценивая остаточные значения, ассоциированные с коэффициентами \\(\\alpha\\) и \\(\\beta\\), на основе тестирования гипотез о том, отличаются ли данные коэффициенты от нуля.\nНапример:\n\n\\(H_0: \\beta=0\\)\n\\(H_1: \\beta \\neq 0\\)\n\nИными словами, если коэффициент \\(\\beta\\) равен нулю, то переменная \\(x\\) никак не объясняет \\(y\\), так как \\(0 \\times x=0\\).\nПоскольку мы основываемся на допущении о том, что остатки имеют нормальное распределение, это нам позволяет рассчитать t-статистику для коэффициентов и проверить их статистическую значимость.\nХотя R все делает автоматически, давайте разберемся, как это происходит.\nПосчитаем разницу между реальными и предсказанными значениями по продажам:\n\nres &lt;- Advertising$Sales - yhat\nhead(res)\n\n[1]  4.1292255  1.2520260  1.4497762  4.2656054 -2.7272181 -0.2461623\n\n\nДалее мы должны посчитать разброс наблюдений вокруг регрессионной прямой, которую мы только что воспроизвели, а также стандартную ошибку остатков, которая используется для оценки ошибок коэффициентов и их статистической значимости.\nЧтобы найти стандартную ошибку остатков, нам требуется:\n\nнайти сумму квадратов отклонений остатков (\\(RSS\\))\nнайти количество степеней свободы (\\(df=N-2\\))\n\n\nres.sqr &lt;- res^2\nRSS &lt;- sum(res.sqr, na.rm=T)\ndf &lt;- length(Advertising$Sales) - 2\ndf\n\n[1] 198\n\nRSE &lt;- sqrt(RSS / df)\nRSE\n\n[1] 3.258656\n\n\nСобственно говоря, это мы и видим в выводе: Residual standard error: 3.259 on 198 degrees of freedom.\nЗная стандартную ошибку остатков, мы можем вычислить стандартные ошибки для наших коэффициентов. Для этого, мы должны сначала вычислить сумму квадратов отклонений по независимой переменной (затраты на рекламу ТВ):\n\nTSSx &lt;- sum((Advertising$TV - x_bar)^2)\nTSSx\n\n[1] 1466819\n\n\nЧтобы найти стандартную ошибку коэффициента \\(\\beta\\), нужно стандартную ошибку остатков разделить на квадратный корень из суммы квадратов отклонений по переменной \\(x\\):\n\\[SE_{\\beta}=\\frac{RSE}{\\sqrt{TSS_x}}\\]\n\nSEB &lt;- RSE / sqrt(TSSx)\nSEB\n\n[1] 0.002690607\n\n\nДля интерцепта алгоритм несколько отличается:\n\\[SE_{\\alpha}=RSE*\\sqrt{\\frac{1}{N}+\\frac{\\bar{x^2}}{TSS_x}}\\]\n\nSEA &lt;- RSE * sqrt((1 / 200)+(x_bar^2 / TSSx))\nSEA\n\n[1] 0.4578429\n\n\nЗная стандартные ошибки, мы можем теперь посчитать соответствующие t-статистики, чтобы оценить, отличаются ли наши коэффициенты от нуля. Для этого нужно значения коэффициентов разделить на их стандартные ошибки.\nДля коэффициента по переменной телевизионной рекламы:\n\nt.B &lt;- beta_x / SEB\nt.B\n\n[1] 17.66763\n\n\nДля интерцепта:\n\nt.A &lt;- intercept / SEA\nt.A\n\n[1] 15.36028\n\n\nЭти значения измеряются в стандартных отклонениях и показывают, насколько далеко наши коэффициенты находятся от нуля. Значения 17,6 и 15,4 очень большие, следовательно, наши коэффициенты статистически значимы, что и подтверждают соответствующие p-значения из вывода: 15.36   &lt;2e-16 *** и 17.67   &lt;2e-16 ***.\n\n\n12.1.3.2 Показатели качества модели\nКакие еще важные показатели мы должны принять во внимание, когда мы анализируем результаты регрессионного анализа?\nОбратимся к оставшейся части вывода.\nОсновной мерой, показывающей, насколько хорошо регрессионная модель объясняет данные, является коэффициент детерминации - \\(R^2\\). Для того, чтобы найти \\(R^2\\), нужны следующие промежуточные вычисления о некоторых компонентах дисперсии зависимой переменной:\n\nсумме квадратов остатков (\\(RSS\\))\nобщей сумме квадратов отклонений от среднего (\\(TSS\\))\nсумме квадратов отклонений, объясненной моделью (\\(ESS\\))\n\n\nTSS &lt;- sum((Advertising$Sales - y_bar)^2)\nTSS\n\n[1] 5417.149\n\nESS &lt;- TSS - RSS\nESS\n\n[1] 3314.618\n\nr.sqr &lt;- ESS / TSS\nr.sqr\n\n[1] 0.6118751\n\n\nТаким образом, 61,2% дисперсии зависимой переменной объясняется регрессионной моделью.\nНесмотря на то, что показатель \\(R^2\\) является довольно информативным, у него есть один существенный недостаток: он имеет свойство неоправданно возрастать, при включении дополнительных переменных в анализ, даже если они не оказывают существенного влияния на зависимую переменную. Иными словами, чем более комплексной будет модель, тем выше будет \\(R^2\\), что не очень хорошо.\nПоэтому вместо обычного \\(R^2\\) в качестве более точной оценки качества модели используется скорректированный показатель - adjusted\\(R^2\\). Проблему множественных предикторов этот показатель решает, путем внесения «наказаний» (пенальти) за включение в модель дополнительных переменных. Чтобы найти скорректированный \\(R^2\\) используется формула:\n\\[1-\\frac{(1-R^{2})(n-1)}{n-k-1},\\]\nгде \\(k\\) - количество предикторов в модели, не считая интерцепта (A).\n\nr.sqr.adj&lt;-1-(((1 - r.sqr) * (200 - 1)) / (200 - 1 - 1))\nr.sqr.adj\n\n[1] 0.6099148\n\n\nПоскольку у в модели один предиктор, значение уменьшилось незначительно.\nУ нас остался нерассмотренным только один показатель из вывода - F-статистика. F-критерий является «глобальным» тестом, показывающим, насколько лучше наша модель базовой модели - такой, в которую включен только один интерцепт.\nЕще одна интерпретация: этот тест показывает, что в нашей модели есть хотя бы один значимый предиктор.\nВ нашем выводе F-statistic: 312.1 on 1 and 198 DF,  p-value: &lt; 2.2e-16, что указывает на то, что модель с предиктором существенно лучше базовой модели объясняет зависимую переменную.\n\n\n12.1.3.3 Сравнение нескольких моделей\nДопустим, мы хотим создать несколько двумерных моделей и сравнить их. Это возможно с помощью функции mtable() из пакета memisc (Management of Survey Data and Presentation of Analysis Results - управление данными исследований и презентация результатов анализа). Для демонстрации создадим три модели, иллюстрирующие взаимосвязь между продажами и каждым типом рекламы.\nПервая модель у нас уже есть, создадим две других:\n\nmod_2 = lm(Sales ~ Radio, data = Advertising)\nmod_3 = lm(Sales ~ Newspaper, data = Advertising)\n\nБлагодаря функции ‘mtable()’ мы можем создать таблицу, в которой сведем всю важную информацию по всем трем моделям:\n\nlibrary(memisc)\nmtable&lt;-mtable(mod_1, mod_2, mod_3)\nmtable\n\n\nCalls:\nmod_1: lm(formula = Sales ~ TV, data = Advertising)\nmod_2: lm(formula = Sales ~ Radio, data = Advertising)\nmod_3: lm(formula = Sales ~ Newspaper, data = Advertising)\n\n===================================================\n                 mod_1       mod_2       mod_3     \n---------------------------------------------------\n  (Intercept)    7.033***    9.312***   12.351***  \n                (0.458)     (0.563)     (0.621)    \n  TV             0.048***                          \n                (0.003)                            \n  Radio                      0.202***              \n                            (0.020)                \n  Newspaper                              0.055**   \n                                        (0.017)    \n---------------------------------------------------\n  R-squared      0.612       0.332       0.052     \n  N            200         200         200         \n===================================================\n  Significance: *** = p &lt; 0.001; ** = p &lt; 0.01;   \n                * = p &lt; 0.05  \n\n\nВидим, что хотя во всех моделях интерцепты и коэффициенты предикторов являются значимыми, показатель \\(R^2\\) максимально высок в модели, где в качестве объясняющей переменной используется показатель затрат на рекламу на телевидении.\n\n\n\n12.1.4 Множественная регрессия\nРассмотрим случай, когда количество предикторов больше одного, то есть наша модель является моделью уже не простой, а множественной регрессии.\nФормула для нескольких предикторов приобретает вид:\n\\[\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1}x_1 + \\hat{\\beta_2}x_2 ...+... \\hat{\\beta_n}x_n\\]\nСинтаксис в R аналогичен тому, что мы использовали для двумерной регрессии. Создадим модель, в которую включим сразу все независимые переменные.\n\nmod_4 = lm(Sales ~ TV + Radio + Newspaper, data = Advertising)\n#mod_4 = lm(Sales ~ ., data = Advertising) можно использовать и такой синтаксис\n\nКроме функции summary() красивую таблицу с результатами можно создать с помощью функции tab_model() из библиотеки sjPlot\n\nsjPlot::tab_model(mod_4)\n\n\n\n\n \nSales\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n2.94\n2.32 – 3.55\n&lt;0.001\n\n\nTV\n0.05\n0.04 – 0.05\n&lt;0.001\n\n\nRadio\n0.19\n0.17 – 0.21\n&lt;0.001\n\n\nNewspaper\n-0.00\n-0.01 – 0.01\n0.860\n\n\nObservations\n200\n\n\nR2 / R2 adjusted\n0.897 / 0.896\n\n\n\n\n\n\n\nРезультаты показывают, что значимыми являются только коэффициенты для радио- и телерекламы, тогда как реклама в газетах не является значимым фактором, определяющим продажи.\nСкорректированный коэффициент детерминации (Adjusted R-squared), показывает, что эта модель гораздо лучше, чем любая модель с одним предиктором, и объясняет 89,6% дисперсии.\nДополнительно, в целях сравнения, давайте создадим более простую модель, без газет.\n\nmod_5 = lm(Sales ~ TV + Radio, data = Advertising)\nsjPlot::tab_model(mod_5)\n\n\n\n\n \nSales\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n2.92\n2.34 – 3.50\n&lt;0.001\n\n\nTV\n0.05\n0.04 – 0.05\n&lt;0.001\n\n\nRadio\n0.19\n0.17 – 0.20\n&lt;0.001\n\n\nObservations\n200\n\n\nR2 / R2 adjusted\n0.897 / 0.896\n\n\n\n\n\n\n\nКак видим, коэффициент детерминации не изменился (что неудивительно, ведь у удаленной переменной коэффициент регрессии равнялся нулю).\n\n12.1.4.1 Сравнение моделей с помощью дисперсионного анализа\nЧтобы сравнить, какая модель работает лучше, можно применить функцию anova(), запускающую дисперсионный анализ. В нашем случае, мы будем сравнивать модель со всеми предикторами mod_1 с сокращенной моделью mod_0. Наша задача будет заключаться в том, чтобы понять, какую роль играет переменная газетной рекламы в аддитивной модели.\n\nanova(mod_4, mod_5)\n\nAnalysis of Variance Table\n\nModel 1: Sales ~ TV + Radio + Newspaper\nModel 2: Sales ~ TV + Radio\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1    196 556.83                           \n2    197 556.91 -1 -0.088717 0.0312 0.8599\n\n\nМы видим, что разница между моделями в одну степень свободы (1 параметр - как раз наша переменная о рекламе в газетах).\n\nЧисло степеней свободы (df) − важный показатель регрессионного анализа, используемый в формулах метрик, показывающих качество модели:\n\nRes.Df - число степеней свободы, рассчитываемых для остатков (разности между предсказанными и реальными значениями).\nRes.Df - количество наблюдений - количество оцениваемых параметров.\nModel 1: Sales ~ TV + Radio\ndf= 197= 200-3 (2 предиктора + константа)\nModel 2: Sales ~ TV + Radio + Newspaper\ndf= 196= 200-4 (2 предиктора + константа)\n\n\nРезультаты дисперсионного анализа показывают, что качество модели не поменялось, и значит мы можем использовать более лаконичную (сокращенную) модель.\n\n\n12.1.4.2 Предсказание значений зависимой переменной для новых данных\nОбычно у регрессионного анализа две основные задачи - объяснение взаимосвязи между переменными и предсказание новых (неизвестных) значений зависимой переменной на основе модели. Для осуществления прогноза чаще всего используется функция predict(), обладающая большой гибкостью (может применяться с различными методами моделирования и типами данных).\nЕсли эту функцию использовать к модели, созданной на основе функции lm(), то она будет рассчитывать предсказанные значения для каждого наблюдения.\nДавайте посмотрим первые десять.\n\nhead(predict(mod_5), n = 10)\n\n        1         2         3         4         5         6         7         8 \n20.555465 12.345362 12.337018 17.617116 13.223908 12.512084 11.718212 12.105516 \n        9        10 \n 3.709379 12.551697 \n\n\nОтметим, что эффект функции predict()будет зависеть от того, какие данные даются на входе. Наша модель относится к классуlm, поэтому predict() запускает функциюpredict.lm() Если нам нужно что-то другое, можно посмотреть подробности с помощью ?predict.lm().\nМы также можем сгенерировать новые данные, и попробовать посчитать зависимую переменную на них.\nДавайте создадим новый набор с идентичными именами переменных.\n\nnew_obs = data.frame(TV = 150, Radio = 40, Newspaper = 1)\n\nТеперь мы можем использовать predict(), чтобы посчитать оценки и доверительные интервалы для новых данных.\nЕсли указать только модель и источник данный, R выдаст точечную оценку, то есть “предсказанное значение” \\(\\hat{y}\\).\n\npredict(mod_5, newdata = new_obs)\n\n       1 \n17.30409 \n\n\nЕсли указать дополнительно аргументinterval со значением \"confidence\", R покажет также 95% доверительные интервалы для среднего значения по данному наблюдению.\n\npredict(mod_1, newdata = new_obs, interval = \"confidence\")\n\n       fit      lwr      upr\n1 14.16309 13.70842 14.61776\n\n\nКроме того, мы можем изменить уровень и выбрать не доверительные интервалы, а предсказательные интервалы (доверительные интервалы прогноза). В чем отличие?\nПредсказательные интервалы показывают, в каком диапазоне значений будет находиться будущее наблюдение, тогда как доверительные интервалы показывают вероятный диапазон, в котором будет находится какой-либо статистический параметр, например, среднее в генеральной совокупности.\nПоскольку предсказательные интервалы рассчитываются в ситуации большей неопределенности, то они обычно шире, чем доверительные интервалы.\n\npredict(mod_1, newdata = new_obs, interval = \"prediction\", level = 0.95)\n\n       fit      lwr      upr\n1 14.16309 7.720898 20.60528\n\n\n\n\n12.1.4.3 Диагностика модели и оценка влияния наблюдений на результаты\nВ R доступны несколько функций, позволяющих оценить, насколько полученная модель хорошо воспроизводит исходные данные, и как различные наблюдения вносят вклад в предсказательные способности этой модели:\n\nresid() выдает остаток (разность между предсказанным и реальным значением)\nhatvalues() показывает leverage - отклонение в значениях по независимым переменным по каждому наблюдению. Данный показатель важен для понимания, как экстремальные значения по независимым переменным могут повлиять на результаты анализа.\n\n\nЧто такое hat - значения? hat - по-английски «шляпа», а также диакритический знак «циркумфлекс» (\\(\\hat{ }\\)), с помощью которого обозначаются значения зависимой переменной, предсказанные с помощью регрессионной модели.\nЭти предсказанные значения обозначаются как \\(\\hat{y}\\) и рассчитываются по формуле:\n\\[\\hat{y}=Xb\\]\nДля коэффициентов линейной регрессии используется следующая формула:\n\\[b = (X^{'}X)^{-1}X^{'}y\\]\nСледовательно, мы можем переписать уравнение для предсказанных значений как:\n\\[\\hat{y}=X(X^{'}X)^{-1}X^{'}y\\]\nТаким образом, предсказанные значения могут быть получены путем умножения \\(n \\times 1\\) вектора \\(y\\), содержащего наблюдаемые значения на \\(n \\times n\\) матрицы \\(H\\):\n\\[H=X(X^{'}X)^{-1}X^{'}\\]\nИли, более лаконично:\n\\[\\hat{y}=Hy\\]\nМатрица \\(H\\) часто называется hat-matrix - «матрица в шляпе», а ее диагональные значения как раз и являются значениями левериджа.\n\n\nrstudent() стьюдентизированные остатки по каждому наблюдению (остаток в регрессионной модели деленный на ее скорректированную стандартную ошибку)\ncooks.distance() рассчитывает важность каждого наблюдения\n\n\nddf &lt;- data.frame(residuals=residuals(mod_5), rstandard=rstandard(mod_5), rstudent=rstudent(mod_5), leverage=hatvalues(mod_5), cookd=cooks.distance(mod_5))\n\nКак мы можем это использовать?\nНапример, мы можем отобрать наблюдения, чьи стандартизированные остатки отклоняются более, чем на 2 стандартных отклонения в обе стороны:\n\nlibrary(dplyr)\nfilter(ddf, abs(rstandard) &gt; 2 | abs(rstudent) &gt; 2)\n\n    residuals rstandard  rstudent   leverage      cookd\n6   -5.312084 -3.215600 -3.295069 0.03465182 0.12372141\n26  -3.608021 -2.172231 -2.193135 0.02410291 0.03884690\n36  -4.192801 -2.530272 -2.565881 0.02870293 0.06306486\n79  -3.489203 -2.096437 -2.114833 0.02013318 0.03010147\n127 -3.990963 -2.403633 -2.433473 0.02479191 0.04895846\n131 -8.797700 -5.303997 -5.714235 0.02678270 0.25806539\n179 -4.213844 -2.541689 -2.577850 0.02772290 0.06140056\n\n\n\n Задание: проанализируйте в таблице исходных данных наблюдения с указанными номерами. Какие выводы можно сделать?\n\nВторой важный момент: анализ показателей leverage и Cook's distance.\nЗамечательная вещь по поводу левериджа заключается в том, что его значения помогают выявить экстремальные значения \\(x\\), которые могут влиять на результаты регрессионного анализа. Каким образом? Мы должны понять, какое значение левериджа нужно признать большим, то есть соответствующим значениям \\(x\\), расположенным максимально далеко от средних значений по всем другим наблюдениям. Общим является правило, согласно которому, любое наблюдение, чье значение левериджа в три раза превышает среднее значение, является нетипичным / странным / достойным внимания:\n\\[\\bar{h}=\\frac{\\sum_{i=1}^{n}h_{ii}}{n}=\\frac{p}{n}\\]\nИными словами, если:\n\\[h_{ii} &gt;3\\left( \\dfrac{p}{n}\\right),\\]\nто мы должны обратить внимание на это наблюдение. Сумма всех значений левериджа равняется количеству параметров модели: 3 - два предиктора + интерцепт (константа).\n\nhat_max = 3*3/200\nfilter(ddf, leverage&gt;hat_max)\n\n[1] residuals rstandard rstudent  leverage  cookd    \n&lt;0 строк&gt; (или 'row.names' нулевой длины)\n\n\nЧто мы видим? Мы видим, что по модели 5 у нас нет таких наблюдений, чей леверидж превышал бы максимально возможное значение.\nМы также можем отсортировать наблюдения по расстоянию Кука, чтобы понять, какие наблюдения являются наиболее влиятельными:\n\\[D_i=\\frac{(y_i-\\hat{y}_i)^2}{(k+1) \\times MSE}\\left[ \\frac{h_{ii}}{(1-h_{ii})^2}\\right],\\]\nгде \\(MSE\\) - среднеквадратическая ошибка регрессии, а \\(h_{ii}\\) - значения левериджа.\n\narrange(ddf, desc(cookd))[1:6,]\n\n    residuals rstandard  rstudent   leverage      cookd\n131 -8.797700 -5.303997 -5.714235 0.02678270 0.25806539\n6   -5.312084 -3.215600 -3.295069 0.03465182 0.12372141\n36  -4.192801 -2.530272 -2.565881 0.02870293 0.06306486\n179 -4.213844 -2.541689 -2.577850 0.02772290 0.06140056\n127 -3.990963 -2.403633 -2.433473 0.02479191 0.04895846\n26  -3.608021 -2.172231 -2.193135 0.02410291 0.03884690\n\n\nРекомендуется исключать из анализа наблюдения, расстояние Кука для которых превышает 1. В нашем анализе таких нет, но вот наблюдения 131 и 6 являются все-таки подозрительными, как имеющие наиболее расстояние Кука и самые большие остатки.\nАналогичную информацию можно получить с помощью специальных графиков:\n\npar(mfrow = c(2, 2))\nplot(mod_5)\n\n\n\n\n\n\n\n\nЧто показывают графики?\n1. Residuals vs Fitted (Остатки vs предсказанные значения)\nЭтот график показывает, есть ли в остатках регресии какие-либо нелинейные паттерны. Такое может случиться, если между предикторными переменными и зависимой переменной имеются нелинейные взаимосвязи, соответственно если эта нелинейность возникает на графике, значит модель плохо воспроизводит эти отношения. Если мы видим, что остатки равномерно распределены вокруг линии предсказанных значений без каких-либо серьезных колебаний, это хороший знак, значит у нас в модели таких нелинейных взаимосвязей нет. На нашем графике есть еле заметный «прогиб», но четким паттерном его назвать вряд ли возможно.\n2. Normal Q-Q residuals\nДанный график показывает, что остатки нормально распределены (то есть маленьких остатков много и их среднее значение приближается к нулю, в больших остатков мало). У нас с нормальностью остатков практически все в порядке, если не считать постоянно выбивающееся наблюдение 131.\n3. Scale-Location\nДанный график позволяет протестировать допущение о гомогенности дисперсии остатков (гомоскедастичности). Если мы видим, что остатки распределены вдоль линии равномерно, и их форма не напоминает «фен», то все хорошо.\n4. Residuals vs Leverage\nНу и, наконец, последний график визуализирует самые влиятельные наблюдения - одновременно через леверидж и расстояние Кука. Сомнительные наблюдения на всех графиках обозначены цифрами.\nБиблиотека olsrr (Tools for Building OLS Regression Models) также содержит несколько полезных функций, которые могут помочь в выявлении таких наблюдений.\n\nlibrary(olsrr)\nols_plot_cooksd_bar(mod_1)\n\n\n\n\n\n\n\n\n\nols_plot_cooksd_chart(mod_1)\n\n\n\n\n\n\n\n\n\nols_plot_dfbetas(mod_1)\n\n\n\n\n\n\n\n\nЧто дальше? Мы выяснили, что некоторые наблюдения являются нетипичными, что может приводить к искаженным вычислениям. Но мы можем попробовать удалить переменные, которые вызвали наибольшее количество вопросов, и сравнить результаты.\n\nAdvertising2&lt;-Advertising[-c(6,131),]\nmod_6 = lm(Sales ~ TV + Radio, data = Advertising2)\nmtable&lt;-mtable(mod_5, mod_6)\nmtable\n\n\nCalls:\nmod_5: lm(formula = Sales ~ TV + Radio, data = Advertising)\nmod_6: lm(formula = Sales ~ TV + Radio, data = Advertising2)\n\n=======================================\n                 mod_5       mod_6     \n---------------------------------------\n  (Intercept)    2.921***    3.052***  \n                (0.294)     (0.265)    \n  TV             0.046***    0.044***  \n                (0.001)     (0.001)    \n  Radio          0.188***    0.195***  \n                (0.008)     (0.007)    \n---------------------------------------\n  R-squared      0.897       0.915     \n  N            200         198         \n=======================================\n  Significance: *** = p &lt; 0.001;   \n                ** = p &lt; 0.01;   \n                * = p &lt; 0.05  \n\n\nПосле удаления экстремальных наблюдений, качество модели улучшилось (скорректированный \\(R^2=91.5%\\)), хотя общие выводы аналогичны.\n\n\n12.1.4.4 Мультиколлинеарность\nЕще один сложный термин))) Что такое мультиколлинеарность? Мультиколлинеарность случается тогда, когда один предиктор может предсказывать другой. Иными словами, мы хотели бы, чтобы предикторы хорошо предсказывали поведение зависимой переменной, но не друг друга, и если такое случается, то это и называется мультиколлинеарностью. Хотя слишком высокая мультиколлинеарность является редкостью, проверка на нее является одной из стандартных процедур регрессионного анализа. Отметим, что проблема мультиколлинеарности является важной, когда мы исследуем важность предикторов, пытаемся на основе интерпретации коэффициентов регрессии обнаружить значимые закономерности (например, доказать, что повышение уровня образования может привести к значительному увеличению доходов или что по мере развития ассоциаций между гражданами увеличивается уровень институционального доверия). Если же первостепенной задачей моделирования является предсказание (как бывает во многих задачах машинного обучения), то проблема мультиколлинеарности не является релевантной, и ее можно проигнорировать.\nКак мы можем проверить, если в нашей модели чрезмерная мультиколлинеарность?\nСамый простой способ - посмотреть на коэффициенты корреляции между предикторами:\n\nAdvertising %&gt;%\n  dplyr::select(TV, Radio, Newspaper) %&gt;%\n  cor()\n\n                  TV      Radio  Newspaper\nTV        1.00000000 0.05480866 0.05664787\nRadio     0.05480866 1.00000000 0.35410375\nNewspaper 0.05664787 0.35410375 1.00000000\n\n\nНаши независимые переменные связаны друг с другом довольно слабо. Специальной мерой, позволяющей проверить мультиколлинеарность, является \\(VIF\\)- variance inflation factor, показывающая увеличение в дисперсии коэффициентов после включения дополнительной переменной:\n\ncar::vif(mod_6)\n\n      TV    Radio \n1.006511 1.006511 \n\n\nVIF &lt; 3 обозначает слабую корреляцию между переменными (идеальные условия). Чаще всего в литературе приводится пороговое значение \\(VIF=5\\), и только переменные \\(VIF&lt;5\\) должны быть включены в модель.\nУ нас в модели с мультиколлинеарностью все в порядке.\n\n\n\n12.1.5 Линейная регрессия с категориальными предикторами\nНапомним, что категориальные переменные (также известные, как качественные, или факторные переменные) - это такие переменные, которые позволяют разделить наблюдения на группы. Их особенностями является ограниченное количество значений (уровней). Типичными являются примеры с полом (два уровня - мужчины и женщины) или национальностью, социальным статусом или уровнем образования (например, лица с общим средним, средним профессиональным и высшим образованием).\nОбычно регрессионный анализ проводится с количественными переменными, и когда исследователь желает включить в модель категориальную переменную, необходимы некоторые шаги, чтобы сделать результаты более интерпретируемыми.\nВ частности, категориальные переменные перекодируются в набор так называемых «dummy» (фиктивных) переменных, в результате создается матрица контрастов. Современные программы, в том числе и R, «умеют» это делать автоматически.\n\n Пример: воспользуемся набором данных Salaries из пакета car, в котором содержатся данные о зарплате ассистентов, ассоциированных профессоров и профессоров в одном из американских колледжей (данные - за 2008-2009 учебный год). Данные были собраны администрацией для того, чтобы отслеживать различия между зарплатой, получаемой преподавателями мужчинами и женщинами,\n\nЗагрузим данные:\n\nlibrary(car)\ndata(\"Salaries\")\nhead(Salaries, 3)\n\n      rank discipline yrs.since.phd yrs.service  sex salary\n1     Prof          B            19          18 Male 139750\n2     Prof          B            20          16 Male 173200\n3 AsstProf          B             4           3 Male  79750\n\n\n\n12.1.5.1 Категориальные переменные с двумя уровнями\nВспомним, что в регрессионном уравнении для того, чтобы предсказать переменную \\(y\\) на основе независимой переменной \\(x\\), нужно суммировать все основные компоненты:\n\\[y = b_0 + b_1*x\\]\nПри этом:\n\n\\(b_0\\) и \\(b_1\\) являются регрессионными коэффициентами, представляющими константу (интерцепт) и угол наклона регрессионной прямой (slope).\n\nДопустим, мы хотим проанализировать различия в заработной плате у мужчин и женщин.\nНа основе переменной пола, мы можем создать новую фиктивную переменную, которая будет принимать значения:\n\n1 если преподаватель мужчина\n0 если преподаватель женщина\n\nи использовать эту переменную в регрессионном уравнении. При этом интерпретация коэффициентов и самого уравнения будет следующей:\n\n\\(b_0\\) средняя зарплата у женщин,\n\\(b_0 + b_1\\) средняя зарплата у мужчин,\n\\(b_1\\) различия в среднем между зарплатой мужчин и женщин.\n\nСоздадим модель:\n\nmod_7  &lt;- lm(salary ~ sex, data = Salaries)\nsummary(mod_7)$coef\n\n             Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept) 101002.41   4809.386 21.001103 2.683482e-66\nsexMale      14088.01   5064.579  2.781674 5.667107e-03\n\n\nИсходя из выведенной информации, средняя зарплата у преподавателей женщин - 101002 долларов (за 9 месяцев), тогда как у мужчин 101002 + 14088 = 115090. Полученное p-значение для фиктивной переменной sexMale очень значимое, что указывает на то, что имеются статистические обоснования наличия различий в зарплате по полу.\nФункция contrasts()позволяет посмотреть код, который использовался для создания фиктивных переменных:\n\ncontrasts(Salaries$sex)\n\n       Male\nFemale    0\nMale      1\n\n\nПри такой кодировке женщины являются референтной группой, с которой сравниваются мужчины, и в целом, любая подобная кодировка является условной, ее результаты будут влиять только на интерпретацию коэффициентов регрессии.\nЕсли нас такая кодировка не устраивает, мы можем использовать функцию relevel() для смены уровней:\n\nSalaries &lt;- Salaries %&gt;%\n  mutate(sex = relevel(sex, ref = \"Male\"))\n\nПосле перекодировки результаты регрессионного анализа будут следующими:\n\nmod_8 &lt;- lm(salary ~ sex, data = Salaries)\nsummary(mod_7)$coef\n\n             Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept) 101002.41   4809.386 21.001103 2.683482e-66\nsexMale      14088.01   5064.579  2.781674 5.667107e-03\n\n\nПоскольку мы теперь сравниваем зарплату женщин с зарплатой мужчин, коэффициент переменной sexFemale негативный, что означает более низкий уровень зарплат у женщин, по сравнению с мужчинами.\nКоэффициент \\(b_0\\) равено 115090 (средняя зарплата у мужчин), тогда как коэффициент \\(b_1\\) - -14088, показывает, на сколько, в среднем, ниже зарплата у женщин. Соответственно, 115090 - 14088 = 101002 - средняя зарплата женщин.\n\n\n12.1.5.2 Категориальная переменная с более чем двумя уровнями\nЧто делать, если в качественной переменной, которую мы хотим использовать, более двух уровней? Наиболее типичным является подход, когда такая категориальная переменная трансформируется в n-1 бинарных переменных, каждая из которых имеет по два уровня. И эти n-1 новых переменных содержат ту же информацию, что исходная переменная. В результате такой кодировки создается таблица контрастов.\nНапример, в нашем наборе есть переменная rank, которая имеет три уровня: AsstProf, AssocProf и Prof. Мы можем создать две фиктивных переменных - AssocProf и Prof:\n\nесли rank = AssocProf, тогда в новом столбце AssocProf преподавателями, являющими ассоциированными профессорами, будет присвоено значение 1, а профессорам - 0.\nесли rank = Prof, тогда в новом столбце Prof все профессора получат значение 1, а ассоциированные профессора - 0.\n\nчто же с ассистентами? В обоих новых столбцах они получат значение 0.\n\nТакого рода кодировка в R осуществляется автоматически. С помощью функции model.matrix() мы можем посмотреть, как такая матрица контрастов может выглядеть:\n\nres &lt;- model.matrix(~rank, data = Salaries)\nhead(res[, -1])\n\n  rankAssocProf rankProf\n1             0        1\n2             0        1\n3             0        0\n4             0        1\n5             0        1\n6             1        0\n\n\nВ практике регрессионного анализа есть различные способы кодирования категориальных переменных (создания контрастов). По умолчанию в R первый уровень используется в качестве референтного, а остальные интерпретируются уже по отношению к этому уровню.\n\nПример, который мы только что рассмотрели, показывает, что дисперсионный анализ - ANOVA (analyse of variance) является специальным случаем линейной модели, в которой предикторами являются категориальные переменные. И поскольку R это тоже «понимает», мы можем извлечь из модели результаты дисперсионного анализа (предпочтительнее использовать функцию Anova() из пакета car (car означает Companion to Applied Regression - компаньон для прикладных задач регрессионного анализа).\n\nСоздадим модель, в которой мы будем предсказывать зарплату от всех других переменных в наборе (знак плюс означает, что мы будем рассматривать только главные эффекты, без интеракций):\n\nmod_9&lt;- lm(salary ~ yrs.service + rank + discipline + sex,\n             data = Salaries)\nAnova(mod_9)\n\nAnova Table (Type II tests)\n\nResponse: salary\n                Sum Sq  Df  F value    Pr(&gt;F)    \nyrs.service 3.2448e+08   1   0.6324    0.4270    \nrank        1.0288e+11   2 100.2572 &lt; 2.2e-16 ***\ndiscipline  1.7373e+10   1  33.8582 1.235e-08 ***\nsex         7.7669e+08   1   1.5137    0.2193    \nResiduals   2.0062e+11 391                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nПосле того, как мы приняли во внимание другие переменные (стаж - yrs.service, должность - rank, область знаний - discipline), стало понятно, что фактор пола уже не имеет значения и не вносит вклада в вариабельность заработной платы. Значимыми становятся должность и область знания.\nЧтобы вывести более подробные результаты анализа, лучше воспользоваться функцией summary():\n\nsummary(mod_9)\n\n\nCall:\nlm(formula = salary ~ yrs.service + rank + discipline + sex, \n    data = Salaries)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-64202 -14255  -1533  10571  99163 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   73122.92    3245.27  22.532  &lt; 2e-16 ***\nyrs.service     -88.78     111.64  -0.795 0.426958    \nrankAssocProf 14560.40    4098.32   3.553 0.000428 ***\nrankProf      49159.64    3834.49  12.820  &lt; 2e-16 ***\ndisciplineB   13473.38    2315.50   5.819 1.24e-08 ***\nsexFemale     -4771.25    3878.00  -1.230 0.219311    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 22650 on 391 degrees of freedom\nMultiple R-squared:  0.4478,    Adjusted R-squared:  0.4407 \nF-statistic: 63.41 on 5 and 391 DF,  p-value: &lt; 2.2e-16\n\n\nРезультаты показывают, что зарплата ассоциированного профессора в среднем на 14560.40 долларов выше, чем у ассистента, при прочих равных условиях, а у профессора - выше на 49159.64 долларов. Интересно, что зарплата значительно варьирует от специализации: на прикладных кафедрах (applied departments) наблюдается в среднем на 13473.38 долее высокая зарплата, по сравнению с теоретическими дисциплинами (theoretical departments).\n\n\n12.1.5.3 Интеракции\nИнтеракции происходят тогда, когда эффект одного из предикторов зависит от другой переменной в модели.\nЧтобы продемонстрировать эффект интеракции, рассмотрим взаимосвязь между должностью и областью знаний в примере про зарплату преподавателей:\n\\[\n\\begin{split}\ny_i &=\\beta_0 + \\beta_1*(rank) + \\beta_2*(discipline) + \\beta_3*(rank*discipline) +\\\\ & \\beta_4*(yrs.service) + \\beta_5*(sex) + \\varepsilon_i\n\\end{split}\n\\]\n\nmod_9 &lt;- lm(salary ~ yrs.service + sex + rank * discipline, data = Salaries)\nsummary(mod_9)\n\n\nCall:\nlm(formula = salary ~ yrs.service + sex + rank * discipline, \n    data = Salaries)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-64153 -14387  -1511  10675  99229 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               75387.60    4743.67  15.892  &lt; 2e-16 ***\nyrs.service                 -86.26     111.91  -0.771    0.441    \nsexFemale                 -4974.39    3897.08  -1.276    0.203    \nrankAssocProf              9603.29    6543.62   1.468    0.143    \nrankProf                  46972.83    5627.26   8.347 1.24e-15 ***\ndisciplineB                9987.33    5802.62   1.721    0.086 .  \nrankAssocProf:disciplineB  8023.36    8189.27   0.980    0.328    \nrankProf:disciplineB       3246.32    6446.03   0.504    0.615    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 22680 on 389 degrees of freedom\nMultiple R-squared:  0.4492,    Adjusted R-squared:  0.4393 \nF-statistic: 45.32 on 7 and 389 DF,  p-value: &lt; 2.2e-16\n\n\n\nОтметим, что хотя в формуле мы указали только интеракцию, в выводе содержатся также сведения и об индивидуальных эффектах. R включает эту информацию автоматически.\n\nИнтерпретируя результаты отметим, что эффект от взаимосвязи не значим, и единственным значимым предиктором в модели остается должность: значительная прибавка в зарплате отмечается только у профессоров, тогда как дисциплинарная принадлежность значима только на уровне статистической тенденции (\\(p=0,086\\)).\n\n\n12.1.5.4 Отбор переменных для модели\nПрежде чем перейти к моделированию, аналитик проводит тщательную работу по отбору переменных. Обычно, этому предшествует теоретический анализ, который позволит определить, какие показатели, важные для целевой переменной, необходимо включить в исследование, а затем - в модель.\nОднако, когда эксперимент уже проведен, наступает время проверки статистических гипотез. Очевидно, что не всегда все включаемые в модель параметры, в конце концов оказываются значимыми.\nКакие алгоритмы мы можем использовать для определения финальной, самой лучшей модели из возможных?\nОтбор переменных (variable selection) - это процесс выбора наиболее значимых переменных для включения в регрессионную модель. Методы отбора помогают улучшить производительность модели и избежать чрезмерной подгонки.\nВ рамках данного занятия мы рассмотрим следующие методы отбора:\n\nанализ всех возможных моделей / лучшей модели, определяемой на основе оценке качества модели\nпошаговые алгоритмы\n\nДля работы мы будем использовать пакет olsrr:\n\ninstall.packages(\"olsrr\")\nlibrary(olsrr)\n\n\n12.1.5.4.0.1 Анализ всех возможных моделей\nПрежде чем мы рассмотрим методы пошагового отбора, давайте вкратце рассмотрим регрессию по всем/лучшим подмножествам. Поскольку они оценивают все возможные комбинации переменных, эти методы требуют больших вычислительных затрат и могут вывести систему из строя, если использовать их с большим набором переменных.\nМетод All subset regression (все возможные варианты) представляет результаты по всем возможным комбинациям предикторов. Если у нас есть \\(k\\) потенциальных независимых переменных, не считая константы, то количество отдельных моделей, которые потребуется проанализировать, составит - \\(2^k\\). Например, если у нас 10 предикторов, то количество моделей - \\(2^10\\) - 1024, а если переменных 20 - то количество комбинаций превышает миллион.\n\nmodel &lt;- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)\nols_step_all_possible(model)\n\n   Index N      Predictors  R-Square Adj. R-Square Mallow's Cp\n3      1 1              wt 0.7528328     0.7445939  0.70869536\n1      2 1            disp 0.7183433     0.7089548  0.67512054\n2      3 1              hp 0.6024373     0.5891853  0.50969578\n4      4 1            qsec 0.1752963     0.1478062  0.07541973\n8      5 2           hp wt 0.8267855     0.8148396  0.78108710\n10     6 2         wt qsec 0.8264161     0.8144448  0.77856272\n6      7 2         disp wt 0.7809306     0.7658223  0.72532105\n5      8 2         disp hp 0.7482402     0.7308774  0.69454380\n7      9 2       disp qsec 0.7215598     0.7023571  0.66395284\n9     10 2         hp qsec 0.6368769     0.6118339  0.52014395\n14    11 3      hp wt qsec 0.8347678     0.8170643  0.78199548\n11    12 3      disp hp wt 0.8268361     0.8082829  0.76789526\n13    13 3    disp wt qsec 0.8264170     0.8078189  0.76988533\n12    14 3    disp hp qsec 0.7541953     0.7278591  0.68301440\n15    15 4 disp hp wt qsec 0.8351443     0.8107212  0.77102968\n\n\n\n\n12.1.5.4.0.2 Подборка лучших моделей (Best Subset Regression)\nДанный метод позволяет отобрать модели, которые являются лучшими по обобщенным критериям модели, например, имеет наибольший \\(R^2\\) или меньшие \\(MSE\\) (средняя квадратичная ошибка) или \\(AIC\\) (информационный критерий Акаике).\n\nmodel &lt;- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)\nols_step_best_subset(model)\n\n   Best Subsets Regression    \n------------------------------\nModel Index    Predictors\n------------------------------\n     1         wt              \n     2         hp wt           \n     3         hp wt qsec      \n     4         disp hp wt qsec \n------------------------------\n\n                                                   Subsets Regression Summary                                                    \n---------------------------------------------------------------------------------------------------------------------------------\n                       Adj.        Pred                                                                                           \nModel    R-Square    R-Square    R-Square     C(p)        AIC        SBIC        SBC         MSEP       FPE       HSP       APC  \n---------------------------------------------------------------------------------------------------------------------------------\n  1        0.7528      0.7446      0.7087    12.4809    166.0294    74.2916    170.4266    296.9167    9.8572    0.3199    0.2801 \n  2        0.8268      0.8148      0.7811     2.3690    156.6523    66.5755    162.5153    215.5104    7.3563    0.2402    0.2091 \n  3        0.8348      0.8171       0.782     3.0617    157.1426    67.7238    164.4713    213.1929    7.4756    0.2461    0.2124 \n  4        0.8351      0.8107       0.771     5.0000    159.0696    70.0408    167.8640    220.8882    7.9497    0.2644    0.2259 \n---------------------------------------------------------------------------------------------------------------------------------\nAIC: Akaike Information Criteria \n SBIC: Sawa's Bayesian Information Criteria \n SBC: Schwarz Bayesian Criteria \n MSEP: Estimated error of prediction, assuming multivariate normality \n FPE: Final Prediction Error \n HSP: Hocking's Sp \n APC: Amemiya Prediction Criteria \n\n\n\n\n12.1.5.4.0.3 Пошаговый отбор (Stepwise Selection)\nПошаговая регрессия - это метод подбора регрессионных моделей, который предполагает итерационный отбор независимых переменных для использования в модели. Он может быть реализован с помощью прямого отбора, обратного исключения или комбинации обоих методов.\nМетод прямого отбора начинается с модели без предикторов и постепенно добавляет каждую новую переменную, проверяя ее статистическую значимость, а метод обратного исключения, напротив,ю начинается с полной модели и затем по очереди удаляет наименее статистически значимые переменные.\n\n Пример: Для иллюстрации возможностей пошагового отбора воспользуемся данными из области недвижимости (HousingData).Набор включает данные по 506 объектам недвижимости, оцененных по 14 показателям.\n\nПеременные:\n\nCRIM : уровень преступности\nZN : proportion of residential land zoned for lots over 25,000 sq.ft.\nINDUS : доля промышленных предприятий среди нежилых объектов\nCHAS : дамми переменная, показывает расположение относительно главной реки\nNOX : концентрация нитрита озота\nRM : среднее количество комнат\nAGE : доля зданий, построенных до 1940\nDIS : взвешенное расстояние до пяти значимых бостонских деловых центров\nRAD : индекс доступности хайвея\nTAX : налоги\nPTRATIO : соотношение между учителями и преподавателями (обеспеченность учителями)\nB : доля чернокожено населения\nLSTAT : доля населения с низкими доходами\nMEDV : медианная стоимость в 1000 долларов\n\nСкачать данные\nЗагрузим данные и создадим общую модель:\n\nHousingData&lt;-read.csv(\"HousingData.csv\")\nmodel &lt;- lm(MEDV ~ ., data = HousingData)\nsummary(model)\n\n\nCall:\nlm(formula = MEDV ~ ., data = HousingData)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.4234  -2.5830  -0.5079   1.6681  26.2604 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  32.680059   5.681290   5.752 1.81e-08 ***\nCRIM         -0.097594   0.032457  -3.007 0.002815 ** \nZN            0.048905   0.014398   3.397 0.000754 ***\nINDUS         0.030379   0.065933   0.461 0.645237    \nCHAS          2.769378   0.925171   2.993 0.002940 ** \nNOX         -17.969028   4.242856  -4.235 2.87e-05 ***\nRM            4.283252   0.470710   9.100  &lt; 2e-16 ***\nAGE          -0.012991   0.014459  -0.898 0.369504    \nDIS          -1.458510   0.211007  -6.912 2.03e-11 ***\nRAD           0.285866   0.069298   4.125 4.55e-05 ***\nTAX          -0.013146   0.003955  -3.324 0.000975 ***\nPTRATIO      -0.914582   0.140581  -6.506 2.44e-10 ***\nB             0.009656   0.002970   3.251 0.001251 ** \nLSTAT        -0.423661   0.055022  -7.700 1.19e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.487 on 380 degrees of freedom\n  (112 пропущенных наблюдений удалены)\nMultiple R-squared:  0.7671,    Adjusted R-squared:  0.7591 \nF-statistic: 96.29 on 13 and 380 DF,  p-value: &lt; 2.2e-16\n\n\nМетод ступенчатого включения (начинаем с нулевой модели и постепенно добавляем предикторы)\n\nols_step_forward_p(model)\n\n\n                               Stepwise Summary                               \n----------------------------------------------------------------------------\nStep    Variable        AIC         SBC         SBIC        R2       Adj. R2 \n----------------------------------------------------------------------------\n 0      Base Model    2864.955    2872.908    1744.164    0.00000    0.00000 \n 1      LSTAT         2549.957    2561.886    1429.569    0.55272    0.55158 \n 2      RM            2445.616    2461.522    1325.564    0.65852    0.65677 \n 3      PTRATIO       2391.749    2411.631    1272.076    0.70366    0.70138 \n 4      B             2377.429    2401.287    1257.753    0.71569    0.71276 \n 5      CHAS          2367.576    2395.411    1247.924    0.72411    0.72056 \n 6      DIS           2361.466    2393.277    1241.828    0.72973    0.72554 \n 7      NOX           2337.205    2372.992    1218.357    0.74716    0.74258 \n 8      ZN            2330.955    2370.719    1212.385    0.75240    0.74725 \n 9      CRIM          2328.804    2372.544    1210.391    0.75499    0.74925 \n 10     RAD           2324.806    2372.522    1206.697    0.75870    0.75240 \n 11     TAX           2313.853    2365.546    1196.546    0.76650    0.75978 \n----------------------------------------------------------------------------\n\nFinal Model Output \n------------------\n\n                         Model Summary                           \n----------------------------------------------------------------\nR                       0.875       RMSE                  4.412 \nR-Squared               0.767       MSE                  20.081 \nAdj. R-Squared          0.760       Coef. Var            20.042 \nPred R-Squared          0.743       AIC                2313.853 \nMAE                     3.067       SBC                2365.546 \n----------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                                 ANOVA                                  \n-----------------------------------------------------------------------\n                 Sum of                                                \n                Squares         DF    Mean Square       F         Sig. \n-----------------------------------------------------------------------\nRegression    25181.422         11       2289.220    113.998    0.0000 \nResidual       7671.047        382         20.081                      \nTotal         32852.468        393                                     \n-----------------------------------------------------------------------\n\n                                    Parameter Estimates                                     \n-------------------------------------------------------------------------------------------\n      model       Beta    Std. Error    Std. Beta      t        Sig       lower      upper \n-------------------------------------------------------------------------------------------\n(Intercept)     32.975         5.631                  5.856    0.000     21.904     44.046 \n      LSTAT     -0.440         0.052       -0.352    -8.532    0.000     -0.541     -0.339 \n         RM      4.167         0.455        0.318     9.149    0.000      3.271      5.063 \n    PTRATIO     -0.915         0.139       -0.217    -6.599    0.000     -1.187     -0.642 \n          B      0.009         0.003        0.093     3.201    0.001      0.004      0.015 \n       CHAS      2.788         0.920        0.077     3.031    0.003      0.980      4.596 \n        DIS     -1.421         0.197       -0.326    -7.201    0.000     -1.808     -1.033 \n        NOX    -18.468         3.895       -0.228    -4.741    0.000    -26.127    -10.809 \n         ZN      0.050         0.014        0.131     3.526    0.000      0.022      0.078 \n       CRIM     -0.098         0.032       -0.099    -3.029    0.003     -0.162     -0.034 \n        RAD      0.282         0.066        0.267     4.309    0.000      0.153      0.411 \n        TAX     -0.012         0.003       -0.228    -3.573    0.000     -0.019     -0.006 \n-------------------------------------------------------------------------------------------\n\n\nМетод пошагового исключения:\n\nols_step_backward_p(model)\n\n\n                               Stepwise Summary                               \n----------------------------------------------------------------------------\nStep    Variable        AIC         SBC         SBIC        R2       Adj. R2 \n----------------------------------------------------------------------------\n 0      Full Model    2316.815    2376.460    1199.720    0.76711    0.75915 \n 1      INDUS         2315.035    2370.704    1197.851    0.76698    0.75965 \n 2      AGE           2313.853    2365.546    1196.546    0.76650    0.75978 \n----------------------------------------------------------------------------\n\nFinal Model Output \n------------------\n\n                         Model Summary                           \n----------------------------------------------------------------\nR                       0.875       RMSE                  4.412 \nR-Squared               0.767       MSE                  20.081 \nAdj. R-Squared          0.760       Coef. Var            20.042 \nPred R-Squared          0.743       AIC                2313.853 \nMAE                     3.067       SBC                2365.546 \n----------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                                 ANOVA                                  \n-----------------------------------------------------------------------\n                 Sum of                                                \n                Squares         DF    Mean Square       F         Sig. \n-----------------------------------------------------------------------\nRegression    25181.422         11       2289.220    113.998    0.0000 \nResidual       7671.047        382         20.081                      \nTotal         32852.468        393                                     \n-----------------------------------------------------------------------\n\n                                    Parameter Estimates                                     \n-------------------------------------------------------------------------------------------\n      model       Beta    Std. Error    Std. Beta      t        Sig       lower      upper \n-------------------------------------------------------------------------------------------\n(Intercept)     32.975         5.631                  5.856    0.000     21.904     44.046 \n       CRIM     -0.098         0.032       -0.099    -3.029    0.003     -0.162     -0.034 \n         ZN      0.050         0.014        0.131     3.526    0.000      0.022      0.078 \n       CHAS      2.788         0.920        0.077     3.031    0.003      0.980      4.596 \n        NOX    -18.468         3.895       -0.228    -4.741    0.000    -26.127    -10.809 \n         RM      4.167         0.455        0.318     9.149    0.000      3.271      5.063 \n        DIS     -1.421         0.197       -0.326    -7.201    0.000     -1.808     -1.033 \n        RAD      0.282         0.066        0.267     4.309    0.000      0.153      0.411 \n        TAX     -0.012         0.003       -0.228    -3.573    0.000     -0.019     -0.006 \n    PTRATIO     -0.915         0.139       -0.217    -6.599    0.000     -1.187     -0.642 \n          B      0.009         0.003        0.093     3.201    0.001      0.004      0.015 \n      LSTAT     -0.440         0.052       -0.352    -8.532    0.000     -0.541     -0.339 \n-------------------------------------------------------------------------------------------\n\n\nПринудительное включение в модель по имени переменной:\n\nols_step_forward_p(model, include = c(\"AGE\", \"LSTAT\"))\n\n\n                               Stepwise Summary                               \n----------------------------------------------------------------------------\nStep    Variable        AIC         SBC         SBIC        R2       Adj. R2 \n----------------------------------------------------------------------------\n 0      Base Model    2549.805    2565.711    1428.520    0.55515    0.55288 \n 1      AGE           2795.421    2807.350    1673.446    0.16603    0.16390 \n 2      LSTAT         2549.805    2565.711    1428.520    0.55515    0.55288 \n 3      RM            2447.070    2466.952    1326.441    0.65899    0.65637 \n 4      PTRATIO       2393.728    2417.587    1273.685    0.70368    0.70063 \n 5      B             2379.428    2407.262    1259.450    0.71569    0.71202 \n 6      DIS           2363.230    2395.041    1243.534    0.72852    0.72431 \n 7      NOX           2346.870    2382.657    1227.647    0.74088    0.73618 \n 8      CHAS          2336.592    2376.355    1217.770    0.74883    0.74361 \n 9      ZN            2331.522    2375.262    1212.973    0.75330    0.74752 \n 10     CRIM          2329.117    2376.834    1210.769    0.75604    0.74967 \n 11     RAD           2325.744    2377.436    1207.701    0.75935    0.75242 \n----------------------------------------------------------------------------\n\nFinal Model Output \n------------------\n\n                         Model Summary                           \n----------------------------------------------------------------\nR                       0.871       RMSE                  4.480 \nR-Squared               0.759       MSE                  20.697 \nAdj. R-Squared          0.752       Coef. Var            20.346 \nPred R-Squared          0.734       AIC                2325.744 \nMAE                     3.107       SBC                2377.436 \n----------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                                 ANOVA                                  \n-----------------------------------------------------------------------\n                 Sum of                                                \n                Squares         DF    Mean Square       F         Sig. \n-----------------------------------------------------------------------\nRegression    24946.389         11       2267.854    109.576    0.0000 \nResidual       7906.079        382         20.697                      \nTotal         32852.468        393                                     \n-----------------------------------------------------------------------\n\n                                    Parameter Estimates                                     \n-------------------------------------------------------------------------------------------\n      model       Beta    Std. Error    Std. Beta      t        Sig       lower      upper \n-------------------------------------------------------------------------------------------\n(Intercept)     30.750         5.722                  5.374    0.000     19.499     42.001 \n        AGE     -0.015         0.015       -0.045    -1.015    0.311     -0.044      0.014 \n      LSTAT     -0.432         0.056       -0.346    -7.760    0.000     -0.542     -0.323 \n         RM      4.459         0.472        0.340     9.454    0.000      3.532      5.387 \n    PTRATIO     -0.982         0.139       -0.233    -7.043    0.000     -1.256     -0.708 \n          B      0.010         0.003        0.097     3.300    0.001      0.004      0.016 \n        DIS     -1.392         0.209       -0.319    -6.666    0.000     -1.802     -0.981 \n        NOX    -20.174         4.054       -0.250    -4.976    0.000    -28.144    -12.203 \n       CHAS      3.123         0.930        0.086     3.360    0.001      1.295      4.951 \n         ZN      0.037         0.014        0.097     2.623    0.009      0.009      0.065 \n       CRIM     -0.093         0.033       -0.094    -2.832    0.005     -0.158     -0.028 \n        RAD      0.101         0.044        0.096     2.290    0.023      0.014      0.188 \n-------------------------------------------------------------------------------------------\n\n\nПринудительное включение по индексу:\n\nols_step_forward_p(model, include = c(5, 7))\n\n\n                               Stepwise Summary                               \n----------------------------------------------------------------------------\nStep    Variable        AIC         SBC         SBIC        R2       Adj. R2 \n----------------------------------------------------------------------------\n 0      Base Model    2770.218    2786.123    1647.004    0.22167    0.21769 \n 1      NOX           2773.716    2785.645    1651.853    0.21073    0.20872 \n 2      AGE           2770.218    2786.123    1647.004    0.22167    0.21769 \n 3      RM            2518.979    2538.861    1397.239    0.59071    0.58757 \n 4      LSTAT         2446.035    2469.894    1324.883    0.66161    0.65813 \n 5      PTRATIO       2389.609    2417.444    1269.358    0.70825    0.70449 \n 6      DIS           2357.931    2389.742    1238.409    0.73215    0.72800 \n 7      CHAS          2345.772    2381.559    1226.591    0.74160    0.73692 \n 8      B             2336.592    2376.355    1217.770    0.74883    0.74361 \n 9      ZN            2331.522    2375.262    1212.973    0.75330    0.74752 \n 10     CRIM          2329.117    2376.834    1210.769    0.75604    0.74967 \n 11     RAD           2325.744    2377.436    1207.701    0.75935    0.75242 \n----------------------------------------------------------------------------\n\nFinal Model Output \n------------------\n\n                         Model Summary                           \n----------------------------------------------------------------\nR                       0.871       RMSE                  4.480 \nR-Squared               0.759       MSE                  20.697 \nAdj. R-Squared          0.752       Coef. Var            20.346 \nPred R-Squared          0.734       AIC                2325.744 \nMAE                     3.107       SBC                2377.436 \n----------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                                 ANOVA                                  \n-----------------------------------------------------------------------\n                 Sum of                                                \n                Squares         DF    Mean Square       F         Sig. \n-----------------------------------------------------------------------\nRegression    24946.389         11       2267.854    109.576    0.0000 \nResidual       7906.079        382         20.697                      \nTotal         32852.468        393                                     \n-----------------------------------------------------------------------\n\n                                    Parameter Estimates                                     \n-------------------------------------------------------------------------------------------\n      model       Beta    Std. Error    Std. Beta      t        Sig       lower      upper \n-------------------------------------------------------------------------------------------\n(Intercept)     30.750         5.722                  5.374    0.000     19.499     42.001 \n        NOX    -20.174         4.054       -0.250    -4.976    0.000    -28.144    -12.203 \n        AGE     -0.015         0.015       -0.045    -1.015    0.311     -0.044      0.014 \n         RM      4.459         0.472        0.340     9.454    0.000      3.532      5.387 \n      LSTAT     -0.432         0.056       -0.346    -7.760    0.000     -0.542     -0.323 \n    PTRATIO     -0.982         0.139       -0.233    -7.043    0.000     -1.256     -0.708 \n        DIS     -1.392         0.209       -0.319    -6.666    0.000     -1.802     -0.981 \n       CHAS      3.123         0.930        0.086     3.360    0.001      1.295      4.951 \n          B      0.010         0.003        0.097     3.300    0.001      0.004      0.016 \n         ZN      0.037         0.014        0.097     2.623    0.009      0.009      0.065 \n       CRIM     -0.093         0.033       -0.094    -2.832    0.005     -0.158     -0.028 \n        RAD      0.101         0.044        0.096     2.290    0.023      0.014      0.188 \n-------------------------------------------------------------------------------------------\n\n\nВыбор на основе коэффициента детерминации:\n\nols_step_forward_adj_r2(model)\n\n\n                               Stepwise Summary                               \n----------------------------------------------------------------------------\nStep    Variable        AIC         SBC         SBIC        R2       Adj. R2 \n----------------------------------------------------------------------------\n 0      Base Model    2864.955    2872.908    1744.164    0.00000    0.00000 \n 1      LSTAT         2549.957    2561.886    1429.569    0.55272    0.55158 \n 2      RM            2445.616    2461.522    1325.564    0.65852    0.65677 \n 3      PTRATIO       2391.749    2411.631    1272.076    0.70366    0.70138 \n 4      B             2377.429    2401.287    1257.753    0.71569    0.71276 \n 5      CHAS          2367.576    2395.411    1247.924    0.72411    0.72056 \n 6      DIS           2361.466    2393.277    1241.828    0.72973    0.72554 \n 7      NOX           2337.205    2372.992    1218.357    0.74716    0.74258 \n 8      ZN            2330.955    2370.719    1212.385    0.75240    0.74725 \n 9      CRIM          2328.804    2372.544    1210.391    0.75499    0.74925 \n 10     RAD           2324.806    2372.522    1206.697    0.75870    0.75240 \n 11     TAX           2313.853    2365.546    1196.546    0.76650    0.75978 \n----------------------------------------------------------------------------\n\nFinal Model Output \n------------------\n\n                         Model Summary                           \n----------------------------------------------------------------\nR                       0.875       RMSE                  4.412 \nR-Squared               0.767       MSE                  20.081 \nAdj. R-Squared          0.760       Coef. Var            20.042 \nPred R-Squared          0.743       AIC                2313.853 \nMAE                     3.067       SBC                2365.546 \n----------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                                 ANOVA                                  \n-----------------------------------------------------------------------\n                 Sum of                                                \n                Squares         DF    Mean Square       F         Sig. \n-----------------------------------------------------------------------\nRegression    25181.422         11       2289.220    113.998    0.0000 \nResidual       7671.047        382         20.081                      \nTotal         32852.468        393                                     \n-----------------------------------------------------------------------\n\n                                    Parameter Estimates                                     \n-------------------------------------------------------------------------------------------\n      model       Beta    Std. Error    Std. Beta      t        Sig       lower      upper \n-------------------------------------------------------------------------------------------\n(Intercept)     32.975         5.631                  5.856    0.000     21.904     44.046 \n      LSTAT     -0.440         0.052       -0.352    -8.532    0.000     -0.541     -0.339 \n         RM      4.167         0.455        0.318     9.149    0.000      3.271      5.063 \n    PTRATIO     -0.915         0.139       -0.217    -6.599    0.000     -1.187     -0.642 \n          B      0.009         0.003        0.093     3.201    0.001      0.004      0.015 \n       CHAS      2.788         0.920        0.077     3.031    0.003      0.980      4.596 \n        DIS     -1.421         0.197       -0.326    -7.201    0.000     -1.808     -1.033 \n        NOX    -18.468         3.895       -0.228    -4.741    0.000    -26.127    -10.809 \n         ZN      0.050         0.014        0.131     3.526    0.000      0.022      0.078 \n       CRIM     -0.098         0.032       -0.099    -3.029    0.003     -0.162     -0.034 \n        RAD      0.282         0.066        0.267     4.309    0.000      0.153      0.411 \n        TAX     -0.012         0.003       -0.228    -3.573    0.000     -0.019     -0.006 \n-------------------------------------------------------------------------------------------\n\n\nВизуализация модели:\n\nk &lt;- ols_step_forward_adj_r2(model)\nplot(k)\n\n\n\n\n\n\n\n\n\n\n12.1.5.4.0.4 Иерархический отбор\nКогда для отбора переменных используются p-значения, возможно использования иерархического отбора. Этот метод предполагает, что поиск значимых переменных ограничен следующей переменной. Если какая-то переменная не отбирается по причине не подходящего p-значения, то и ни одна последующая переменная не рассматривается для включения.\n\nols_step_forward_p(model, 0.1, hierarchical = TRUE)\n\n\n                               Stepwise Summary                               \n----------------------------------------------------------------------------\nStep    Variable        AIC         SBC         SBIC        R2       Adj. R2 \n----------------------------------------------------------------------------\n 0      Base Model    2864.955    2872.908    1744.164    0.00000    0.00000 \n 1      CRIM          2799.295    2811.224    1677.300    0.15779    0.15564 \n 2      ZN            2743.805    2759.711    1620.779    0.27213    0.26841 \n 3      INDUS         2709.933    2729.815    1585.906    0.33548    0.33037 \n 4      CHAS          2691.217    2715.075    1566.162    0.36951    0.36303 \n----------------------------------------------------------------------------\n\nFinal Model Output \n------------------\n\n                         Model Summary                           \n----------------------------------------------------------------\nR                       0.608       RMSE                  7.251 \nR-Squared               0.370       MSE                  53.247 \nAdj. R-Squared          0.363       Coef. Var            32.635 \nPred R-Squared          0.347       AIC                2691.217 \nMAE                     5.208       SBC                2715.075 \n----------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                                ANOVA                                  \n----------------------------------------------------------------------\n                 Sum of                                               \n                Squares         DF    Mean Square      F         Sig. \n----------------------------------------------------------------------\nRegression    12139.449          4       3034.862    56.996    0.0000 \nResidual      20713.019        389         53.247                     \nTotal         32852.468        393                                    \n----------------------------------------------------------------------\n\n                                  Parameter Estimates                                    \n----------------------------------------------------------------------------------------\n      model      Beta    Std. Error    Std. Beta      t        Sig      lower     upper \n----------------------------------------------------------------------------------------\n(Intercept)    26.633         0.901                 29.557    0.000    24.862    28.405 \n       CRIM    -0.220         0.044       -0.221    -5.036    0.000    -0.305    -0.134 \n         ZN     0.076         0.018        0.200     4.233    0.000     0.041     0.112 \n      INDUS    -0.436         0.067       -0.329    -6.527    0.000    -0.567    -0.305 \n       CHAS     6.697         1.461        0.185     4.583    0.000     3.824     9.570 \n----------------------------------------------------------------------------------------\n\n\nПошаговая регрессия может оказаться хорошей идеей, особенно, когда количество предикторов велико и нужно отобрать только самые значимые. Между тем, исследователи отмечают большое количество «подводных камней» и статистических проблем, которые могут возникнут в процессе применения регрессионного анализа, таких как переобученность данных, смещенные оценки, ошибки I рода (Harrell, 2015). Кроме того, в процессе применения пошаговых методов возникает опасная иллюзия итого, что компьютер автоматически отбирает правильные переменные, на самом деле это происходит без связи с теоретическими основаниями и гипотезами исследования. Более того, модель, которая была отобрана на основе какого-то критерия, на самом деле может оказаться нестабильной и малоинформативной. Во многих случаях правильным было бы опираться на теорию и предыдущие исследования, тогда как методы отбора могут рассматриваться в качестве поисковых техник.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Регрессионный анализ в R</span>"
    ]
  },
  {
    "objectID": "Regression.html#логистическая-регрессия",
    "href": "Regression.html#логистическая-регрессия",
    "title": "12  Регрессионный анализ в R",
    "section": "12.2 Логистическая регрессия",
    "text": "12.2 Логистическая регрессия\nЛогистическая регрессия применяется в том случае, если наша зависимая перменная имеет вид 0-1, то есть является дихотомической и имеет значения 1 и 0. По сути, такой регрессионный анализ решает задачу классификации, то есть определения принадлежности к одному из двух классов (“победит” или “проиграет”, примут на работу или нет и т.д.).\n\n Пример: В качестве примера, мы будем рассматривать данные о приеме в высшие учебные заведения. В частности, нас будет интересовать, как результаты выпускных экзаменов GRE (Graduate Record Exam scores) и средние оценки GPA (grade point average), а также престиж учебного заведения связаня с допуском в высшее учебное заведение. Зависимой является переменная admit/don’t admit, которая уже закодирована в формате 0-1.\n\n\ndata &lt;- read.csv(\"https://stats.idre.ucla.edu/stat/data/binary.csv\")\nhead(data)\n\n  admit gre  gpa rank\n1     0 380 3.61    3\n2     1 660 3.67    3\n3     1 800 4.00    1\n4     1 640 3.19    4\n5     0 520 2.93    4\n6     1 760 3.00    2\n\n\nМодель логистической регрессии имеет вид:\n\\[\n\\log\\left(\\frac{p(x)}{1 - p(x)}\\right) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots  + \\beta_p x_p.\n\\]\nОткуда, путем перестановки, мы можем вывести вероятность принадлежности к группе 1:\n\\[\np(x) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots  + \\beta_p x_p)}} = \\sigma(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots  + \\beta_p x_p)\n\\]\nОбычно основное уравнение представляется в виде сигмоиды (логистической функции):\n\\[\n\\sigma(x) = \\frac{e^x}{1 + e^x} = \\frac{1}{1 + e^{-x}}\n\\]\n\nПодгонка модели осуществляется путем максимизации функции правдоподобия, что практически никогда не происходит вручную, и мы предоставим возможность это сделатьR\nНачнем с того, что переведем ранг заведения в факторную переменную:\n\ndata$rank &lt;- factor(data$rank)\n\n\nmodel_glm &lt;- glm(admit ~ gre + gpa + rank, data = data, family = \"binomial\")\n\nРезультаты логистической регрессии очень похожи на то, что мы видели в линейной регрессии, только вместо lm() мы используем glm(). Другая особенность - в атрибуте family = \"binomial\", что означает, что у нас будет зависимая переменная, состоящая из двух классов. Если использовать glm() с family = \"gaussian\" то получится обычная линейная регрессия.\nДавайте посмотрим на общие результаты\n\nsummary(model_glm)\n\n\nCall:\nglm(formula = admit ~ gre + gpa + rank, family = \"binomial\", \n    data = data)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -3.989979   1.139951  -3.500 0.000465 ***\ngre          0.002264   0.001094   2.070 0.038465 *  \ngpa          0.804038   0.331819   2.423 0.015388 *  \nrank2       -0.675443   0.316490  -2.134 0.032829 *  \nrank3       -1.340204   0.345306  -3.881 0.000104 ***\nrank4       -1.551464   0.417832  -3.713 0.000205 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 499.98  on 399  degrees of freedom\nResidual deviance: 458.52  on 394  degrees of freedom\nAIC: 470.52\n\nNumber of Fisher Scoring iterations: 4\n\n\nВывод в целом напоминает то, что мы видели в линейной регрессии. Видим, что все наши зависимые переменные являются значимыми. Переменные ранга имеют отрицательные коэффициенты, так как сравниваются со значением 1 - группа учебных заведений с наиболее высокими позициями.\nОднако, стоит помнить, что коэффициенты в логистической регрессии не простые, они представляют собой логарифм шансов. Что это значит?\nДавайте посмотрим на таблицу с нашей зависимой переменной:\n\ntable(data$admit)\n\n\n  0   1 \n273 127 \n\n\nВсего допущено 127 человек из 400, то есть вероятность допуска составит: 127/400=0.3175, а отношение шансов (допуска к недопуску): 0.3175/(1-0.1375)=0.3681159. Логарифм данного выражения составит:\n\nlog(0.3175/(1-0.3175))\n\n[1] -0.7652847\n\n\nЭто именно то, что мы бы получили, если бы создали модель только с одним интерцептом:\n\nmodel_null&lt;-glm(admit ~ 1, data = data, family = \"binomial\")\nsummary(model_null)\n\n\nCall:\nglm(formula = admit ~ 1, family = \"binomial\", data = data)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -0.7653     0.1074  -7.125 1.04e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 499.98  on 399  degrees of freedom\nResidual deviance: 499.98  on 399  degrees of freedom\nAIC: 501.98\n\nNumber of Fisher Scoring iterations: 4\n\n\nВернемся к коэффициентам нашей большой модели:\n\nизменения в одну единицу по переменной gre, логарифм шансов допуска увеличится на 0.002.\nизменение на одну единицу в gpa, логарифм шансов допуска увеличится на 0.804.\n\nИндикаторные переменные для ранга имеют слегка иную интерпретацию. Например, посещая школу, входящую во вторую группу по престижности, по сравнению с группой 1 изменяет логарифм шансов на -0.675.\nВнизу таблицы с коэффициентами располагаются индексы подгонки (AIC).\nЧтобы перевести коэффициенты в обычное отношение шансов, применяется экспоненциальная функция. Можно соединить это действие с вычислением доверительных интервалов:\n\nexp(cbind(OR = coef(model_glm), confint(model_glm)))\n\n                   OR       2.5 %    97.5 %\n(Intercept) 0.0185001 0.001889165 0.1665354\ngre         1.0022670 1.000137602 1.0044457\ngpa         2.2345448 1.173858216 4.3238349\nrank2       0.5089310 0.272289674 0.9448343\nrank3       0.2617923 0.131641717 0.5115181\nrank4       0.2119375 0.090715546 0.4706961\n\n\nКак интерпретировать отношение шансов?\nДля количественных переменных меняется мало что:\n\nизменение на одну единицу gre на 0,2% увеличивает шансы быть принятыми\nизменение среднего балла на единицу увеличивает шансы на 123%\nа вот учеба в школе ранга 2 снижает шансы на 50%, ранга 3 - на 73.8%, ранга 4 - на 78,8%.\n\n\nОбщая формула перевода отношения шансов в проценты: (OR-1) * 100\n\nСледующий этап - посмотреть, как работает функция predict() вместе с glm():\n\nhead(predict(model_glm))\n\n         1          2          3          4          5          6 \n-1.5671256 -0.8848442  1.0377118 -1.5273305 -2.0081113 -0.5323458 \n\n\nПо умолчанию predict.glm() использует type = \"link\".\n\nhead(predict(model_glm, type = \"link\"))\n\n         1          2          3          4          5          6 \n-1.5671256 -0.8848442  1.0377118 -1.5273305 -2.0081113 -0.5323458 \n\n\nЭто означает, что R возвращает:\n\\[\n\\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 + \\cdots  + \\hat{\\beta}_p x_p\n\\] для каждого наблюдения.\nВажно понимать, что это не предсказанные вероятности, и для того, чтобы их получить:\n\\[\n\\hat{p}(x) = \\hat{P}(Y = 1 \\mid X = x)\n\\]\nмы должны написать type = \"response\"\n\nhead(predict(model_glm, type = \"response\"))\n\n        1         2         3         4         5         6 \n0.1726265 0.2921750 0.7384082 0.1783846 0.1183539 0.3699699 \n\n\nСоответственно, это вероятности, но не результаты классификации. Для того, чтобы их получить, мы должны сравнить вероятности с пороговым значением.\n\nmodel_glm_pred = ifelse(predict(model_glm, type = \"response\") &gt; 0.5, 1, 0)\nhead(model_glm_pred)\n\n1 2 3 4 5 6 \n0 0 1 0 0 0 \n\n\nЧто мы сделали?\n\\[\n\\hat{C}(x) =\n\\begin{cases}\n      1 & \\hat{f}(x) &gt; 0 \\\\\n      0 & \\hat{f}(x) \\leq 0\n\\end{cases}\n\\]\nгде\n\\[\n\\hat{f}(x) =\\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 + \\cdots  + \\hat{\\beta}_p x_p.\n\\]\nТот код, который мы запустили, делает следующее:\n\\[\n\\hat{C}(x) =\n\\begin{cases}\n      1 & \\hat{p}(x) &gt; 0.5 \\\\\n      0 & \\hat{p}(x) \\leq 0.5\n\\end{cases}\n\\]\nгде\n\\[\n\\hat{p}(x) = \\hat{P}(Y = 1 \\mid X = x).\n\\]\nПосчитав классификации, мы можем также посчитать метрики для ошибок.\n\ntab = table(predicted = model_glm_pred, actual = data$admit)\ntab\n\n         actual\npredicted   0   1\n        0 254  97\n        1  19  30\n\nlibrary(caret)\nconfusionMatrix = confusionMatrix(tab, positive = \"1\")\nc(confusionMatrix$overall[\"Accuracy\"], \n  confusionMatrix$byClass[\"Sensitivity\"], \n  confusionMatrix$byClass[\"Specificity\"])\n\n   Accuracy Sensitivity Specificity \n  0.7100000   0.2362205   0.9304029 \n\n\n\nМы можем также предсказать результаты допуска в вуз для новых данных.\nПопробуем их сгенерировать на основе исходных данных:\n\nnewdata1 &lt;- with(data, data.frame(gre = mean(gre), gpa = mean(gpa), rank = factor(1:4)))\n\nПредскажем результаты зачисления:\n\nnewdata1$rankP &lt;- predict(model_glm, newdata = newdata1, type = \"response\")\nnewdata1\n\n    gre    gpa rank     rankP\n1 587.7 3.3899    1 0.5166016\n2 587.7 3.3899    2 0.3522846\n3 587.7 3.3899    3 0.2186120\n4 587.7 3.3899    4 0.1846684\n\n\nВидим, что при средних оценках, учащиеся, обучавшиеся в престижных школах имеют большую вероятность поступить, чем те, кто училися не в очень престижных заведениях.\nМы также можем захотеть узнать, насколько хорошо наша модель соответствует действительности. Это может быть особенно полезно при сравнении конкурирующих моделей.\nАналогом \\(R^2\\) для логистической регрессии является \\(R^2 Макфаддена\\):\n\nwith(summary(model_glm), 1 - deviance/null.deviance)\n\n[1] 0.08292194\n\n\nЕсли не хочется вычислять вручную, можно воспользоваться готовой функцией pR2:\n\n#install.packages('pscl')\nlibrary(pscl)\n\npR2(model_glm)['McFadden']\n\nfitting null model for pseudo-r2\n\n\n  McFadden \n0.08292194 \n\n\nЕще одной популярной псевдомерой является \\(R^2 Nagelkerke\\):\n\n#install.packages('fmsb')\nlibrary(fmsb)\n\nNagelkerkeR2(model_glm)\n\n$N\n[1] 400\n\n$R2\n[1] 0.1379958",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Регрессионный анализ в R</span>"
    ]
  },
  {
    "objectID": "Regression.html#мультиномиальная-логистическая-регрессия",
    "href": "Regression.html#мультиномиальная-логистическая-регрессия",
    "title": "12  Регрессионный анализ в R",
    "section": "12.3 Мультиномиальная логистическая регрессия",
    "text": "12.3 Мультиномиальная логистическая регрессия\nЧто делать, если наша зависимая переменная имеет не две, а более категорий? Для этого случая больше подходит мультиномиальная логистическая регрессия.\n\\[\nP(Y = k \\mid { X = x}) = \\frac{e^{\\beta_{0k} + \\beta_{1k} x_1 + \\cdots +  + \\beta_{pk} x_p}}{\\sum_{g = 1}^{G} e^{\\beta_{0g} + \\beta_{1g} x_1 + \\cdots + \\beta_{pg} x_p}}\n\\]\nМы не будем погружаться в технические детали, но попробуем реализовать этот подход на практике. мы воспользуемся знакомым нам набором данных iris.\nЧтобы выполнить мультиномиальный регрессионный анализ нам потребуется функция multinom из библиотеки nnet, где используется синтаксис, похожий на lm() и glm(). Лучше добавить trace = FALSE, чтобы не выводилась информация об оптимизационных процессах во время обучения.\n\nlibrary(nnet)\nmodel_multi = multinom(Species ~ ., data = iris)\n\n# weights:  18 (10 variable)\ninitial  value 164.791843 \niter  10 value 16.177348\niter  20 value 7.111438\niter  30 value 6.182999\niter  40 value 5.984028\niter  50 value 5.961278\niter  60 value 5.954900\niter  70 value 5.951851\niter  80 value 5.950343\niter  90 value 5.949904\niter 100 value 5.949867\nfinal  value 5.949867 \nstopped after 100 iterations\n\nsummary(model_multi)\n\nCall:\nmultinom(formula = Species ~ ., data = iris)\n\nCoefficients:\n           (Intercept) Sepal.Length Sepal.Width Petal.Length Petal.Width\nversicolor    18.69037    -5.458424   -8.707401     14.24477   -3.097684\nvirginica    -23.83628    -7.923634  -15.370769     23.65978   15.135301\n\nStd. Errors:\n           (Intercept) Sepal.Length Sepal.Width Petal.Length Petal.Width\nversicolor    34.97116     89.89215    157.0415     60.19170    45.48852\nvirginica     35.76649     89.91153    157.1196     60.46753    45.93406\n\nResidual Deviance: 11.89973 \nAIC: 31.89973 \n\n\nЗаметим, что на выходе у нас коэффициенты только для двух классов, так же как и в обычной регрессии у нас есть только коэффициент для одного класса.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Регрессионный анализ в R</span>"
    ]
  },
  {
    "objectID": "Regression.html#непараметрическая-регрессия.-метод-k-ближайших-соседей",
    "href": "Regression.html#непараметрическая-регрессия.-метод-k-ближайших-соседей",
    "title": "12  Регрессионный анализ в R",
    "section": "12.4 Непараметрическая регрессия. Метод k-ближайших соседей",
    "text": "12.4 Непараметрическая регрессия. Метод k-ближайших соседей\nВсе методы, которые мы рассматривали до этого момента, являются параметрическими. Это можно представить в виде обобщающей формулы.\n\\[\nf(x) = \\mathbb{E}[Y \\mid X = x]\n\\]\nНапример, типичная форма для множественной линейной регрессии:\n\\[\nf(x) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_p x_p\n\\]\nЗадача аналитика в этом случае заключается в оценке параметров модели и предсказания на их основе.\nНепараметрические методы основываются на самих данных, а не на параметрах. В этом случае используется понятие локальности.\nРассуждения при этом примерно такие: чему будет равняться y, если x равен…?\n\\[\n\\hat{f}(x) = \\text{average}(\\{ y_i : x_i = x \\})\n\\]\nПоскольку не всегда это требование выполняется, то условия чуть-чуть меняются:\n\\[\n\\hat{f}(x) = \\text{average}( \\{ y_i : x_i \\text{ equal to (or very close to) x} \\} )\n\\]\nОдним из конкретных примером использования непараметрического подхода является метод ближайших соседей:\n\\[\n\\hat{f}_k(x) = \\frac{1}{k} \\sum_{i \\in \\mathcal{N}_k(x, \\mathcal{D})} y_i\n\\]\n\n\n12.4.1 KNN в R\nПосмотрим, как работает этот метод на данных набора HousingData:\n\nlibrary(FNN)\nlibrary(MASS)\ndata(Boston)\n\nСоздаем тренировочную и тестируемые выборки:\n\nset.seed(42)\nboston_idx = sample(1:nrow(Boston), size = 250)\ntrn_boston = Boston[boston_idx, ]\ntst_boston  = Boston[-boston_idx, ]\n\n\nX_trn_boston = trn_boston[\"lstat\"]\nX_tst_boston = tst_boston[\"lstat\"]\ny_trn_boston = trn_boston[\"medv\"]\ny_tst_boston = tst_boston[\"medv\"]\n\nСоздадим дополнительный набор для переменной lstat по которым мы будем предсказывать medv для создания графики.\n\nX_trn_boston_min = min(X_trn_boston)\nX_trn_boston_max = max(X_trn_boston)\nlstat_grid = data.frame(lstat = seq(X_trn_boston_min, X_trn_boston_max, \n                                    by = 0.01))\n\nЧтобы применить метод KNN в качестве разновидности регрессионного анализа, нам понадобится функция knn.reg() из библиотеки FNN.\nЕе общая архитектура следующая:\n\nknn.reg(train = ?, test = ?, y = ?, k = ?)\n\nДанные\n\ntrain: предикторы (тренировочные данные)\ntest: предикторы на тестовых данных, \\(x\\), по которым мы хотели бы сделать предсказания\ny: зависимая переменная (на тренировочных данных)\nk: количество “соседей”\n\nРезультат:\n\nвывод функции knn.reg() представляет собой \\(\\hat{f}_k(x)\\)\n\n\npred_001 = knn.reg(train = X_trn_boston, test = lstat_grid, y = y_trn_boston, k = 1)\npred_005 = knn.reg(train = X_trn_boston, test = lstat_grid, y = y_trn_boston, k = 5)\npred_010 = knn.reg(train = X_trn_boston, test = lstat_grid, y = y_trn_boston, k = 10)\npred_050 = knn.reg(train = X_trn_boston, test = lstat_grid, y = y_trn_boston, k = 50)\npred_100 = knn.reg(train = X_trn_boston, test = lstat_grid, y = y_trn_boston, k = 100)\npred_250 = knn.reg(train = X_trn_boston, test = lstat_grid, y = y_trn_boston, k = 250)\n\nМы сделали предсказания на основе lstat, для различных значений k. Отметим, что 250 это общее количество наблюдений в тренировочном датасете.\n\n\n\n\n\n\n\n\n\n\nОранжевые “кривые” представляют собой \\(\\hat{f}_k(x)\\) где \\(x\\) это значения, которые мы определили через lstat_grid.\n\nмы видим, что k = 1 приводит к большой переобученности, так как k = 1 это очень комплексная, вариативная модель. В свою очередь, k = 250 страдает недообученностью данных, так как k = 250 это очень простой пример с маленькой дисперсией, то есть по сути, всегда будет предсказываться одно и то же значение.\n\n\n12.4.2 Выбор параметра \\(k\\)\nДилемма: - низкое значение k = слишком сложная модель - высокое значение k = слишком жесткая модель.\nГде золотая середина?\n-мы хотим минимизировать \\(\\hat{f}_k\\):\n\\[\n\\text{EPE}\\left(Y, \\hat{f}_k(X)\\right) =\n\\mathbb{E}_{X, Y, \\mathcal{D}} \\left[  (Y - \\hat{f}_k(X))^2 \\right]\n\\]\nПроведем тестирование на ошибку RMSE:\n\nrmse = function(actual, predicted) {\n  sqrt(mean((actual - predicted) ^ 2))\n}\n\n\n# создадим вспомогательную функцию, чтобы \"вытащить\" предсказанные значения\nmake_knn_pred = function(k = 1, training, predicting) {\n  pred = FNN::knn.reg(train = training[\"lstat\"], \n                      test = predicting[\"lstat\"], \n                      y = training$medv, k = k)$pred\n  act  = predicting$medv\n  rmse(predicted = pred, actual = act)\n}\n\n\n# определяем возможные значения k\nk = c(1, 5, 10, 25, 50, 250)\n\n\n# Получаем train RMSEs\nknn_trn_rmse = sapply(k, make_knn_pred, \n                      training = trn_boston, \n                      predicting = trn_boston)\n# Получаем test RMSEs\nknn_tst_rmse = sapply(k, make_knn_pred, \n                      training = trn_boston, \n                      predicting = tst_boston)\n\n# Определяем лучшее значение k\nbest_k = k[which.min(knn_tst_rmse)]\n\n# Найдем значения для переобученности, недообученности и для лучшего случая\nfit_status = ifelse(k &lt; best_k, \"Over\", ifelse(k == best_k, \"Best\", \"Under\"))\n\n\n# Суммируем результаты\nknn_results = data.frame(\n  k,\n  round(knn_trn_rmse, 2),\n  round(knn_tst_rmse, 2),\n  fit_status\n)\ncolnames(knn_results) = c(\"k\", \"Train RMSE\", \"Test RMSE\", \"Fit?\")\n\n# Отобразим результаты\nknitr::kable(knn_results, escape = FALSE, booktabs = TRUE)\n\n\n\n\nk\nTrain RMSE\nTest RMSE\nFit?\n\n\n\n\n1\n1.65\n8.32\nOver\n\n\n5\n4.98\n5.83\nOver\n\n\n10\n5.26\n5.05\nOver\n\n\n25\n5.51\n4.79\nBest\n\n\n50\n5.94\n5.05\nUnder\n\n\n250\n9.61\n8.75\nUnder\n\n\n\n\n\nВопрос на засыпку: почему при k=1 ошибка на тренировочной выборке не равна 0?\n\n\n12.4.3 Сравнение с линейной регрессией\nЕсли у нас линейная зависимость: - lm() работает хорошо - knn “подгоняет автоматически”\nЕсли связь независимая: - lm() работает плохо - или работает лучше при определенных условиях - knn “делает все автоматически”\n\n\n\n\n\n\n\n\n\nТе же шаги, но быстрее, можно осуществить с помощью библиотеки caret:\n\nmodel_knn_caret &lt;- train(\n  medv ~ .,\n  data = trn_boston,\n  method = 'knn',\n  preProcess = c(\"center\", \"scale\"), tuneLength = 20 #этот параметр позволяет рассчитать разное количество соседей\n)\n\nmodel_knn_caret\n\nk-Nearest Neighbors \n\n250 samples\n 13 predictor\n\nPre-processing: centered (13), scaled (13) \nResampling: Bootstrapped (25 reps) \nSummary of sample sizes: 250, 250, 250, 250, 250, 250, ... \nResampling results across tuning parameters:\n\n  k   RMSE      Rsquared   MAE     \n   5  5.933835  0.6382250  3.799103\n   7  5.846760  0.6503739  3.810948\n   9  5.794543  0.6596814  3.805512\n  11  5.808902  0.6644311  3.820602\n  13  5.847843  0.6650228  3.846754\n  15  5.917803  0.6610201  3.904047\n  17  5.993814  0.6563786  3.954787\n  19  6.051791  0.6564321  4.001023\n  21  6.142314  0.6497636  4.053036\n  23  6.235960  0.6442404  4.138807\n  25  6.326002  0.6388550  4.205069\n  27  6.403704  0.6345582  4.262726\n  29  6.476211  0.6296321  4.305763\n  31  6.549668  0.6248754  4.352380\n  33  6.615244  0.6199260  4.392180\n  35  6.671859  0.6178162  4.432505\n  37  6.738799  0.6115130  4.475878\n  39  6.800152  0.6062191  4.517187\n  41  6.842031  0.6027472  4.542485\n  43  6.881135  0.6002764  4.584084\n\nRMSE was used to select the optimal model using the smallest value.\nThe final value used for the model was k = 9.\n\n\n\nknnPredict &lt;- predict(model_knn_caret, newdata = tst_boston)\nrsq_knn_cv &lt;- cor(knnPredict, tst_boston$medv) ^ 2\nrsq_knn_cv\n\n[1] 0.7549495",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Регрессионный анализ в R</span>"
    ]
  },
  {
    "objectID": "Regression.html#самостоятельная-работа",
    "href": "Regression.html#самостоятельная-работа",
    "title": "12  Регрессионный анализ в R",
    "section": "12.5 Самостоятельная работа",
    "text": "12.5 Самостоятельная работа\n\nПровести регрессионный анализ данных об успеваемости студентов и определяющих их фактора.\n\nНезависимые переменные:\n\nHours Studied: Общее количество часов, потраченных на учебу каждым студентом.\nPrevious Scores - предшествующие результаты: Баллы, полученные студентами на предыдущих экзаменах.\nExtracurricular Activities - Внеклассная деятельность: Участвует ли студент во внеклассных мероприятиях (да или нет).\nSleep Hours - Часы сна: Среднее количество часов сна студента в сутки.\nSample Question Papers Practiced: Количество пробных экзаменационных работ, которые студент практиковал.\n\nЦелевая переменная:\n\nPerformance Index: Показатель общей успеваемости каждого студента. Индекс успеваемости представляет собой академическую успеваемость студента и округляется до ближайшего целого числа. Индекс варьируется от 10 до 100, при этом более высокие значения свидетельствуют о более высокой успеваемости.\n\nСкачать данные\n\nПровести анализ методом логистической регрессии на данных по климату. В качестве зависимой переменной будет выступать вопрос про оценку опасности проживания вблизи ледников (вопрос 19) , а в качестве объясняющих - пол, возраст и переменная проживания в определенном районе (type).",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Регрессионный анализ в R</span>"
    ]
  },
  {
    "objectID": "Statistical-Inference.html",
    "href": "Statistical-Inference.html",
    "title": "10  Статистический вывод и тестирование исследовательских гипотез",
    "section": "",
    "text": "10.1 Общее понятие и логика статистического вывода\nСтатистический вывод (statistical inference) представляет собой процесс получения логических выводов о генеральной совокупности и ее свойствах на основании данных выборочного исследования.\nНа основании выборки исследователь тестирует те или иные гипотезы, часто:\nРезультатом статистического вывода является статистическое суждение, например:\nЛогика статистического вывода представляет собой порядок действий аналитика при проведении статистического анализа. В целом, она не зависит от конкретной проблемы и используемых статистических методов, однако, на практике, благодаря большому репертуару статистических инструментов, конечно, имеет свои особенности.\nЭтапы:",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Статистический вывод и тестирование исследовательских гипотез</span>"
    ]
  },
  {
    "objectID": "Statistical-Inference.html#общее-понятие-и-логика-статистического-вывода",
    "href": "Statistical-Inference.html#общее-понятие-и-логика-статистического-вывода",
    "title": "10  Статистический вывод и тестирование исследовательских гипотез",
    "section": "",
    "text": "о различии статистических совокупностей (представления о распределении семейных обязанностей различаются у мужчин и женщин),\nо наличии закономерностей (на основе анализа совокупности явлений) об отсутствии случайностей (например, случайности распределения данных в последовательности)\n\n\n\nточечная оценка (например, среднее значение, факторная нагрузка)\nдоверительный интервал (для статистического параметра)\nотвержение гипотезы.\n\n\n\n\n\n\n\nСовет\n\n\n\nСтатистический анализ = анализ описательных статистик + статистический вывод\n\n\n\n\n\nФормулировка статистических гипотез (нулевых и альтернативных), позволяющих подтвердить существующую теорию или доказать авторскую.\nВыбор статистического критерия (метода анализа), позволяющего подтвердить гипотезы и расчет его статистических значений.\nОпределение статистической значимости (p-value) и доверительных интервалов.\nВывод о сохранении нулевых или подтверждении выдвинутых гипотез.\n\n\n10.1.1 Статистические гипотезы\nСтатистическая гипотеза - предположение о виде распределения и свойствах случайной величины, которое можно подтвердить или опровергнуть применением статистических методов к данным выборки.\nНулевая гипотеза (\\(H_0\\), null hypothesis) – содержит предположение об отсутствии различий, влияния фактора, различия значения выборочной характеристики от заданной величины (например, нуля) и т. п. Как правило, Н0 не является для исследователя содержательной гипотезой, т. е. предметом и целью доказательства.\nАльтернативная гипотеза (\\(H_1\\), alternative hypothesis) – другое проверяемое предположение, конкурирующая гипотеза (о наличии различий, взаимосвязей, отсутствии случайности, отличии от нуля и пр.). Обычно, за исключением некоторых случаев, профессиональный интерес исследователя сводится именно к верификации альтернативной гипотезы.\nНулевая гипотеза сохраняется или отвергается исходя из того, насколько вероятным оказывается наблюдаемый результат.\nДля оценки статистических гипотез используются статистические критерии (математические правила), для которых имеются рассчитанные распределения и по которым эти вероятности можно посчитать.\n\nПримеры: критерии согласия (Пирсона, Колмогорова-Смирнова), проверки на однородность (например, тест Ливиня), параметрические (t-критерий, коэффициент корреляции Пирсона – содержат в формулах средние и дисперсии) и непараметрические критерии (Манна-Уитни, Уилкоксона, часто имеют ранговый характер).\n\n\nС каждым критерием связана некоторая статистика \\(S\\), которая измеряет отклонение в наблюдаемом процессе от ситуации, соответствующей \\(H_0\\).\n\nВ силу случайности извлекаемых выборок случайными оказываются и значения статистики \\(S\\), вычисляемые в соответствии с этими выборками. То есть, если мы много раз будем извлекать выборки из генеральной совокупности, значения статистики будут отличаться, и разница между ними будет носить случайный характер.\n \nПри справедливости проверяемой гипотезы \\(H_0\\) статистика \\(S\\) подчиняется некоторому распределению \\(g(𝑆│𝐻_0 )\\), например, нормальному распределению, t-распределению, распределению \\(\\chi^2\\) и др.\nВ этом распределении выделяется два множества: случайных отклонений и критических значений. Если статистика попадает в область критических значений – нулевая гипотеза отклоняется, в противном случае – нет.\n\n\n\n\n\n\n\n\n\n\nРисунок 10.1: Области критических значений\n\n\n\n\n\n10.1.2 Доверительная вероятность и доверительный интервал\nЧтобы отклонить нулевую гипотезу, мы выбираем субъективное суждение относительно уровня риска, который мы готовы принять для того, чтобы ошибиться. Этот риск оказывается отраженным в понятиях доверительного интервала и доверительной вероятности.\nДоверительный интервал (confidence interval, \\(CI\\))- диапазон, в котором находятся истинные средние значения в генеральной совокупности с определенной доверительной вероятностью.\nДоверительная вероятность (confidence level, \\(CL\\)) – вероятность того, что доверительный интервал содержит значение оцениваемого параметра.\nТипичные значения доверительной вероятности – 90%, 95% (чаще всего), 99%. Чем больше доверительная вероятность, тем шире (и иногда бесполезнее) интервал.\nПример: с 95% вероятностью можно утверждать, что данного мнения о реализации национального проекта придерживаются 48% до 73% жителей региона.\n \n\n\n10.1.3 Ошибки при оценке статистических гипотез\nОшибка первого рода – показывает вероятность того, что мы найдем различия там, где их на самом деле нет! Нездоровые сенсации, большой вред (Пример: поставили диагноз, а болезни нет, признали виновным, а вины нет)\nМожем контролировать путем подбора порога значений, ниже которого будем считать, что различий нет, то есть уровня значимости.\nТипичный порог – \\(\\alpha\\) = 0,05\nОшибка второго рода – различия есть, но мы их не нашли. Близорукость, слепота критерия, мы ее не можем контролировать! Вред небольшой.\nМинимизировать ошибку второго рода можно путем подбора статистического критерия.\nОшибку первого рода можно совершить, только, если мы отвергли \\(H_0\\), а ошибку второго рода – если мы приняли \\(H_0\\). Сразу две ошибки совершить нельзя!\nОптимальная величина α (критический уровень значимости) должна удовлетворять двум противоречивым требованиям:\n\nона должна быть достаточно мала, чтобы обеспечить высокое доверие к выводу об отклонении \\(H_0\\)\nно она должна быть достаточно велика, чтобы реже допускать ошибки 2-го рода\n\nПри этом вероятность ошибки 𝛽 уменьшается при увеличении значения 𝛼, а для фиксированного значения 𝛼:\n\nпри увеличении объема выборки;\nпри уменьшении выборочной дисперсии.\n\n\n\n\n\n\n\nРисунок 10.2: Ошибки первого и второго рода\n\n\n\n\n\n10.1.4 Односторонние и двусторонние критерии. Определение уровня значимости\nВ случае одностороннего критерия (one-tailed, one-sided) полученное значение статистики \\(S^*\\) сравнивают с критическим значением \\(𝑆_{(1−\\alpha)}\\) при заданном уровне значимости \\(\\alpha\\) или делают вывод на основе «достигнутого уровня значимости» (p-value): вероятности возможного превышения полученного значения статистики при справедливости \\(H_0\\).\nОдносторонний критерий применяется для оценки направленных гипотез, в которых содержатся утверждения «больше (выше)» или «меньше (ниже)».\nПример: уровень доверия к некоммерческим организациям у женщин выше, чем у мужчин.\n\n\n\n\n\n\nРисунок 10.3: Графическая интерпретация одностороннего статистического критерия\n\n\n\nВ случае двустороннего критерия (two-tailed, two-sided) критическая область состоит из двух частей. И проверяемая гипотеза \\(H_0\\) отклоняется, если \\(S^∗&gt;𝑆_{(\\alpha/2)}\\) или \\(S^∗&lt;S_{1-\\alpha /2}\\).\nДвусторонний критерий применяется для оценки ненаправленных гипотез (действуют в обе стороны), в которых содержатся утверждения «отличается» или «не равен».\nПример: уровень доверия к некоммерческим организациям у женщин у мужчин различается.\n\n\n\n\n\n\nРисунок 10.4: Графическая интерпретация двустороннего статистического критерия\n\n\n\n\n\n10.1.5 Мощность статистического критерия\nМощность статистического критерия — это способность критерия обнаружить эффект, в случае если этот эффект действительно существует. С точки зрения статистики, это вероятность справедливого опровержения нулевой гипотезы.\nПри проверке любой статистической гипотезы желательно использовать наиболее мощный критерий, который для заданной вероятности \\(𝛼\\) ошибки 1-го рода обеспечивает минимальную вероятность \\(𝛽\\) ошибки 2-го рода относительно любой конкурирующей гипотезы \\(H_1\\).\nЖелательно всегда, если позволяют данные, применять более мощный критерий, так как это позволяет избежать ошибки 2-го рода.\n\n\n\n\n\n\nРисунок 10.5: Плотности распределения статистик при справедливости соответственно гипотез \\(H_0\\) и \\(H_1\\) в случае двустороннего критерия.\n\n\n\n\n\n10.1.6 Выбор метода для анализа\nВыбор метода, с помощью которого будут анализироваться данные, осуществляется еще на этапе разработки программы научного исследования на основе его цели и задач, определяющих общий дизайн, методологические подходы и основные показатели исследования. Сразу, на «берегу», если исследователь делает выбор в пользу «количественной» аналитический стратегии, решается вопрос относительно объема и принципов формирования выборочной совокупности, необходимой для получения достаточных данных для доказательства исследовательских гипотез, а также обрисовываются контуры инструментария исследования, в который оказываются вплетены не только тщательно операционализированные и эмпирически интерпретированные понятия, но и подразумеваемые уровни измерения соцологических явлений и феноменов, от которых будет зависеть выбор конкретного статистического метода. Так, например, в исследовании перспектив развития гражданского общества в регионах России, может ставиться исследовательский вопрос об участии граждан в деятельности общественных, благотворительных организаций.\nОт того, в какой форме будет задан вопрос, как будет сформулирована исследовательская задача: от простого установления факта такого участия (что может быть достигнуто вопросом: «Участвуете ли Вы в деятельности какой-либо общественной организации или благотвориктельного общества?“) до количественного измерения вклада в деятельность НКО путем определения временных или трудозатрат («Сколько часов своего личного времени Вы тратите на общественную деятельность?», «Если перевести затраченное время в денежный эквивалент, согласно получаемой Вами зарплате на основном месте работы, сколько Вы тратите на помощь общественной организации?») будет зависеть, к какому уровню измерения (категориальному или уровню отношений) будут отнесены данные, и какой статистический метод может быть использован для проверки гипотез о том, какие слои населения в большей степени принимают участие в деятельности общественных организаций, или какие профессионалы вносят больший вклад, участвуя в программах pro bono.\nЧтобы выбрать метод, с помощью которого можно было бы проверить статистические гипотезы, нужно выполнить ряд простых действий:\n\nУточнить тип данных (количественные или качественные)\nВ случае количественных данных уточнить тип распределения (нормальное или отличное от нормального)\nУточнить количество сравниваемых групп (если они есть)\nУточнить, связаны ли группы сравнения между собой, т. е. являются ли единицы наблюдения в группах разными носителями признака (независимые группы), или это одни респонденты, которых опрашивали несколько раз (связанные группы).\n\nОтветы на эти вопросы будут определять выбор статистического метода.\nПримерная схема принятия решения для случая, когда планируется сравнивать выраженность количественного признака в одной или нескольких группах, может быть следующей:\n\n\n\n\n\nflowchart LR\n  A[Сколько групп сравнивается?] --&gt;F[1]\n  subgraph Сравнение с заданным значением\n  F--&gt;|Параметрические|O[z-критерий \\n t-критерий]\n  F--&gt;|Непараметрические|P[Критерий \\n Уилкоксона]\n  end\n  A--&gt;D[2]\n  A--&gt;J[3+]\n  subgraph Зависимые группы\n  D--&gt;|Параметрические|T[t-критерий \\n для парных \\n выборок]\n  D--&gt;|Непараметрические|L[Критерий \\n Уилкоксона]\n  J --&gt;|Параметрические|K[Дисперсионный анализ \\n для повторных \\n измерений]\n  J--&gt;|Непараметрические|M[Критерий \\n Фридмана]\n  end\n  A--&gt;R[2]\n  A--&gt;Q[3+]\n  subgraph Независимые группы \n  R--&gt;|Параметрические|S[t-критерий для \\n независимых выборок]\n  R--&gt;|Непараметрические|U[U-критерий \\n Манна-Уитни]\n  Q--&gt;|Параметрические|W[Однофакторный \\n дисперсионный анализ]\n  Q--&gt;|Непараметрические|Y[Непараметрический \\n дисперсионный анализ\\n Краскела-Уоллиса]\n  end\n\n\n\n\n\n\nЕсли в центре анализа находятся качественные данные, то рассуждения аналитика могут выстраиваться таким образом:\n\n\n\n\n\nflowchart LR\n  classDef dark color:#fff,fill:#0d5caa\n  A[Сколько групп сравнивается]--&gt;B[Одна]\n  A--&gt;C[Две]\n  A--&gt;D[Три и больше]\n  subgraph Сравнение с заданным значением\n  B--&gt;E[z-критерий]\n  end\n  subgraph Зависимые группы\n  C--&gt;F[Критерий Мак-Немара]\n  D--&gt;G[Q-критерий Кокрена]\n  end\n  A--&gt;K[Две]\n  A--&gt;L[Три и больше]\n  subgraph Независимые группы\n  K--&gt;I[Хи-квадрат Пирсона]\n  L--&gt;J[Хи-квадрат Пирсона с поправкой на правдоподобие]\n  end\n  class C,K,F,I dark\n\n\n\n\n\n\nРассмотрим каждый из приведенных критериев подробнее.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Статистический вывод и тестирование исследовательских гипотез</span>"
    ]
  },
  {
    "objectID": "Statistical-Inference.html#сопоставление-количественных-результатов-с-заданным-значением",
    "href": "Statistical-Inference.html#сопоставление-количественных-результатов-с-заданным-значением",
    "title": "10  Статистический вывод и тестирование исследовательских гипотез",
    "section": "10.2 Сопоставление количественных результатов с заданным значением",
    "text": "10.2 Сопоставление количественных результатов с заданным значением\n\n10.2.1 z-критетий и t-критерий Стьюдента для одной выборки\nВ том случае, когда мы имеем какой-либо количественный показатель и хотим сравнить со средним значением в генеральной совокупности, которое известно заранее (из ранее проведенных исследований, регламентов, переписей населения и др. источников), мы можем для этого использовать z и t-критерии.\nОба теста преследуют весьма сходные цели - сравнить выборочные средние значения с гипотетическими значениями, и являются параметрическими, что означает соблюдение требования о непрерывности распределения и нормальности данных.\nИх отличия кроются в используемых статистических распределениях (z - критерий использует нормальное распределение, тогда как t-критерий, естественно, t-распределение с более «тяжелыми» хвостами, что делает его более консервативным), а также отношение к стандартному отклонению. Если стандартное отклонение в генеральной совокупности известно, предпочтительнее использовать z-критерий, если его нет - одновыборочный критерий Стьюдента.\n\n10.2.1.1 z-критерий\nДля того, чтобы корректно использовать z-критерий для тестирования гипотез, необходимо выполнять следующие требования:\n\nпоказатель, который мы исследуем, в генеральной совокупности должен иметь нормальное распределение (проверить практически нереально, но мы можем сделать проверку выборочного распределения в качестве «прокси»);\nнаблюдения в выборке должны быть независимыми, а сама выборка - случайной\nмы должны знать стандартное отклонение в генеральной совокупности\nразмер выборки должен быть достаточно большим, по меньшей мере \\(n &gt; 30\\).\n\nКакие гипотезы мы формулируем, когда используем z-критерий?\nЧто касается нулевой гипотезы, то здесь все просто:\n\\(H_0: \\bar{x}=\\mu_0\\) – выборочное среднее равно предполагаемому среднему в генеральной совокупности\nА вот альтернативная гипотеза может быть сформулирована тремя возможными способами:\n\n\\(H_{a1} : \\bar{x}&lt;\\mu_0\\) – выборочное среднее меньше предполагаемого среднего значения (левостронний тест)\n\\(H_{a2} : \\bar{x}&gt;\\mu_0\\) – выборочное среднее больше предполагаемого среднего значения (правосторонний тест)\n\\(H_{a3} : \\bar{x} \\neq \\mu_0\\) – выборочное среднее отличается от предполагаемого среднего значения (двусторонний тест)\n\nФормула для расчета z-критерия: \\[z=\\frac{\\bar{x}-\\mu_0}{\\sigma \\ \\sqrt{n}},\\]\nгде: \\(\\bar{x}\\) – выборочное среднее \\(\\mu_0\\) – гипотетическое среднее, с которым мы сравниваем выборочное среднее \\(n\\) – объем выборки \\(\\sigma\\) – гипотетическое стандартное отклонение\n\n Пример: Предположим, вы хотите оценить уровень удовлетворенности студентов материально-техническим оснащением на вашем университетском кампусе. Ваша гипотеза состоит в том, что средний уровень удовлетворенности составляет 6.5, что было определено в результате прошлогоднего исследования, и хотите проверить, верно ли это. Вы знаете, что стандартное отклонение оценок составляет 1,4.\n\n\nВы случайным образом отобрали 100 студентов и попросили их оценить свой уровень удовлетворенности по шкале от 1 до 10. После сбора данных, вы рассчитали среднюю оценку уровня удовлетворенности студентов, которая оказалась равной 6,8, со стандартным отклонением 1,2.\n\nСгенерируем аналогичные данные с помощью функции rnorm():\n\nset.seed(123)\ndata&lt;-rnorm(n=100, mean = 6.8, sd = 1.2)\n\nПоскольку мы заранее определили, что наши данные происходят из нормального распределения, довольно бессмысленно их проверять на нормальность. В доказательство сделаем гистограмму:\n\nhist(data)\n\n\n\n\n\n\n\nРисунок 10.6: Проверка данных на нормальность графическим способом\n\n\n\n\n\nУстановим пакет BSDA:\n\ninstall.packages(\"BSDA\")\n\nПосчитаем статистику теста:\n\nlibrary(\"BSDA\")\nz.test(data,\nalternative = \"two.sided\",\nmu = 6.5,\nsigma.x = 1.4,\nconf.level = 0.95\n)\n\n\n    One-sample z-Test\n\ndata:  data\nz = 2.9178, p-value = 0.003526\nalternative hypothesis: true mean is not equal to 6.5\n95 percent confidence interval:\n 6.634092 7.182882\nsample estimates:\nmean of x \n 6.908487 \n\n\nРезультаты теста показывают, что оценка удовлетворенности студентов значимо отличается от гипотетической.\nВизуально это можно представить в виде графика, показывающего, насколько далеко полученное z-значение находится от нулевой отметки, символизирующей отсутствие отличий эмпирического и теоретического средних значений. Поскольку мы использовали ненаправленную гипотезу (средние отличаются, но не понятно, как именно) и двусторонний тест, то и вероятности мы будем считать тоже как бы «в обе стороны». По тесту у нас получилось p-значение = 0,003 (вероятность ошибочно отвергнуть нулевую гипотезу составляет 0.3%) и мы разбиваем его на две части - по 0,0015 (около 0,15%) с каждой стороны.\n\n\n\n\n\n\n\n\nРисунок 10.7: Графическая интерпретация z-теста (двусторонний критерий)\n\n\n\n\n\nЕсли бы мы ставили гипотезу о том, что получившееся среднее превышает гипотетическую величину (мы предполагали бы, что оценка удовлетворенности выросла), то есть использовали бы направленную гипотезу и, соответственно, односторонний критерий, то результаты были бы следующими:\n\n\n\n\n\n\n\n\nРисунок 10.8: Графическая интерпретация z-теста (односторонний критерий)\n\n\n\n\n\n\n\n10.2.1.2 Одновыборочный t-критерий\nОдновыборочный t-критерий «ведет себя» в целом аналогично z-критерию: он также применяется к данным, подчиняющимся закону нормального распределения.\nНулевая гипотеза:\n\n\\(H_0:m=μ\\)\n\nАльтернативные гипотезы:\n\n\\(H_a:m≠μ\\) (двусторонний критерий)\n\\(H_a:m&gt;μ\\) (правосторонний)\n\\(H_a:m&lt;μ\\) (левосторонний)\n\nФормула:\n\\[t = \\frac{m-\\mu}{s/\\sqrt{n}}\\]\nРазберем возможности применения критерия на следующем примере.\n\n Пример 2: Минимальный размер оплаты труда в Алтайском крае с 1 января 2024 года составляет 20 454 руб. Вы работаете в научно-исследовательском центре и занимаетесь социально-экономическими исследованиями. В результате опроса населения были получены следующие данные о заработной плате жителей одного из сел. Докажите, что среднее значение в выборочной совокупности отличается от установленного минимального размера заработной платы в регионе.\n\nРассмотрим следующие данные:\n\ndata&lt;-c(18431, 21211, 18200, 17502, 25581, 29684, 30319, 27533, 15328, 30801, 18650, 22702, 17807, 35468, 17693, 17966, 21690, 19580, 24581, 25817, 28493, 33954, 22030, 22300, 16290, 15371, 26745, 20320, 21226, 20522)\n\nВычислим t-критерий, сравнив выборочное среднее с минимальным размером оплаты труда\n\nt_test &lt;- t.test(data, mu = 20454)\nt_test \n\n\n    One Sample t-test\n\ndata:  data\nt = 2.3188, df = 29, p-value = 0.02765\nalternative hypothesis: true mean is not equal to 20454\n95 percent confidence interval:\n 20730.00 24856.33\nsample estimates:\nmean of x \n 22793.17 \n\n\nРезультаты анализа указывают на то, что средняя зарплата в данном населенном пункте достоверно отличается от минимального размера оплаты труда в регионе (\\(t_{29}=2,3188, p=0,028\\)). Мы видим, что сравниваемое значение 20454 не попадает в 95% интервал, границы которого определяются значениями 20730,0 и 24856,33. Откуда, кстати, берутся эти значения?\nЭто станет понятным, если взглянуть на формулу доверительных интервалов для t-критерия: \\[\\left( \\bar x + t_{n-1, \\alpha / 2} \\cdot \\frac{s}{\\sqrt{n}},\n  \\bar x + t_{n-1, 1 - \\alpha / 2} \\cdot \\frac{s}{\\sqrt{n}} \\right)\\],\nгде:\n\n\\(\\bar{x}\\) - среднее значение\n\\(t_{n-1, \\alpha / 2}\\) - квантиль \\(\\alpha /2\\) t-распределения с \\(n-1\\) степенями свободы\n\\(s = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})^2}\\) - выборочное стандартное отклонение\n\\(n\\) - размер выборки\n\nПопробуем воспроизвести доверительные интервалы, используя возможности библиотеки distributions3:\n\n#install.packages(\"distributions3\")\nlibrary(distributions3)\nT_9&lt;-StudentsT(df=29)\nmean(data) + quantile(T_9, 0.05 / 2) * sd(data) / sqrt(30)\n\n[1] 20730\n\nmean(data) + quantile(T_9, 1 - 0.05 / 2) * sd(data) / sqrt(30)\n\n[1] 24856.33\n\n\nМы получили аналогичные результаты, что нас утверждает в мысли, что мы на правильном пути. Посмотрим, как результаты проверки гипотезы с помощью t-критерия можно представить графически:\n\n\n\n\n\n\n\n\nРисунок 10.9\n\n\n\n\n\n\n\n\n10.2.2 Критерий Уилкоксона\nВ том случае, когда по каким-то причинам мы не можем применять параметрические критерии (например, из-за погрешностей в распределении - слишком большой асимметрии), альтернативой z- и t-критериям может являться критерий знаковых рангов Уилкоксона (Wilcoxon signed-rank test). Он был разработат Фрэнком Уилкоксонов в 1945 году, и является одним из самых первых «непараметрических» тестов.\nВ отличие от рассмотренных выше тестов в нем сравниваются не средние значения, а медианы, что делает тест устойчивым к экстремальным значениям.\nНулевая и альтернативная гипотезы также формулируются в терминах выборочной медианы (\\(m\\)), которая сравнивается с другим медианным значением из генеральной совокупности, которое определяет исследователь:\n\n\\(H_0 : m = m_0\\)\n\\(H_1 : m \\neq m_0\\) (двусторонняя)\n\\(H_1 : m &gt; m_0\\) (правосторонняя)\n\\(H_1 : m &lt; m_0\\) (левосторонняя)\n\nКак и в другом тесте здесь есть свои допущения:\n\nраспределение оценок (различий между эмпирическим и теоретическим значением) должно быть симметричным (то есть должны быть разные данные - те, которые отклоняются от тестируемого значения в положительную сторону и в отрицательную сторону, в противном случае, тест проводить не стоит);\nвыборка должна быть случайной, а наблюдения независимыми друг от друга.\n\nОдновыборочный тест Уилкоксона основан на следующей тестовой статистике. Могут быть использованы два способа подсчета, которые приводят к идентичным результатам.\nОбозначим первый как \\(W_1\\) (также известный как \\(T\\)), а второй как \\(W_2\\). Для того, чтобы посчитать статистику критерия по каждому способу, нужно осуществить следующие действия:\n\nДля каждого значения выявить знак отличия с теоретическим значением: \\(sign_d=sgn(score−m_0)\\) . Показатель \\(sign\\) равен 1, если различия больше нуля, -1, если различия меньше нуля, и 0 - если равны нулю.\nДля каждого значения посчитать абсолютную разницу с теоретическим значением: \\(|score−m_0|\\).\nУдалить значения, различия по которым равны нулю (по этому поводу есть несколько спорных мнений - удалять или не удалять, а сдвигать значения на небольшую константу, но мы будем следовать «классическому объяснению»). После удаления «нулей» окончательный объем выборки становится равным \\(N_r\\).\nПрисвоить ранги \\(R_d\\) всем абсолютным разностям в \\(N_r\\). Наименьшему значению присваивается ранг 1, а наибольшему - ранг, соответствующий \\(N_r\\). Если есть повторные ранги (ties), то они заменяются на средние значения.\n\nДалее необходимо посчитать сумму рангов, соответствующих положительным различиям: \\(W_1=\\Sigma R^+_d\\)\nи\n\\(W_2=\\Sigma sign_d × R_d\\) (то есть просто умножить разницу на знак, а затем суммировать все произведения).\nРаспределение статистики \\(W_1\\) при условии, что верна нулевая гипотеза (\\(H_0\\)) и \\(N_r\\) достаточно большое, приближается к нормальному со средним значением \\(\\mu W_1\\) и стандартным отклонением \\(\\sigma W_1\\), которые рассчитываются по следующим формулам:\n\\[\\mu_{W_1} = \\frac{N_r(N_r + 1)}{4}\\] и\n\\[\\sigma_{W_1} = \\sqrt{\\frac{N_r(N_r + 1)(2N_r + 1)}{24}}\\]\nСледовательно, стандартизированная z-статистика принимает следующий вид:\n\\[z = \\frac{W_1 - \\mu_{W_1}}{\\sigma_{W_1}}\\]\nРаспределение статистики \\(W_2\\) при достаточно большом количестве выборки также имеет нормальное распределение со средним значением \\(0\\) и стандартным отклонением \\(\\sigma W_2\\):\n\\[ \\sigma_{W_2} = \\sqrt{\\frac{N_r(N_r + 1)(2N_r + 1)}{6}}\\]\nСтандартизированная статистика рассчитывается в этом случае по формуле:\n\\[z = \\frac{W_2}{\\sigma_{W_2}}\\]\nЕсли выборки маленькие (менее 50 наблюдений), следует использовать точные распределения.\nВернемся к нашему первому примеру и попробуем применить к этим же данным тест Уилкоксона:\n\nset.seed(123)\ndata&lt;-rnorm(n=100, mean = 6.8, sd = 1.2)\n\nres &lt;- wilcox.test(data, mu = 6.5)\n# Printing the results\nres\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  data\nV = 3506, p-value = 0.0007482\nalternative hypothesis: true location is not equal to 6.5",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Статистический вывод и тестирование исследовательских гипотез</span>"
    ]
  },
  {
    "objectID": "Statistical-Inference.html#сравнение-исследуемого-признака-в-двух-и-более-независимых-выборках",
    "href": "Statistical-Inference.html#сравнение-исследуемого-признака-в-двух-и-более-независимых-выборках",
    "title": "10  Статистический вывод и тестирование исследовательских гипотез",
    "section": "10.3 Сравнение исследуемого признака в двух и более независимых выборках",
    "text": "10.3 Сравнение исследуемого признака в двух и более независимых выборках\n\n10.3.1 t-критерий\nТ-критерий для независимых выборок (t-тест, t-критерий Стьюдента) предназначен для сравнения средних в двух независимых группах с целью предоставления статистического обоснования того, что и в основной популяции (генеральной совокупности) соответствующие средние так же различаются. Своим именем этот критерий обязан английскому химику и математику Уильяму Сили Госсету, работавшему в конце XIX века на пивоварне Гиннесс в Дублине. Владельцы пивоварни поставили перед ученым весьма амбициозную задачу - сохранить репутацию лучших пивоваров и вывести процесс производства пива на новый уровень, что потребовало проведения исследований в области контроля качества. Как известно, производство пива - процесс не только высокотехнологичный, требующий четкости следования процедурам, но и результат случайного стечения обстоятельств, поскольку натуральные продукты, из которых производится пиво - хмель, ячмень, солод, как и другие сельскохозяйственные культуры, имеют большую степень вариабельности вкусовых качеств из-за влияния состава почвы, климата и других факторов. Задача Госсета как подмастерья пивовара заключалась не только в оценке качества этих продуктов, но и том, чтобы сделать это наиболее экономным и точным образом.\n\n\n\n\n\n\nРисунок 10.10: Памятная табличка в Дублине, на складе пивоварни Гиннесс\n\n\n\nРаботая с небольшими выборками, Госсет заметил, что распределение средних значений отклонялось от нормального распределения, и, значит, он не мог использовать обычные статистические методы, основанные на нормальном распределении для того, чтобы принимать решения.\nВ 1904 году Госсет опубликовал внутренний отчет, в котором математически обосновал свой «закон ошибок» среднего (Law of Error) и возможности его применения для нужд пивоварни. В своей работе он описал, что «чем больше наблюдений, по которым рассчитано среднее значение, тем меньше (вероятная) ошибка». Госсет также отметил, что в сравнении с нормальным распределением, «кривая, представляющая частоту ошибок становится выше и уже по мере того, как объем выборки уменьшается. Управление Гинесса предложило Госсету проконсультироваться с другими специалистами, и так состоялась встреча Госсета с Карлом Пирсоном, под руководством которого в 1908 году в журнале «Биометрика» и появляется знаменитая работа Госсета об оценке ошибки среднего, напсанная под псевдонимом Стьюдент. Госсет не мог публиковаться под своим собственным именем, поскольку речь шла о данных, составляющих коммерческую тайну, а завод Гинесса очень строго относился к своим данным. Поэтому, вероятно, под влиянием названия на блокноте, который он использовал для ведения записей (The Student’s Science Notebook), Госсет выбрал такой псевдоним, который он использовал в 19 из своих 21 научных работ (Brown).\n Примеры: сравнение результатов академической успеваемости в двух группах учащихся, доходов у мужчин и женщин, доверия к социальным институтам среди городских и сельских жителей и др.\nТребования:\n\nЗависимая переменная должна быть непрерывной, измеренной по интервальной шкале или шкале отношений Независимая переменная - категориальная (номинальная), состоящая из двух и более групп (но мы сравниваем только две)\nДля анализа отбираются только случаи, где есть валидные значения по обеим переменным (отсутствующие значения удаляются)\nСравниваемые группы должны быть независимыми (должно выполняться требование независимости наблюдений). Что это значит?\n\n\nМежду наблюдениями в разных группах нет никаких взаимосвязей, то есть:\nСубъекты (респонденты) в первой группе не могут быть одновременно и во второй группе (респонденты в каждой группе - разные)\nСубъекты (респонденты) одной группы никаким образом не могут влиять на субъектов (респондентов) в другой группе (например, в одной группе - родители, в другой - дети, или в одной начальники, а в другой - подчиненные и т. д.) Независимость двух выборок означает, что средние значения «будут совершенно некоррелируемыми для бесконечного множества пар выборок».\n\n\nВыборка должна быть случайной\nЗависимая переменная в каждой группе должна быть нормально распределена Ненормальное распределение, особенно с «тяжелыми хвостами» или слишком большой асимметрией значительно снижает мощность теста (его способность отвергать нулевую гипотезу) В случае если выборка по размеру средняя или большая, нарушениями нормальности можно пренебречь, так как они меньше влияют на величину ошибки \\(p\\).\nГомогенность дисперсий (то есть дисперсии в группах должны быть практически равными) Когда это требование нарушается и размеры групп не совпадают, значению \\(p\\) нельзя доверять.К счастью, R позволяет рассчитать модифицированную статистику t-критерия по формулам, которые не основываются на допущении о равенстве дисперсий. Этот альтернативный критерий носит название t-критерия Уэлча, он также известен под названиями t-критерия для неравных дисперсий (Unequal Variance t-Test или Separate Variances t-Test).\nВ измерениях не должно быть выбросов (можно проверить по ящичной диаграмме).\n\nНулевая гипотеза (\\(H_0\\)) и альтернативная гипотеза (\\(H_1\\)) при использовании t-критерия Стьюдента может быть выражена двумя аналогичными способами:\n\n\\(H_0: \\mu_1 = \\mu_2\\) («средние в двух группах равны»)\n\\(H_1: \\mu_1 \\neq \\mu_2\\) («средние в двух группах не равны»)\n\nИЛИ\n\n\\(H_0: \\mu_1- \\mu_2 = 0\\) («различия между средними равны 0»)\n\\(H_1:\\mu_1 - \\mu_2 \\neq 0\\) («различия между средними в двух группах не равны 0»)\n\nгде \\(µ_1\\) и \\(µ_2\\) это средние в генеральной совокупности для группы 1 и группы 2, соответственно. Заметим, что вторая группа гипотез выводится просто путем переноса \\(\\mu_2\\) в левую часть уравнения (неравенства) - или путем ее вычитания из обеих частей.\nКогда доказано, что две выборки происходят из групп генеральной совокупности с равными дисперсиями \\(\\sigma^2_1=\\sigma^2_2\\)), статистика t-критерия рассчитывается по формулам:\n\\[t=\\frac{\\bar{x_1}-\\bar{x_2}}{s_p/\\sqrt{n_\\sigma}}\\]\nгде\n\\[s_p=\\sqrt{\\frac{(n_1-1)*s^2_1+(n_2-1)*s^2_2}{n_1+n_2-2}}\\]\nа\n\\[n_\\sigma=\\frac{1}{\\frac{1}{n_1}+\\frac{1}{n_2}}\\]\nгде \\(\\bar{x_1}\\),\\(\\bar{x_2}\\) – средние значения в сравниваемых выборках, \\(n_1, n_2\\) – количество наблюдений в первой и второй группах, \\(s_1, s_2\\) – стандартные отклонения в первой и второй группах, \\(s_p\\) – объединенное стандартное отклонение.\nРаспределение статистики t-критерий является t-распределением Стьюдента с \\(df\\) - степенями свободы. Если гипотеза о равенстве дисперсий подтверждается, то количество степеней свободы подсчитывается по формуле \\(n_1+n_2-2\\).\nКогда независимые выборки (и соответствующие им группы в генеральной совокупности) имеют неравные дисперсии (то есть, \\(𝜎_1^2≠𝜎_2^2\\)), t-критерий рассчитывается по формуле (известной также как t-критерий Уэлча):\n\\[t=\\frac{\\bar{x_1}-\\bar{x_2}}{\\sqrt{\\frac{s^2_1}{n_1}+\\frac{s^2_2}{n_2}}}\\] где (\\(\\bar{x_1}, \\bar{x_2}\\),– средние значения в сравниваемых выборках,\\(n_1, n_2\\) - количество наблюдений в первой и второй группах, \\(s_1, s-2\\) – стандартные отклонения в первой и второй группах.\nКоличество степеней свободы при этом высчитывается по формуле: \\[df=\\frac{(\\frac{s^2_1}{n_1}+\\frac{s^2_2}{n_2})^2}{\\frac{1}{n_1-1}(\\frac{s^2_1}{n_1})^2+\\frac{1}{n_2-1}(\\frac{s^2_2}{n_2})^2}\\] Полученная статистика \\(t\\) сравнивается с критическими значениями из таблиц с t-распределения Стьюдента для количества степеней свободы и выбранного уровня значимости α (как правило 0,05). Если рассчитанное значение t больше табличного, мы отвергаем нулевую гипотезу (\\(H_0\\)) о равенстве средних значений.\nРассмотрим возможности анализа данных с помощью t-критерия Стьюдента на следующем примере.\n\n Пример: В нашем исследовании по климату есть переменные, оценивающие важность для коренных народов, проживать на территории традиционного проживания, соблюдать обычаи и этнические традиции, осуществлять традиционную хозяйственную деятельность. Это табличный вопрос В18. Создадим усредненное значение по пяти подвопросам данного блока и используем его в качестве обобщенной оценки важности сохранения традиционных основ жизнедеятельности коренных народов, проживающих на высокогорных территориях. Проведем сравнительный анализ средних оценок в различных возрастных группах - в группе молодежи до 35 лет и среди жителей старше 35-летнего возраста.\n\nЗагрузим данные:\n\nlibrary(haven)\ndf&lt;-read_sav(\"База_КлимРиск_2023.sav\")\n\nПосчитаем новую переменную - среднее значение по переменным B18_1 - B18_5 и сделаем гистограмму:\n\nlibrary(dplyr)\ndf&lt;-df %&gt;%\n  rowwise() %&gt;% \n  mutate(V18_mean=mean(c_across(starts_with(\"V18\")), na.rm = TRUE))\nhist(df$V18_mean, col=\"steelblue\")\n\n\n\n\n\n\n\n\nМы видим, что наши данные весьма далеки от совершенства, и есть значительная отрицательная асимметрия, происходящая оттого, что большинство опрошенных дали самые высокие оценки значимости по всем пяти подвопросам. Строго говоря, с такими данными применять t-критерий не совсем корректно, и любой тест на нормальность это подтвердит.\n\nshapiro.test(df$V18_mean)\n\n\n    Shapiro-Wilk normality test\n\ndata:  df$V18_mean\nW = 0.80376, p-value &lt; 2.2e-16\n\n\n\nt(psych::describe(df$V18_mean))\n\n                   X1\nvars       1.00000000\nn        884.00000000\nmean       3.44560709\nsd         0.68243009\nmedian     3.80000000\ntrimmed    3.56541902\nmad        0.29652000\nmin        1.00000000\nmax        4.00000000\nrange      3.00000000\nskew      -1.24143852\nkurtosis   0.92310844\nse         0.02295261\n\n\nТак и есть, у нас асимметрия превышает допустимые пределы от +1 до -1 (Hair et al., 2022). Но исключительно в учебных целях мы продолжим.\nСоздадим группирующую переменную по возрасту:\n\ndf&lt;-df %&gt;% \n  mutate(age_bin=case_when(\n    age&lt;35 ~ \"До 35 лет\",\n    age&gt;=35 ~ \"Старше 35 лет\"\n      ))\n\nДля проверки гомогенности дисперсий в группах проведем лест Ливиня, преимуществом которого является возможность использования, когда в данных есть отклонения от нормальности:\n\nlibrary(car)\nleveneTest(V18_mean~age_bin, data = df)\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value  Pr(&gt;F)  \ngroup   1  5.7383 0.01681 *\n      858                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nСудя по тесту, у нас значимые различия в дисперсиях, а значит, если уж использовать t-критерий, то его модифицированную версию Уэлча:\n\nt.test(V18_mean~age_bin, data = df)\n\n\n    Welch Two Sample t-test\n\ndata:  V18_mean by age_bin\nt = -3.362, df = 414.04, p-value = 0.0008457\nalternative hypothesis: true difference in means between group До 35 лет and group Старше 35 лет is not equal to 0\n95 percent confidence interval:\n -0.28310252 -0.07419598\nsample estimates:\n    mean in group До 35 лет mean in group Старше 35 лет \n                   3.314108                    3.492757 \n\n\nТест показал наличие значимых отличий в оценке важности традиционной деятельности по возрасту (\\(t_{414,04}=-3,362, p = 0,001\\)), указывающий на то, что современная молодежь, проживающая вблизи ледников, в меньшей степени ориентирована на сохранение традиционных ценностей и устоев своего народа.\nСоздадим визуализацию к результатам теста:\n\nlibrary(ggpubr)\ndf %&gt;% \n  filter(!is.na(age_bin)) %&gt;% \n  ggboxplot(x = \"age_bin\", y = \"V18_mean\",\n          color = \"age_bin\", palette = \"jco\", order = c(\"До 35 лет\", \"Старше 35 лет\"), add = \"jitter\", xlab = \"Возраст\", )+stat_compare_means(method = \"t.test\", label.y = 4.5)\n\n\n\n\n\n\n\nРисунок 10.11: Ящичная диаграмма по результатам рассчета t-критерия (ggpubr)\n\n\n\n\n\nЛибо как вариант:\n\nlibrary(ggstatsplot)\nggbetweenstats(df, age_bin, V18_mean, xlab = \"Возраст\")\n\n\n\n\n\n\n\nРисунок 10.12: Ящичная диаграмма по результатам рассчета t-критерия (ggstatsplot)\n\n\n\n\n\nНесмотря на то, что различия статистически достоверны, величина эффекта представляющая собой стандартизованную разницу между средними, невысока: Hedges’ g (метрика, позволяющая оценить различия в средних) составляет всего 0,26, что можно считать очень незначительной разницей (согласно негласным нормам средний эффект наблюдается при g = 0,5, значительный - при g выше 0,8).\n\n\n10.3.2 U-критерий Манна-Уитни\nКритерий Манна-Уитни (U критерий, также называемый критерием Манна-Уитни-Вилкоксона, MWW/MWU, критерием ранговых сумм Вилкоксона, тестом Вилкоксона-Манна-Уитни) – непараметрический тест, проверяющий нулевую гипотезу о том, что в случайных значениях двух групп X и Y вероятность того, что X больше Y равна вероятности, что Y больше, чем X.\nПредложен в 1945 году американским химиком и статистиком Фрэнком Уилкоксоном, доработан австрийским и американским математиком Генри Манном и Дональдом Уитни в 1947 году.\nU-критерий Манна-Уитни используется для сравнения выраженности показателей в двух несвязных (независимых) выборках, является непараметрическим аналогом t-критерия Стьюдента.\nТребования:\n\nВсе наблюдения из двух групп должны быть независимыми друг от друга\nДанные должны быть измеренными по крайней мере в порядковой шкале (взяв два значения мы должны точно сказать, какое из них больше)\nНулевая гипотеза \\(H_0\\) предполагает, что распределения в двух группах являются идентичными Альтернативная гипотеза \\(H_1\\) заключается в том, что распределения не идентичны.\n\nДля применения U-критерия Манна — Уитни нужно:\n\nСоставить единый ранжированный ряд из обеих сопоставляемых выборок, расставив их элементы по степени нарастания признака и приписав меньшему значению меньший ранг. Общее количество рангов получится равным: \\(N=n_1+n_2\\), где \\(n_1\\) — количество элементов в первой выборке, а \\(n_2\\) — количество элементов во второй выборке.\nРазделить единый ранжированный ряд на два, состоящие соответственно из единиц первой и второй выборок. Подсчитать отдельно сумму рангов, пришедшихся на долю элементов первой выборки, и отдельно — на долю элементов второй выборки.\nОпределить значение U-критерия Манна — Уитни в каждой группе по формуле:\n\n\\[U_x=T_x-\\frac{n_x(n_x+1)}{2},\\] где \\(T_x\\) - сумма рангов.\nМеньшее значение и будет итоговым значением критерия.\n\nПо таблице для избранного уровня статистической значимости определить критическое значение критерия для данных \\(n_1\\) и \\(n_2\\). Если полученное значение \\(U\\) меньше табличного или равно ему, то признается наличие существенного различия между уровнем признака в рассматриваемых выборках (принимается альтернативная гипотеза). Если же полученное значение \\(U\\) больше табличного, принимается нулевая гипотеза. Достоверность различий тем выше, чем меньше значение \\(U\\).\n\nПроведем тест Манна-Уитни с примером выше:\n\nwilcox.test(V18_mean~age_bin, data = df)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  V18_mean by age_bin\nW = 62591, p-value = 0.0001254\nalternative hypothesis: true location shift is not equal to 0\n\n\nРезультаты аналогичны полученным с помощью t-критерия.\n\nlibrary(ggstatsplot)\nggbetweenstats(df, age_bin, V18_mean, xlab = \"Возраст\", type=\"nonparametric\")\n\n\n\n\n\n\n\nРисунок 10.13: Ящичная диаграмма по результатам рассчета U-критерия (ggstatsplot)\n\n\n\n\n\n\n\n10.3.3 Однофакторный дисперсионный анализ\nДисперсионный анализ (Analysis of Variance, ANOVA) – статистический метод выявления различий между выборочными средними для двух или больше совокупностей.\nСуществуют различные разновидности дисперсионного анализа, различающиеся по количеству группирующих (факторных) и зависимых переменных, используемых в анализе, типам выборок (независимые наблюдения или повторные эксперименты), наличию кластеров внутри выборки и др.\nСамым простым является однофакторный дисперсионный анализ (One-way или one-factor ANOVA), в котором используются одна зависимая и одна независимая переменные.\n\n Например, нас может интересовать влияние принадлежности к тому или иному социальному классу (низший, средний и высший класс) на показатели здоровья, такие как доступ к медицинским услугам или распространенность хронических заболеваний, или мы могли бы сравнить, как лица с различным уровнем религиозности (высокорелигиозные, со средней религиозностью или совсем не религиозности) оценивают уровень социальной справедливости или проявляют электоральное поведение, а также как уровень потребления медиаконтента (например, заядлые ТВ-зрители или типичные пользователи социальных сетей или читатели определенных газет) взаимосвязан с установками в отношении социальных проблем, так как проблемы миграции или гендерного неравенства.\n\nПри этом, мы предполагаем, что исходные значения зависимой переменной можно разложить на несколько компонентов, определяющих различия между ними:\n\\[x_{ij}=\\mu + F_i+ \\epsilon_{ij}, \\] где\n\n\\(x_{ij}\\) - значение зависимой переменной, полученной на \\(i-м\\) уровне фактора с порядковым номером \\(j\\);\n\\(\\mu\\) - общее среднее значение;\n\\(F_i\\) - эффект, обусловленный влиянием \\(i-го\\) уровня фактора;\n\\(\\epsilon_{ij}\\) - остаточный член, возмущение, вызванное влиянием неконтролируемых факторов, то есть вариацией переменной внутри уровня.\n\nГипотезы:\n\n\\(H_0\\): \\(\\mu_1=\\mu_2=\\mu_3=...= \\mu_k\\)\n\\(H_1\\): не все средние равны (хотя бы между одной парой средних имеются различия)\n\nДисперсионный анализ сравнивает дисперсии двух видов — внутри групп (связанную со случайными, неконтролируемыми различиями между испытуемыми) и между группами (связанную с влиянием группирующей переменной, или фактора). Как мы увидели выше, при сравнении двух групп t-статистика измеряет разность средних стандартной ошибкой. Дисперсионный анализ измеряет квадрат разности средних квадратами стандартной ошибки, т.е. результат для двух выборок равен квадрату, рассчитанной по этим же данным t-статистики.\nМежгрупповая (факторная) дисперсия рассчитывается по формуле:\n\\[MS_b=\\frac{SSb}{k-1},\\] где\n\n\\(SS_b=\\Sigma n_i(\\bar{x_i}-\\bar{x})\\) - межгрупповая сумма квадратов отклонений среднего значения в каждой группе от общего среднего;\n\\(k-1\\) - степень свободы (количество уровней группирующей переменной минус единица).\n\nВнутригрупповая (остаточная) дисперсия:\n\\[MS_w=\\frac{SSw}{N-k},\\] где\n\\(SS_w=\\Sigma(x-\\bar{x_i})\\) - внутригрупповая сумма квадратов отклонений \\(N-k\\) - количество степеней свободы\nОбщая сумма квадратов отклонений есть сумма межгрупповых и внутригрупповых квадратов отклонений:\n\\[SS=SS_w+SS_b\\] Результаты вычислений можно представлять в виде следующей таблицы:\n\nОсновной в дисперсионном анализе является статистика \\(F\\), показываются соотношение между межгрупповой и внутригрупповой дисперсиями. Если межгрупповая дисперсия существенно выше, чем внутригрупповая, то между изучаемыми группами (уровнями) существуют статистически значимые различия.\n\\[F=\\frac{MS_b}{MS_w}\\]\n\n\n\n\n\n\nРисунок 10.14: Межгрупповая и внутригрупповая дисперсии\n\n\n\n\n10.3.3.1 Множественные сравнения (post hoc tests – апостериорные тесты)\nСам по себе дисперсионный анализ показывает, что есть различия хотя бы между одной парой средних значений (наличие общего эффекта). Но между какими именно группами? Обычно на вычислении \\(F\\) все не заканчивается, а только начинается.\nПервое, что может прийти в голову: почему бы не сравнить группы попарно с помощью того же t-критерия? Однако, все не так просто, поскольку когда групп много, возникает проблема одновременной проверки множественных гипотез.\nВкратце, эта проблема заключается в том, что при одновременной проверке большого числа гипотез на том же наборе данных вероятность сделать неверное заключение в отношении хотя бы одной из этих гипотез значительно превышает изначально принятый уровень значимости (\\(\\alpha\\)).\nТак, если мы будем сравнивать десять групп по тридцать испытуемых в каждой, то мы практически всегда найдем отличие лучшей группы от худшей на уровне значимости меньше 0.05, даже если группы эти набирались совершенно случайно и ни о каком воздействии, которое могло бы привести к систематическому сдвигу среднего значения, речи не шло. Например, вы можете сравнить пассажиров десяти вагонов поезда по тесту, измеряющему социальную дистанцию к какой-либо группе и убедиться, что значимость отличия лучшего вагона от худшего по Т-критерию, как правило, меньше 0.05.\nМожно сказать, что 10(10−1)/2 (число сочетаний из 10 по 2) попарных сравнений «всех со всеми» практически гарантируют хотя бы одну ошибку первого рода, т.е. отвержение гипотезы \\(H_0\\), когда она на самом деле верна.\nКогда мы сравниваем три группы (например, А и В, А и С, В и С), вероятность совершить ошибку хотя бы в одном из этих трех сравнений составит:\n\\[P' = 1 - (1 - \\alpha)^m =  1- (1 - 0.05)^3 = 0.143,\\] Если же количество сравнений 45, как в примере с 10 вагонами (10*9/2=45), то вероятность ошибки начинает превышать 90%:\n\\[P' = 1 - (1 - \\alpha)^m =  1- (1 - 0.05)^{45} = 0.90,\\]\nЧто делать?\nДля устранения эффекта множественных сравнений существует большой репертуар методов, позволяющих снизить вероятность ошибочного решения. Они различаются как своей консервативностью, так и условиями применения.\nОдним из самых простых и известных способов контроля над групповой вероятностью ошибки является Метод Бонферрони (назван так в честь предложившего его итальянского математика Карло Эмилио Бонферрони; Carlo Emilio Boferroni). Он заключается в умножении полученных при сравнении групп p-значений на количество сравниваемых групп.\n\n Пример: Предположим, Предположим, что мы применили определенный статистический критерий 3 раза (например, сравнили при помощи критерия Стьюдента средние значения групп А и В, А и С, и В и С) и получили следующие три Р-значения: 0.01, 0.02 и 0.005. Чтобы применить метод Бонферрони, мы должны умножить каждое из p-значений на 3, а затем сравнить с выбранным уровнем значимости:\n\n0.01 * 3 = 0.03 &lt; 0.05: гипотеза отклоняется;\n0.02 * 3 = 0.06 &gt; 0.05: гипотеза принимается;\n0.005 * 3 = 0.015 &lt; 0.05: гипотеза отклоняется.\n\n\nХотя метод Бонферрони очень прост в реализации, он обладает одним существенным недостатком: при возрастании числа проверяемых гипотез мощность этого метода резко снижается. Другими словами, при возрастании числа гипотез нам будет все сложнее и сложнее отвернуть даже те из них,которые должны быть отвергнуты. Например, при проверке 10 гипотез, применение поправки Бонферрони привело бы к снижению исходного уровня значимости до 0.05/10 = 0.005. Соответственно, для отклонения той или иной гипотезы, соответствующие Р-значения должны были бы оказаться меньше 0.005, и такого условия достичь маловероятно. В связи с этим метод Бонферрони не рекомендуется использовать, если число проверяемых гипотез превышает 7-8.\nДля преодоления проблем, связанных с низкой мощностью метода Бонферрони, в 1978 г. Стур Холм (Holm 1978) предложил гораздо более мощную его модификацию (часто этот метод называют еще методом Холма-Бонферрони). Этот модифицированный метод основан на алгоритме, который включает следующие шаги:\n\nИсходные Р-значения упорядочиваются по возрастанию: \\(p_{(1)} \\leq p_{(2)} \\leq \\dots \\leq p_{(m)}\\). Эти Р-значения соответствуют проверяемым гипотезам \\(H_{(1)}, H_{(2)}, \\dots H_{(m)}\\).\nЕсли \\(p_{(1)} \\geq \\alpha/m\\), все нулевые гипотезы \\(H_{(1)}, H_{(2)}, \\dots H_{(m)}\\) принимаются и процедура останавливается. Иначе следует отвергнуть гипотезу \\(H_{(1)}\\) и продолжить.\nЕсли \\(p_{(2)} \\geq \\alpha/(m-1)\\), нулевые гипотезы \\(H_{(2)}, H_{(3)}, \\dots H_{(m)}\\) принимаются и процедура останавливается. Иначе гипотеза \\(H_{(2)}\\) отвергается и процедура продолжается.\n…\nЕсли \\(p_{(m)} \\geq \\alpha\\), нулевая гипотеза $H_{(m)} принимается и процедура останавливается.\n\nОписанную процедуру называют нисходящей (англ. step-down): она начинается с наименьшего P-значения в упорядоченном ряду и последовательно «спускается» вниз к более высоким значениям. На каждом шаге соответствующее значение \\(p_{(i)}\\) сравнивается со скорректированным уровнем значимости \\(\\alpha / (m+i-1)\\). Как и в случае с поправкой Бонферрони, вместо корректировки уровня значимости мы можем скорректировать непосредственно Р-значения - конечный результат (в смысле принятия или отклонения той или иной гипотезы) окажется идентичным. Соответствующая поправка выполняется в виде \\(q_i = p_{(i)} (m + i -1)\\). Так, для рассмотренного выше примера с тремя Р-значениями получаем:\n\n\\(q_1 = p_{(1)}(m - 1 + 1) = 0.005*3 = 0.015\\)\n\\(q_2 = p_{(2)}(m - 2 + 1) = 0.01*2 = 0.02\\)\n\\(q_3 = p_{(2)}(m - 3 + 1) = 0.02*1 = 0.02\\)\n\nИменно последний подход реализован в R-функции p.adjust():\n\n# Скорректированные Р-значения:\np.adjust(c(0.01, 0.02, 0.005), method = \"holm\")\n\n[1] 0.020 0.020 0.015\n\n\nНа практике, в различных программах статистической обработки данных используются разные критерии для множественных сравнений.\nТак, критерии диапазона выявляют однородные подмножества средних, не различающихся между собой. Парные множественные сравнения проверяют разности между каждой парой средних значений и выдают матрицу, в которой звездочками обозначены групповые средние, значимо различающиеся на уровне \\(\\alpha\\).\nЕсли требование о равенстве дисперсий выполняется, то рекомендуется использовать такие апостериорные тесты как Тьюки-b, С-Н-К (Стьюдента-Ньюмена-Келса), Дункана, Р-Э-Г-У F ( F-критерий Райана-Эйнота-Габриэля-Уэлша), Р-Э-Г-У Q (критерий диапазона Райана-Эйнота-Габриэля-Уэлша) и Уоллера-Дункана, Бонферрони, Тьюки LSD, Шидака, Габриэля, Гохберга, Даннетта, Шеффе и НЗР (наименьшей значимой разности).\nЕсли требование о равенстве дисперсий не выполняется: Тамхейна T2, Даннетта T3, Геймса-Хоуэлла и Даннетта C.\nПродолжим наш пример с климатическими данными, однако, на этот раз, разделим выборку не на две, а на три возрастные группы, используя уже имеющуюся в наборе переменную age_cat3.\nПоскольку нам нужна факторная переменная, прежде всего восстановим метки значений для этой переменной:\n\ndf$age_cats3&lt;-sjlabelled::as_label(df$age_cats3)\n\nПроведем тест на гомогенность дисперсий (тест Ливиня из библиотеки rstatix - не забываем, что ее нужно предварительно установить):\n\ndf %&gt;% \n  rstatix::levene_test(V18_mean ~ age_cats3)\n\n# A tibble: 1 × 4\n    df1   df2 statistic      p\n  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n1     2   857      2.62 0.0732\n\n\nТест показывает отсутствие значимых различий, а значит, мы можем использовать «обычную» практику расчетов, в ппротивномслучае было бы лучше воспользоваться специальной формулой Уэлча, которая есть и для дисперсионного анализа.\nПроведем дисперсионный анализ, используя функцию anova_test из уже упомянутого пакета rstatix. В качестве аргумента мы должны задать формулу: V18_mean ~ age_cats3, означающую, что мы хотим исследовать зависимость значений V18_mean от уровней факторной переменной age_cats3:\n\nres.aov &lt;- df %&gt;% rstatix::anova_test(V18_mean ~ age_cats3)\nres.aov\n\nANOVA Table (type II tests)\n\n     Effect DFn DFd    F        p p&lt;.05   ges\n1 age_cats3   2 857 7.29 0.000725     * 0.017\n\n\nЗначение F составило 7,29, весьма далеко от случайного отклонения:\n\n\n\n\n\n\n\n\n\n\npwc &lt;- df %&gt;% rstatix::tukey_hsd(V18_mean ~ age_cats3)\npwc\n\n# A tibble: 3 × 9\n  term      group1    group2      null.value estimate conf.low conf.high   p.adj\n* &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;            &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 age_cats3 До 30 лет 31-49 лет            0   0.245    0.0930    0.397  4.82e-4\n2 age_cats3 До 30 лет 50 лет и с…          0   0.204    0.0442    0.364  7.87e-3\n3 age_cats3 31-49 лет 50 лет и с…          0  -0.0409  -0.163     0.0807 7.09e-1\n# ℹ 1 more variable: p.adj.signif &lt;chr&gt;\n\n\nВизуализация результатов (первый вариант):\n\n# Прежде, чем создать визуализацию, создадим список пар групп, которые будем сравнивать\nmy_comparisons &lt;- list( c(\"До 30 лет\", \"31-49 лет\"), c(\"До 30 лет\", \"50 лет и старше\"), c(\"31-49 лет\", \"50 лет и старше\"))\ndf %&gt;% \n  filter(!is.na(age_cats3)) %&gt;% \n  ggviolin(x = \"age_cats3\", y = \"V18_mean\",\n          color = \"age_cats3\", fill = \"age_cats3\", add = \"boxplot\", add.params = list(fill = \"white\"), palette = \"jco\",  xlab = \"Возраст\", ylab = \"Оценка важности сохранения \\n традиционных основ жизнедеятельности\", legend.title = \"Возрастные группы\" )+stat_compare_means(comparisons = my_comparisons)\n\n\n\n\n\n\n\nРисунок 10.15: Визуализация результатов однофакторного дисперсионного анализа (ggpubr)\n\n\n\n\n\nВизуализация результатов (вариант c помощью пакета ggstatsplot). Отметим, что по умолчанию функция основывается на предположении об отсутствии равенства дисперсий и использует критерий Геймса-Хоуэлла, поэтому результаты отличаются p-значениями, но общий вывод не меняется: основные различия пролегают между оценками младшей группы и всеми остальными.\n\nlibrary(ggstatsplot)\nggbetweenstats(\n  df,\n  x    = age_cats3,\n  y    = V18_mean,\n  type = \"parametric\",\n  xlab = \"Возраст\"\n)\n\n\n\n\n\n\n\nРисунок 10.16: Визуализация результатов однофакторного дисперсионного анализа (ggstatsplot)\n\n\n\n\n\n\n\n\n10.3.4 Однофакторный непараметрический дисперсионный анализ Краскела-Уоллиса\nАдекватное применение дисперсионного анализа основывается на допущении о том, что зависимая переменная является непрерывной, имеет нормальное распределение и достаточно большую выборку (желательно \\(n_j&gt; 30\\) где \\(j=1, 2, ..., k\\), а \\(k\\) обозначает количество независимых сравниваемых групп). Дополнительно, ANOVA требует равенства дисперсий в сравниваемых выборках. Этот метод достаточно устойчив к отклонением, если выборки небольшие, но одинаковые. Когда есть проблемы с нормальностью или выборки маленькие и неравные, а также когда данные измерены в порядковой шкале, лучше использовать непараметрический аналог\nОдним из часто используемых непараметрических аналогов, позволяющих сравнить более двух независимых группn, является критерий Краскела-Уоллиса (Kruskal Wallis test). Этот тест сравнивает распределения в k группах (k &gt; 2), при этом исходные данные также заменяются рангами. Является обобщением U-критерия Манна-Уитни для количества групп более двух.\nНулевая и альтернативная исследовательские гипотезы формулируются следующим образом:\n\n\\(H_0\\): Медианы в \\(k\\) группах населения являются равными\n\\(H_1\\): Медианы в \\(k\\) группах не равны\n\nПроцедура тестирования предполагает проведение следующих шагов:\n\nсведение всех \\(k\\) выборок в один комбинированный набор\nранжирование всех значений от 1 до \\(N\\), где \\(N = n_1+n_2 + ...+ n_k\\) и присвоение рангов\nподсчет суммы рангов в каждой группе\nвычисление статистики критерия\nопределение уровня значимости и формулировка вывода.\n\nТребования\n\nВыборки являются случайными.\nНаблюдения не зависят друг от друга\nКак минимум порядковый уровень измерения для зависимой переменной и порядковый или номинальный – для независимого фактора.\nНет требований о характере распределения или нормальности данных.\n\nФормула для расчета H-критерия Краскела-Уоллиса:\n\\[H=\\frac{12}{N(N+1)}\\Sigma^k_{i=1}\\frac{R^2_i}{n_i}-3(N+1)\\]\nгде:\n\n\\(k\\) количество сравниваемых групп\n\\(N\\) - общий объем выборки\n\\(n_i\\) - объем выборки в группе \\(i\\)\n\\(R_i\\) - сумма рангов в группе \\(i\\).\n\n\nkruskal.test(V18_mean ~ age_cats3, data = df)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  V18_mean by age_cats3\nKruskal-Wallis chi-squared = 17.074, df = 2, p-value = 0.0001961\n\n\nМы также можем сделать парные сравнения:\n\npairwise.wilcox.test(df$V18_mean, df$age_cats3,  p.adjust.method = \"BH\")\n\n\n    Pairwise comparisons using Wilcoxon rank sum test with continuity correction \n\ndata:  df$V18_mean and df$age_cats3 \n\n                До 30 лет 31-49 лет\n31-49 лет       0.00014   -        \n50 лет и старше 0.00428   0.18172  \n\nP value adjustment method: BH \n\n\n\nlibrary(ggstatsplot)\nggbetweenstats(\n  df,\n  x    = age_cats3,\n  y    = V18_mean,\n  type = \"non-parametric\",\n  xlab = \"Возраст\"\n)\n\n\n\n\n\n\n\nРисунок 10.17: Визуализация результатов непараметрического однофакторного дисперсионного анализа (ggstatsplot)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Статистический вывод и тестирование исследовательских гипотез</span>"
    ]
  },
  {
    "objectID": "Statistical-Inference.html#анализ-зависимых-выборок",
    "href": "Statistical-Inference.html#анализ-зависимых-выборок",
    "title": "10  Статистический вывод и тестирование исследовательских гипотез",
    "section": "10.4 Анализ зависимых выборок",
    "text": "10.4 Анализ зависимых выборок\nНапомним, что зависимые (иногда их еще называют связанные, парные) выборки представляют собой данные, в которых каждому наблюдению в одном датасете соответствует другое, связанное с ним наблюдение в другом наборе данных.\nНапример, могут сравниваться:\n\nизмерения в разные периоды времени (до и после лечения, обучения, смены работы и пр.)\nусловия, в которых были проведены измерения (лекарство натощак или после завтрака, выполнение теста с предварительным тренингом и без, результаты адаптации мигрантов через программу интеграции и без нее и др.)\nрезультаты разных измерений (разные тесты тревожности, например)\nспаренные пары, имеющие сходные характеристики (например, при сравнении результатов различных образовательных программ).\n\nДля анализа результатов таких исследований требуются особые статистические методы, которые мы будем обсуждать в данном разделе.\n\n\n\n\n\n\nРисунок 10.18: Связь между наблюдениями в зависимых выборках. Источник: www.statology.org\n\n\n\n\n10.4.1 t-критерий для зависимых выборок\nАналогично ситуациям, рассмотренным выше, t-критерий для зависимых выборок также используется для того, чтобы сравнить средние значения, но при этом предполагается, что группы связаны друг с другом каким-то образом. Чаще всего речь идет об одних и тех же испытуемых (респондентах, наблюдениях), у которых измеряется один и тот же показатель через некоторый промежуток времени. Однако, это могут быть и разные люди (объекты исследования), но выборки все равно проектируются как связанные.\n\n\n\n\n\n\nРисунок 10.19: Отличия между различными критериями\n\n\n\nБудучи параметрической процедурой, в которой оцениваются неизвестные параметры, t-критерий основывается на некоторых допущениях, проверка которых обычно осуществляется перед проверкой статистических гипотез. Поскольку речь идет о зависимых выборках, в качестве наблюдений учитываются не исходные данные, а различия между двумя наборами значений, и каждое допущение касается именно этих различий, а не исходных данных. Таких допущений четыре:\n\nЗависимая переменная должна быть непрерывной (интервальная шкала / шкала отношений). В отдельных случаях могут использоваться дискретные шкалы (например, в ходе измерения социальных установок с помощью шкал Лайкерта).\nНаблюдения должны быть независимы друг от друга (различия между парами по одной строке не должны зависеть от различий по другим строкам).\nЗависимая переменная (то есть различия между значениями) должна иметь нормальное распределение.\nУ зависимой переменной не должно быть выбросов.\n\nКак всегда, формулируем гипотезы:\n\\(H_0:μ_d=0\\) – разница в средних значениях двух спаренных выборок равна нулю\n\\(H_1:μ_d≠0\\) - (двухсторонний) разница в средних значениях двух выборок – не равна нулю \\(H_1:μ_d&gt;0\\) - (правосторонний) – разница больше нуля\n\\(H_1:μ_d&gt;0\\) - (левосторонний) – разница меньше нуля\nПорядок вычислений:\n\nПосчитать различия в значениях между двумя измерениями – \\(D\\)\nПосчитать среднее значение получившихся различий \\(d_1, d_2, … , d_n\\):\n\n\\[\\bar{d}=\\frac{d_1+d_2+...+ d_n}{n}\\] - Посчитать значение t-критерия по формуле:\n\\[t = \\frac{\\bar{d}}{s/\\sqrt{n}},\\] где \\(s\\) - стандартное отклонение различий (\\(d\\)), \\(n\\) - объем выборки (количество пар значений, по которым высчитываются \\(d\\))/\nДалее мы можем вычислить p-значение для абсолютного значения статистики критерия (\\(|t|\\)) на основе сведений о количестве степеней свободы (\\(df\\)): \\(df=n−1\\) (приблизительно с помощью таблиц критических значений или точно на основе специальной программы).\n\n Пример: Одной из извечных российских проблем являются дороги. Для повышения безопасности и комфорта дорожного движения созден специальный национальный проект - «Безопасные качественные дороги», постоянно проводятся профилактические мероприятия, направленные на повышение уровня информированности граждан о правилах дорожного движения, ответственности и сознательности, как водителей, так и пешеходов. Допустим, мы хотели бы выяснить, как меняется ситуация на дорогах, происходит ли снижение количества погибших в ДТП за последние пять лет.\n\nСкачать данные\nЗагрузим данные Росстата о количестве погибших в ДТП на 100 тыс. населения:\n\nlibrary(readxl)\ndtp&lt;-read_excel(\"dtp.xls\")\n\nВычислим различия между двумя рядами значений и сделаем проверку на нормальность - графически и с помощью теста Шапиро-Уилка.\n\nnames(dtp)&lt;-c(\"region\", \"dtp2018\", \"dtp2023\")\ndtp$d&lt;-dtp$dtp2018-dtp$dtp2023\n# Сделаем гистограмму\nhist(dtp$d)\n\n\n\n\n\n\n\nshapiro.test(dtp$d)\n\n\n    Shapiro-Wilk normality test\n\ndata:  dtp$d\nW = 0.87308, p-value = 4.961e-07\n\n\nДа, пока результаты не радуют, мы видим, что есть существенные отклонения от нормальности. Если посмотреть на данные, то можно увидеть, что ситуацию осложняет Республика Калмыкия, где произошло большое увеличение количества ДТП. Попробуем убрать этот регион и посмотреть, что получится.\n\nlibrary(dplyr)\ndtp2&lt;-dtp %&gt;% \n  filter(region!=\"Республика Калмыкия\")\nhist(dtp2$d)\n\n\n\n\n\n\n\nshapiro.test(dtp2$d)\n\n\n    Shapiro-Wilk normality test\n\ndata:  dtp2$d\nW = 0.989, p-value = 0.6942\n\n\nСовсем другое дело!\nСуществует два способа подсчета t-критерий для зависимых выборок в зависимости от того, как организованы данные. В нашей таблице данные не совсем «чистые», один и тот же показатель указан в двух столбцах, однако такая ситуация встречается довольно часто.\n\n10.4.1.1 Метод 1. Разные столбцы для разных изменений\nФункция останется прежней, но к ней добавится аргумент paired = TRUE, который и позволит провести измерения с зависимыми выборками:\n\nres &lt;- t.test(dtp2$dtp2018, dtp2$dtp2023, paired = TRUE) \nres\n\n\n    Paired t-test\n\ndata:  dtp2$dtp2018 and dtp2$dtp2023\nt = 8.6723, df = 84, p-value = 2.711e-13\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 1.938852 3.092597\nsample estimates:\nmean difference \n       2.515725 \n\n\nПоскольку мы сравниваем 2018 год и 2023, то положительная средняя разница означает, что в среднем количество погибших в ДТП сокращается. Статистика t-критерия (8,67) при количестве степеней свободы 84 указывает на достоверные различия (p&lt;0,001).\n\n\n10.4.1.2 Метод 2. Один столбец для всех изменений + группирующая переменная\nДля того, чтобы воспроизвести эту ситуацию, переструктурируем наши данные таким образом, чтобы вся информация о ДТП была в одном столбце, а период, в который собирались статистические данные, - в другом.\n\nlibrary(tidyr)\ndtp3&lt;-dtp2 %&gt;% \n  pivot_longer(cols=dtp2018:dtp2023, \nnames_to=\"year\",\nvalues_to = \"dtp_number\")\n\nПосчитаем t-критерий вторым способом, через формулу:\n\nres &lt;- t.test(dtp_number ~ year, data = dtp3, paired = TRUE)\nres\n\n\n    Paired t-test\n\ndata:  dtp_number by year\nt = 8.6723, df = 84, p-value = 2.711e-13\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 1.938852 3.092597\nsample estimates:\nmean difference \n       2.515725 \n\n\nКак и следовало ожидать, результат идентичный.\nДавайте визуализируем это:\n\nlibrary(\"ggpubr\")\nggboxplot(dtp3, x = \"year\", y = \"dtp_number\", \n          color = \"year\", palette = c(\"#00AFBB\", \"#E7B800\"),\n          order = c(\"dtp2018\", \"dtp2023\"),\n          ylab = \"Количество погибших в ДТП на 100 тыс. населения\", xlab = \"\")\n\n\n\n\n\n\n\n\n\nlibrary(ggstatsplot)\nggwithinstats(\n  data = dtp3,\n  x = year,\n  y = dtp_number\n)\n\n\n\n\n\n\n\n\n\n\n\n10.4.2 Критерий Уилкоксона для зависимых выборок\nЕсли все же проблемы с распределением непреодолимы или данные не являются непрерывными, лучше воспользоваться непараметрическим аналогом для зависимых выборок - критерием знаковых рангов Уилкоксона.\nВыше мы уже разбирали формулу данного критерия, здесь же подчеркнем, что мы будем сравнивать два распределения, соответствующие одной и той же группе респондентов (объектов, наблюдений).\n\n\n\n\n\n\nРисунок 10.20\n\n\n\nПрименим тест Уилкоксона для анализа данных о погибших в ДТП. Попробуем воспроизвести все действия вручную:\n\ndtp2$rank&lt;-rank(abs(dtp2$d),  ties.method = \"average\")# посчитаем ранги\ndtp2$sign&lt;-sign(dtp2$d)#найдем знак различий\ndtp2$signed_ranks&lt;-dtp2$rank*dtp2$sign#умножим ранги на знак\n\nПосчитаем сумму положительных рангов:\n\ndtp2 %&gt;% \n  filter(signed_ranks&gt;0) %&gt;% \n  summarise(sum_ranks=sum(signed_ranks))\n\n# A tibble: 1 × 1\n  sum_ranks\n      &lt;dbl&gt;\n1      3321\n\n\nВ R мы можем использовать знакомую уже нам функцию wilcox.test(), но указываем аргумент paired=TRUE.\n\nwilcox.test(dtp2$dtp2018, dtp2$dtp2023, paired = TRUE, alternative = \"two.sided\")\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  dtp2$dtp2018 and dtp2$dtp2023\nV = 3321, p-value = 6.071e-11\nalternative hypothesis: true location shift is not equal to 0\n\n\nВизуализируем это:\n\nggwithinstats(\n  data = dtp3,\n  x = year,\n  y = dtp_number,\n  type = \"non-parametric\"\n)\n\n\n\n\n\n\n\n\n\n\n10.4.3 ANOVA с повторными измерениями\nДисперсионный анализ с повторными измерениями расширяет идею t-критерия для зависимых выборок на случай, когда количество сравниваемых групп превышает две.\nВычисления сходны с теми, что мы рассматривали на примере «обычной» однофакторной ANOVA.\nРазличия будут заключаться в том, как мы будем подсчитывать среднюю межгрупповую сумму квадратов отклонений, для повторных измерений у в качестве группирующей переменной будет использоваться фактор времени:\n\\[F=\\frac{MS_{time}}{MS_{error}}\\]\nили фактор условия: \\[F=\\frac{MS_{condition}}{MS_{error}}\\]\nНапример, мы хотим сравнить значения, соответствующие различным временным отрезкам.\nТогда разница группового (внутри отдельных временных диапазонов) и общего среднего составит:\n\\[S_{time}=SS_b=\\sum_{i=1}^{k}(\\bar{x_i}-\\bar{x})^2\\] Разница индивидуальных значений и групповых средних:\n\\[SSw=\\sum_{1}(x_{i1}-\\bar{x_1})^2+\\sum_{2}(x_{i2}-\\bar{x_2})^2+ ... + \\sum_{k}(x_{ik}-\\bar{x_k})^2\\] Разница средних по всем измерениям и общего среднего, k – количество уровней:\n\\[SS_{subjects}=k*\\sum(\\bar{x_i}-\\bar{x})^2\\]\nСумма квадратов ошибки:\n\\[SS_{error}=SS_w-SS_{subjects}\\]\nТогда средний квадрат отклонений по времени будет:\n\\[MS_{time}=\\frac{SS_{time}}{k-1}\\] Соответственно, будет определяться и средняя ошибка:\n\\[MS_{error}=\\frac{SS_{error}}{(n-1)(k-1)}\\]\nРассмотрим возможности применения данного метода на следующем примере:\n\nПример: В исследовании изучалось влияние на восприятие алкогольных напитков различной информации. Сравнивались три напитка – вода, вино и пиво. Участники эксперимента смотрели положительные, нейтральные и негативные информационные материалы, а затем оценивали свое отношение к напитку по шкале от -100 до 100.\n\nСкачать данные\n\n\n\n\n\n\n\n\n\n\nРисунок 10.21: Примерно так могли выглядеть информационные материалы эксперимента\n\n\n\n\nalcohol_attitudes&lt;-read.csv(\"Alcohol_Attitudes.csv\")\n\nНаши данные представлены в «широком» формате, переведем их в длинный, оставив только переменные, касающиеся пива:\n\nalc_long&lt;-alcohol_attitudes %&gt;% \n  select(participant, contains(\"beer\")) %&gt;% \n  pivot_longer(cols=contains(\"beer\"), names_to = \"emotion\", values_to = \"score\")\n\nПосмотрим описательные статистики по группам:\n\nlibrary(rstatix)\nalc_long %&gt;%\n  group_by(emotion) %&gt;%\n  get_summary_stats(score, type = \"mean_sd\")\n\n# A tibble: 3 × 5\n  emotion  variable     n  mean    sd\n  &lt;chr&gt;    &lt;fct&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 beerneg  score       20  4.45  17.3\n2 beerneut score       20 10     10.3\n3 beerpos  score       20 21.0   13.0\n\n\nВидим, что оценки разные и следуют логике увеличения по мере смены эмоции с негативной на позитивную.\n\n#Сделаем переменную эмоций факторной\nalc_long$emotion&lt;-factor(alc_long$emotion, levels=c(\"beerneg\", \"beerneut\", \"beerpos\"))\n#Сделаем график\nbxp &lt;- ggboxplot(alc_long, x = \"emotion\", y = \"score\", fill=\"emotion\", add = \"point\",  )\nbxp\n\n\n\n\n\n\n\n\nСделаем проверку на нормальность:\n\nalc_long %&gt;%\n  group_by(emotion) %&gt;%\n  shapiro_test(score)\n\n# A tibble: 3 × 4\n  emotion  variable statistic      p\n  &lt;fct&gt;    &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n1 beerneg  score        0.896 0.0354\n2 beerneut score        0.961 0.573 \n3 beerpos  score        0.951 0.375 \n\n\nВидим, что в группе с негативной эмоцией есть отклонения от нормальности, но мы пока закроем на это глаза.\nСопроводим графической проверкой:\n\nggqqplot(alc_long, \"score\", facet.by = \"emotion\")\n\n\n\n\n\n\n\n\n\nres.aov &lt;- anova_test(data = alc_long, dv = score, wid=participant, within = emotion)\nget_anova_table(res.aov)\n\nANOVA Table (type III tests)\n\n   Effect DFn DFd      F        p p&lt;.05   ges\n1 emotion   2  38 17.574 3.95e-06     * 0.207\n\n\nРезультаты показывают, что оценки отношения к напитку были статистически различными в разных условиях эксперимента, F(2, 38) = 17.57 p &lt; 0.001, однако обобщенная величина эффекта не очень велика eta2[g] = 0.21.\nПроведем парные сравнения:\n\npwc &lt;- alc_long %&gt;%\n  pairwise_t_test(\n    score ~ emotion, paired = TRUE,\n    p.adjust.method = \"bonferroni\"\n    )\npwc\n\n# A tibble: 3 × 10\n  .y.   group1   group2    n1    n2 statistic    df       p   p.adj p.adj.signif\n* &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;       \n1 score beerneg  beern…    20    20     -2.13    19 4.6 e-2 1.39e-1 ns          \n2 score beerneg  beerp…    20    20     -5.12    19 6.02e-5 1.81e-4 ***         \n3 score beerneut beerp…    20    20     -4.14    19 5.51e-4 2   e-3 **          \n\n\nРазличия значимы между оценками, полученными в результате предъявления позитивных материалов, и двумя другими группами (нейтральной и негативной), тогда как различия между негативными и нейтральными материалами не значимы.\nДобавим результаты статистического анализа в исходный график:\n\npwc &lt;- pwc %&gt;% add_xy_position(x = \"emotion\")\nbxp + \n  stat_pvalue_manual(pwc) +\n  labs(\n    subtitle = get_test_label(res.aov, detailed = TRUE),\n    caption = get_pwc_label(pwc)\n  )\n\n\n\n\n\n\n\n\nНу, или вот так:\n\nggwithinstats(\n  data = alc_long,\n  x = emotion,\n  y = score\n)\n\n\n\n\n\n\n\n\n\n\n10.4.4 Критерий Фридмана - непараметрический аналог дисперсионного анализа для зависимых выборок\nТест Фридмана – разработан Милтоном Фридманом, американским экономистом и нобелевским лауреатом.\n\n\n\n\n\n\nРисунок 10.22: Милтон Фридман - нобелевский лауреат за исследования в области потребления, монетаризма и политики стабилизации\n\n\n\nЯвляется аналогом ANOVA с повторными измерениями и непараметрического дисперсионного анализа для связанных выборок.\nАлгоритм следующий:\n\nпредставить данные в виде матрицы $ {x_{ij} }_{n*k}$ с \\(n\\) количеством строк и \\(k\\) количеством столбцов\nзаменить исходные значения на ранги (\\(r\\)) по каждой строке.\nпосчитать сумму рангов (\\(R\\)) по каждому столбцу\nподставить значения в формулу:\n\n\\[Q=\\frac{12}{N*k*(k+1)}*\\sum R^2+(3*N*(k+1))\\]\n\nВ тех случаях, когде \\(n\\) или \\(k\\) достаточно большие (например, \\(n&gt;15\\) или \\(k&gt;4\\) статистика критерия может быть аппроксимирована распределением \\(\\chi^2\\). Если \\(n\\) или \\(k\\) маленькие, статистика \\(\\chi^2\\) может быть некорректной и лучше воспользоваться специальными таблицами.\n\n\nres.fried &lt;- alc_long %&gt;% friedman_test(score ~ emotion |participant)\nres.fried\n\n# A tibble: 1 × 6\n  .y.       n statistic    df        p method       \n* &lt;chr&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;        \n1 score    20      15.1     2 0.000521 Friedman test\n\n\nДля определения величины эффекта можно воспользоваться критерием W Кендалла:\n\\[W=\\frac{Q}{N(k-1)},\\]\nгде \\(Q\\) - статистика теста по Фридману, \\(N\\) объем выборки, \\(k\\) количество измерений по каждому субъекту (M. T. Tomczak and Tomczak 2014).\nЗначение коэффициента \\(W\\) может принимать значения от 0 (отсутствие взаимосвязи между оценками и уровням измерений) до 1 (тесная взаимосвязь).\nИнтерпретация коэффициента аналогична \\(d\\) Коэна:\n\n0.1 - &lt; 0.3 (маленький эффект)\n0.3 - &lt; 0.5 (средний эффект)\n\n= 0.5 (большой эффект).\n\n\nДоверительные интервалы рассчитываются с помощью бутстрэпа (многократного расщепления выборки на наборы данных).\n\nalc_long %&gt;% friedman_effsize(score ~ emotion |participant)\n\n# A tibble: 1 × 5\n  .y.       n effsize method    magnitude\n* &lt;chr&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;     &lt;ord&gt;    \n1 score    20   0.378 Kendall W moderate \n\n\nВ данном случае наблюдается средний эффект.\nПроведем парные сравнения с помощью теста Уилкоксона:\n\npwc &lt;- alc_long %&gt;%\n  wilcox_test(score ~ emotion, paired = TRUE, p.adjust.method = \"bonferroni\")\npwc\n\n# A tibble: 3 × 9\n  .y.   group1   group2      n1    n2 statistic        p p.adj p.adj.signif\n* &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;    &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;       \n1 score beerneg  beerneut    20    20      37   0.036    0.109 ns          \n2 score beerneg  beerpos     20    20       6   0.000579 0.002 **          \n3 score beerneut beerpos     20    20       7.5 0.00046  0.001 **          \n\n\nРезультаты аналогичны тем, что были найдены в ходе применения параметрических процедур.\nВизуализируем это:\n\npwc &lt;- pwc %&gt;% add_xy_position(x = \"emotion\")\nggboxplot(alc_long, x = \"emotion\", y = \"score\", fill=\"emotion\", add = \"point\") +\n  stat_pvalue_manual(pwc) +\n  labs(\n    subtitle = get_test_label(res.fried,  detailed = TRUE),\n    caption = get_pwc_label(pwc)\n  )\n\n\n\n\n\n\n\n\nВторой вариант визуализации:\n\nggwithinstats(\n  data = alc_long,\n  x = emotion,\n  y = score,\n  type=\"non-parametric\"\n)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Статистический вывод и тестирование исследовательских гипотез</span>"
    ]
  },
  {
    "objectID": "Statistical-Inference.html#анализ-качественных-данных",
    "href": "Statistical-Inference.html#анализ-качественных-данных",
    "title": "10  Статистический вывод и тестирование исследовательских гипотез",
    "section": "10.5 Анализ качественных данных",
    "text": "10.5 Анализ качественных данных\nРассмотрев случаи, когда зависимая переменная являлась количественной, перейдем к большому классу исследовательских ситуаций, когда мы исследуем взаимосвязи между категориальными показателями.\nК слову, это огромное количество случаев, встречающихся в ходе анализа социологических данных.\n\n10.5.1 Сравниваем независимые группы\n\n10.5.1.1 Одновыборочный z-критерий: сравниваем значение одной доли с теоретическим\nПроводя социологическое исследование, мы можем быть заинтересованы в том, чтобы сравнить результаты, полученные на нашей выборочной с какими-либо предполагаемыми результатами, выраженными в виде пропорции.\n\nПример: Мы исследуем оценки жителей одного из городских кварталов относительно того, согласны ли они, чтобы рядом с их домами оборудовали большой парк, с велодорожками и прочими благами цивилизации. В этот же момент времени предполагается благоустройство и других объектов, поэтому городские власти бы выбрать именно этот объект для инвестиций, но только в том случае, если оценки населения будут благоприятны и составят не менее 70%. Проведя опрос среди 500 жителей мы обнаружили, что 370 из них выразили согласие с обустройством парка, тогда как 130 человек по каким-то причинам были против парка. Превышает ли количество жителей допустимый предел?\n\nПомочь в решении нашей задачи может z-критерий для одной пропорции.\nКак и в других случаях мы сформулируем гипотезы:\nНулевая гипотеза:\n\n\\(H_0:p_o=p_e\\)\n\nАльтернативные гипотезы:\n\n\\(H_1:p_o\\neq p_e\\)\n\\(H_1:p_o&gt;p_e\\)\n\\(H_1:p_o&lt;p_e\\)\n\nСтатистика критерия:\n\\[z = \\frac{p_o-p_e}{\\sqrt{p_oq/n}},\\]\nгде:\n\n\\(p_o\\) наблюдаемая доля\n\\(q=1−p_o\\)\n\\(p_e\\) ожидаемая доля\n\\(n\\) объем выборки.\n\n\nres &lt;- prop.test(x = 370, n = 500, p = 0.7, alternative = \"two.sided\")\nres \n\n\n    1-sample proportions test with continuity correction\n\ndata:  370 out of 500, null probability 0.7\nX-squared = 3.6214, df = 1, p-value = 0.05704\nalternative hypothesis: true p is not equal to 0.7\n95 percent confidence interval:\n 0.6987864 0.7774592\nsample estimates:\n   p \n0.74 \n\n\n\n\n\n\n\n\n\n\n\n\n\n10.5.1.2 Сравнение двух долей (пропорций) - z-критерий\nМы можем распространить наши рассуждения на тот случай, когда нам требуется сравнить две пропорции, в каких-то группах.\nВ этом случае критерий несколько преображается:\n\\[z = \\frac{p_1-p_2}{\\sqrt{pq/n_1+pq/n_2}},\\] где:\n\n\\(p_1\\) доля, наблюдаемая в группе 1, имеющей размер \\(n_1\\)\n\\(p_2\\) доля, наблюдаемая в группе 2, имеющей размер \\(n_2\\)\n\\(p\\) и \\(q\\) - общие пропорции по всей выборке\n\\(q=1−p\\)\n\nДопустим, мы анализируем результаты исследования по климату, и нам хотелось бы знать, как относятся к климатическим изменениям мужчины и женщины. Если точнее, считают ли мужчины более опасным проживание вблизи тающих ледников, чем женщины (или наоборот)?\n\ndf$V1&lt;-sjlabelled::as_label(df$V1)\nprop.table(table(df$V19, df$V1), margin = 2)\n\n   \n      Мужской   Женский\n  1 0.4178571 0.6135182\n  2 0.5821429 0.3864818\n\n\nНа первый взгляд кажется, что женщины, действительно, воспринимают риски тающих ледников как более серьезные. Но как это доказать?\n\nlibrary(infer)\ndf&lt;-df %&gt;% \n  mutate(V19=case_when(\n    V19==1 ~ \"Да\",\n    V19==2~\"Нет\"\n  ))\npr_test&lt;-prop_test(df, V19 ~ V1, order = c(\"Мужской\", \"Женский\"))\npr_test\n\n# A tibble: 1 × 6\n  statistic chisq_df     p_value alternative lower_ci upper_ci\n      &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;\n1      28.4        1 0.000000100 two.sided     -0.268   -0.123\n\n\n\n\n10.5.1.3 Наш любимый хи-квадрат\nКритерий \\(\\chi^2\\) Пирсона – один из самых известных тестов для анализа категориальных данных. Основан на сравнении наблюдаемых и ожидаемых частот.\n\\[\\chi^2=\\sum\\frac{(O-E)^2}{E},\\]\nГде: - \\(O\\) - наблюдаемые значения - \\(E\\) - ожидаемые значения (которые были бы в том случае, если бы никакой взаимосвязи между частотами не было)\nГипотезы:\n\n\\(H_0\\) – между зависимой и независимой переменной нет взаимосвязи – наблюдаемые частоты не отклоняются от ожидаемых частот.\n\\(H_1\\)- между зависимой и зависимой переменной есть взаимосвязь, наблюдаемые частоты значимо отклоняются от ожидаемых.\n\nУсловия применения статистического критерия хи-квадрата Пирсона\n\nТип данных: параметры должны быть качественными целочисленными частотами, измеренными в номинальной шкале:\n\nбинарными (пол: мужской/женский) или с большим количеством градаций (регион, группа - обучения, специальность)\nпорядковыми (степень артериальной гипертензии)\n\nКоличество наблюдений более 20\n\nОжидаемая частота, соответствующая нулевой гипотезе должна быть более 5, если ожидаемое явление принимает значение менее 5, то необходимо использовать точный Критерий Фишера.\nДля четырехпольных таблиц (2х2): Если ожидаемое значение принимает значение менее 10 (а именно 5&lt;x&lt;10), необходим расчет поправки Йетса.\n\nСравниваемые группы должны быть примерно одного размера.\nСопоставляемые группы должны быть независимыми (то есть единицы наблюдения не зависят друг от друга). Для парных сравнений (типа «до-после» существует отдельный тест МакНемара (McNemar).\n\n\nЗапрещается: использовать хи-квадрат для анализа непрерывных абсолютных данных, процентов и долей без предварительной перекодировки!\n\n\nchisq.test(df$V1, df$V19)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  df$V1 and df$V19\nX-squared = 28.37, df = 1, p-value = 1.002e-07\n\n\nОдной из наиболее удачных идей визуализации таблиц сопряженности являются мозаичные диаграммы:\n\nlibrary(vcd)\n options(OutDec= \",\") \n#создадим базовую таблицу\nM&lt;- table(df$V19, df$V1)\n#подпишем оси\nnames(dimnames(M)) = c(\"Опасны ли тающие ледники?\",\"Пол\")\n#создадим таблицу с надписями\nlabs &lt;- round(prop.table(M,margin=2)*100, 1)\n#обычная мозаичная диаграмма\nmosaic(M, pop = FALSE, shade = TRUE)\n#добавим значения в ячейках\nlabeling_cells(text = labs, margin = 0)(M)\n\n\n\n\n\n\n\n\nЕще один вариант - ассоциативный (структурный) график:\n\nstruct &lt;- structable(~ V19 + V1, data = df)\nassoc(struct, data = df, shade=T, labeling_args = list(set_varnames = c(V1 = \"Пол\", V19=\"Опасны ли тающие ледники?\")))\n\n\n\n\n\n\n\n\n\n\n\n10.5.2 Сравнение зависимых выборок\n\n10.5.2.1 Тест МакНемара\nТест Макнемара используется для определения наличия статистически значимой разницы в пропорциях для парных номинальных данных, представленных в таблицах 2 × 2.\nНазван в честь Куинна Майкла Макнемара (1900 –1986), американского психолога и статистика, президента Американской психологической ассоциации (1964), разработавшего критерий в 1947 году.\n\nТест МакНемара. Формула и процедура оценки\n\n\n\n\n\n\nРисунок 10.23: Типичная таблица для проведения теста МакНемара\n\n\n\nТест основывается на нулевой гипотезе (\\(H_0\\)) о маргинальных частотах: маргинальные вероятности для каждого исхода равны:\n\\(p_a + p_b = p_a + p_c\\) и \\(p_c + p_d = p_b + p_d\\).\n\n\\(H_0: p_b=p_c\\)\n\\(H_1: p_b\\neq p_c\\)\n\n\\[\\chi^2=\\frac{(b-c)^2}{b+c}\\]\n\n Пример: рассмотрим пример из области общественного здравоохранения. В нашем наболе содержатся данные о страховых компаниях и покрытиях страховыми полисами услуг по оказании инфузионной терапии (часто применяется при раковых заболеваниях). Вопрос: меняется ли количество страховщиков, включающих в полис данную услугу?\n\nСкачать данные\nЗагрузим данные:\n\nload(\"dataset-him-2014-2016-subset2.Rdata\")\n\nСоздадим таблицу по данным 2014 и 2016 гг.:\n\ntable_for_McNemar &lt;-table(data$X.2014, data$X.2016c)\ntable_for_McNemar\n\n     \n      No  Yes\n  No   22 105\n  Yes  54 145\n\n\nДовольно заметно, что количество страховых компаний, покрывающих своими полисами инфузионную терапию за два года выросло довольно существенно: 105 компаний, которые в 2014 году отказывались от этой опции, в 2016 году включили ее в список услуг. Обратная ситуация встречается в два раза реже: только 54 организации, которые в 2014 году оплачивали стоимость инфузионной терапии по страховке, в 2016 году отказались это делать.\nПроведем тест МакНемара:\n\nmcnemar.test(table_for_McNemar)\n\n\n    McNemar's Chi-squared test with continuity correction\n\ndata:  table_for_McNemar\nMcNemar's chi-squared = 15,723, df = 1, p-value = 7,332e-05\n\n\nТест показывает наличие значимых отличий.\n\n\n10.5.2.2 Тест Кохрана\nЕсли количество сравниваемых групп больше двух, то используется расширение теста Макнемара - Q-критерий Кохрана.\nНулевая гипотеза (\\(H_0\\)): доли (пропорции) «успеха» во всех группах равны. Предполагается, что данные организованы в «блоки». «Блоками» могут быть, например, отдельные люди.\n\nФормула теста:\n где:\n\n\\(b\\) = количество блоков,\n\\(k\\) = количество условий,\n\\(X•j\\) = сумма по столбцу для условия j,\n\\(Xi•\\) = сумма по строке для блока i\n\\(N\\) = общая сумма.\n\nПродолжим работу с предыдущим примером.\nПереведем данные из широкого в длинный формат и отберем данные без :\n\ndata&lt;-data %&gt;% \n  select(-ends_with(\"c\")) %&gt;% \n  pivot_longer(cols = contains(\"X.\"), values_to = \"outcome\", names_to = \"year\")\n\nПроведем тест Кохрана:\n\ncochran_qtest(data, outcome ~ year|IssuerId)\n\n# A tibble: 1 × 6\n  .y.         n statistic    df            p method          \n* &lt;chr&gt;   &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt; &lt;chr&gt;           \n1 outcome   326      33.2     2 0.0000000609 Cochran's Q test\n\n\nВыявлены значимые различия между периодами. Но какие? Как и всегда с множеством групп, необходимы парные сравнения.\nСделаем сначала общую таблицу по трем периодам:\n\ndata %&gt;% \n  group_by(year, outcome) %&gt;% \n  summarise(sum=n()) %&gt;% \n  pivot_wider(names_from = outcome, values_from = sum)\n\n# A tibble: 3 × 3\n# Groups:   year [3]\n  year   `No `   Yes\n  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;\n1 X.2014   127   199\n2 X.2015    69   257\n3 X.2016    76   250\n\n\nТеперь проведем парные сравнения, с помощью теста МакНемара:\n\npairwise_mcnemar_test(data, outcome ~ year|IssuerId)\n\n# A tibble: 3 × 6\n  group1 group2            p        p.adj p.adj.signif method      \n* &lt;chr&gt;  &lt;chr&gt;         &lt;dbl&gt;        &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;       \n1 X.2014 X.2015 0.0000000166 0.0000000498 ****         McNemar test\n2 X.2014 X.2016 0.0000733    0.00022      ***          McNemar test\n3 X.2015 X.2016 0.55         1            ns           McNemar test\n\n\nРезультаты сравнительного анализа показывают, что наиболее важный скачок произошел в 2015 году, когда количество организаций увеличилось на 58, в последующий год их количество даже сократилось, но это сокращение не является статистически значимым.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Статистический вывод и тестирование исследовательских гипотез</span>"
    ]
  },
  {
    "objectID": "Statistical-Inference.html#самостоятельная-работа",
    "href": "Statistical-Inference.html#самостоятельная-работа",
    "title": "10  Статистический вывод и тестирование исследовательских гипотез",
    "section": "10.6 Самостоятельная работа",
    "text": "10.6 Самостоятельная работа\n\nПроанализируйте различия в оценках уверенности в нахождении заработка в случае потери актуального места работы (вопрос В21) в зависимости от образовательного уровня респондента (вопрос А6), перекодировав его в две категории - те, кто имеют высшее образование, и кто его не имеет.\nИспользуя созданную переменную важности соблюдения традиций, проведите анализ по уровню образования (по переменной, созданной в упр.1) и по самооценке материального положения (вопрос А10), создав три категории - «низкие доходы», «средние доходы» и «высокие доходы, обеспеченные граждане». Используйте параметрические методы там, где это уместно, и непараметрические там, где возможно применение только таких методов. Обязательно создайте визуализации к вашим результатам.\nПровести анализ оценки лиц, придерживающихся определенной диеты, за три промежутка времени, используя параметрический ANOVA и его непараметрический аналог с парными сравнениями. Сделать соответствующие визуализации.\n\nДанные:\n\nlibrary(datarium )\ndata(\"selfesteem\")\n\n\nПровести анализ таблиц сопряженности по исследованию по климату (две таблицы на ваш выбор) с помощью критерия хи-квадрат. Создать мозаичную диаграмму и ассоциативный график.\nОформите ваши результаты любым удобным для вас способом и приложите в качестве ответа на задание.\n\n\n\n\n\nBrown, Angus. «The strange origins of the Student’s t-test — physoc.org». https://www.physoc.org/magazine-articles/the-strange-origins-of-the-students-t-test/.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Статистический вывод и тестирование исследовательских гипотез</span>"
    ]
  },
  {
    "objectID": "ImportExport.html",
    "href": "ImportExport.html",
    "title": "6  Импорт и экспорт данных",
    "section": "",
    "text": "6.1 Импорт и экспорт файлов Excel\nИмпортировать данные из файла Excel можно несколькими способами.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Импорт и экспорт данных</span>"
    ]
  },
  {
    "objectID": "ImportExport.html#импорт-и-экспорт-файлов-excel",
    "href": "ImportExport.html#импорт-и-экспорт-файлов-excel",
    "title": "6  Импорт и экспорт данных",
    "section": "",
    "text": "Важно!: сохраните все файлы с дополнительными материалами к заданию в ту же директорию, где будете работать (лучше всего создать новый проект под это занятие). Это нужно для того, чтобы лишний раз не прописывать путь к файлу).\nЕще один удобный ход - установить в настройках рабочую директорию в ту же папку, где лежит файл со скриптом:\nSession - Set Working Directory - To Source File Location",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Импорт и экспорт данных</span>"
    ]
  },
  {
    "objectID": "ImportExport.html#способ-1.-скопировать-из-excel-и-сохранить-в-r.",
    "href": "ImportExport.html#способ-1.-скопировать-из-excel-и-сохранить-в-r.",
    "title": "6  Импорт и экспорт данных",
    "section": "6.2 Способ 1. Скопировать из Excel и сохранить в R.",
    "text": "6.2 Способ 1. Скопировать из Excel и сохранить в R.\nДля того, чтобы воспользоваться этим способом, сохраните файл, откройте его, выделите все данные и скопируйте их (Ctrl+C). Затем запустите код ниже:\n\nmy_data &lt;- read.table(file = \"clipboard\", sep = \"\\t\", header=TRUE)\n\nЕсли все сделано правильно, то у Вас в рабочем окружении появится объект my_data, в котором будет 9 наблюдений и 14 переменных (это результаты реализации государственной программы содействия добровольному переселению соотечественников). Чтобы увидеть данные, нажмите на него и он откроется в просмотрщике.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Импорт и экспорт данных</span>"
    ]
  },
  {
    "objectID": "ImportExport.html#способ-2.-импорт-из-файла-excel.",
    "href": "ImportExport.html#способ-2.-импорт-из-файла-excel.",
    "title": "6  Импорт и экспорт данных",
    "section": "6.3 Способ 2. Импорт из файла Excel.",
    "text": "6.3 Способ 2. Импорт из файла Excel.\nПредположим, файл Excel у нас большой, содержит несколько листов, скопировать все и вставить - не очень удачная идея. В этом случае (честно говоря, такие случаи случаются гораздо чаще, чем описанные в способе 1), нам нужно импортировать данные из файла.\nЭто сделать не так сложно, к счастью, у нас есть библиотека readxl, которая как раз и предназначена для таких целей.\nПорядок действий следующий:\n\nЗагружаем библиотеку readxl.\nЗапускаем функцию read_excel(file excel, sheet = \"name of sheet\")\n\nДавайте загрузим тот же файл с соотечественниками, что и в примере выше:\n\nlibrary(readxl)\ndf_excel&lt;-read_excel(\"ProgResCompatriots.xlsx\", sheet=1)\n\nВ рабочем окружении отобразился новый объект, его содержание идентично my_data. Чтобы посмотреть - нужно нажать на голубой кружок с белым треугольником рядом с именем:\n\nПамятка по работе с readxl:",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Импорт и экспорт данных</span>"
    ]
  },
  {
    "objectID": "ImportExport.html#экспорт-результатов-в-excel",
    "href": "ImportExport.html#экспорт-результатов-в-excel",
    "title": "6  Импорт и экспорт данных",
    "section": "6.4 Экспорт результатов в Excel",
    "text": "6.4 Экспорт результатов в Excel\nДавайте попробуем создать новую переменную, куда посчитаем среднее количество перехавших за год в период с 2010 по 2021 гг.\n\nlibrary(dplyr)\ndf_excel$Среднее&lt;-apply(df_excel[,2:13],1, mean)\n\nУ нас появился новый столбец в конце таблицы.\nТеперь эту новую таблицу нужно сохранить в новый файл. Для этого нам понадобится библиотека xlsx, а в ней - функция write.xlxs().\nЗапустите следующий код:\n\ninstall.packages(\"xlsx\") #установим библиотеку\nlibrary(xlsx)\nwrite.xlsx(df_excel, \"df_excel_new.xlsx\")",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Импорт и экспорт данных</span>"
    ]
  },
  {
    "objectID": "ImportExport.html#импорт-и-экспорт-файла-в-формате-csv",
    "href": "ImportExport.html#импорт-и-экспорт-файла-в-формате-csv",
    "title": "6  Импорт и экспорт данных",
    "section": "6.5 Импорт и экспорт файла в формате CSV",
    "text": "6.5 Импорт и экспорт файла в формате CSV\nCSV (от англ. Comma-Separated Values — значения, разделённые запятыми) — текстовый формат, предназначенный для представления табличных данных. Строка таблицы соответствует строке текста, которая содержит одно или несколько полей, разделенных запятыми, то есть, по сути, это данные, представленные в текстовом формате.\nСтатистическая информация часто хранится именно в формате CSV, этому формату уже более 40 лет. Текстовые файлы открываются читаются на любом устройстве и в любой среде без дополнительных инструментов. Из-за своих преимуществ CSV — сверхпопулярный формат обмена данными.\nЗа импорт таких файлов отвечает библиотека readr, а в ней - функция read_csv2 (есть и просто read_csv, но там - разделитель действительно запятая, а в read_csv2 - точка с запятой). В чем проблема? Проблема в том, что в России запятая используется в качестве разделителя десятичных разрядов, и когда мы начинаем сохранять в формате csv возникает конфликт. Чтобы его не было, программа вместо запятой использует разделитель - точку с запятой. Это хорошо видно, если посмотреть, как эти данные выглядят в блокноте.\nЕсть и аналогичные функции базового R - read.csv()и read.csv2. Их отличие от функций read_csv2 и read_csvзаключается в быстроте последних, нюансах в обработке строковых переменных (базовые функции принудительно преобразуют их в факторные переменные, тогда как более современные read_csv2 и read_csv оставляют тип character). Есть и некоторые другие отличие, но их обсуждение выходит за рамки данного пособия.\nПорядок действий:\n\nСкачать файл в формате csv.\nОтрыть файл:\n\n\nlibrary(readr)\ndf_csv&lt;-read_csv2(\"ProgResCompatriots.csv\")\n\nУ нас теперь уже три файла с одинаковыми данными). Что с ними делать? Что-нибудь придумаем по ходу занятия…\nНапример, давайте посчитаем прирост соотечественников в 2021 году по сравнению с 2020 годом.\n\ndf_csv$change2021&lt;-df_csv$`2021`-df_csv$`2020`\n\nСохраним наш файл в этом же формате:\n\nwrite.csv2(df_csv, \"df_csv_new.csv\")",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Импорт и экспорт данных</span>"
    ]
  },
  {
    "objectID": "ImportExport.html#импорт-файла-по-url",
    "href": "ImportExport.html#импорт-файла-по-url",
    "title": "6  Импорт и экспорт данных",
    "section": "6.6 Импорт файла по URL",
    "text": "6.6 Импорт файла по URL\nВ Интернете хранится огромное количество различных данных, но с точки зрения импорта можно рассмотреть несколько случаев:\n\nкогда нам нужно просто загрузить файл из Интернета и сохранить его в директорию\nкогда ссылка является прямой и по ней мы можем загрузить данные определенного формата\nкогда у нас есть ссылка на ресурс, в котором содержатся данные вперемешку с текстом, и нам нужно сохранить только табличные данные\n\n\n6.6.1 Загрузка файла в рабочую папку\nВоспользуемся данными, представленными на портале “Открытые данные России” и скачаем оттуда перечень стран и режимов въезда на их территорию.\nМы будем использовать очень простую фунцию download.file(), в которой мы должны указать два аргумента - ссылку и имя файла, в который мы будем сохранять данные. Заметьте, что формат выгружаемого и сохраняемого файла должны совпадать.\n\ndownload.file(\"https://www.isras.ru/files/File/Bank/010%2080055.pdf\", \"anketa.pdf\", mode=\"wb\")#\n\nПосмотрите в своей рабочей директории, скачался ли файл, попробуйте его открыть.\n\n\n6.6.2 Импорт по прямой ссылке в формате csv\nЗдесь тоже нет ничего сложного. Мы уже только что пробовали открывать файлы в формате csv, которые хранятся у нас на компьютере. То же самое происходит с ссылками из Интернета.\nЗагрузим данные об исследованиии по науке и разработкам, представленное на портале статистического Информационного центра при правительстве Новой Зеландии:\n\nSys.getlocale()\nlibrary(readr)\nRnD&lt;-read_csv(\"https://www.stats.govt.nz/assets/Uploads/Research-and-development-survey/Research-and-development-survey-2022/Download-data/research-and-development-survey-2022.csv\")\n\n\n\n6.6.3 Парсинг таблиц из Интернета\nПожалуй, это самое интересное.\nПредположим, мы читаем статью в Википедии, и нам понравились данные, которые там приводятся. Конечно, мы можем скопировать эти данные с помощью мышки, но зачем? Ведь у нас есть R. К тому же результаты такого копирования часто оставляют желать лучшего.\nНапример, возьмем страницу в Википедии, посвященную международному индексу счастья.\nВпрочем, Вы можете взять любую другую страницу.\nДля того, чтобы выгрузить данные непосредственно со страницы, нам понадобится библиотека rvest.\nЗагрузив библиотеку, создадим объект content, в который мы загрузим данные страницы с помощью функции read_html():\n\nlibrary(rvest)\ncontent &lt;- read_html(\"https://ru.wikipedia.org/wiki/%D0%9C%D0%B5%D0%B6%D0%B4%D1%83%D0%BD%D0%B0%D1%80%D0%BE%D0%B4%D0%BD%D1%8B%D0%B9_%D0%B8%D0%BD%D0%B4%D0%B5%D0%BA%D1%81_%D1%81%D1%87%D0%B0%D1%81%D1%82%D1%8C%D1%8F\")\n\nЕсли рассмотреть этот объект, то можно увидеть, что это список, содержащий содержимое страницы по тэгам - head и body, в которых что-то хранится в формате xml.\nДалее, с помощью функции html_table давайте “вытащим” таблицы и сохраним их отдельно с именем tables:\n\ntables &lt;- content %&gt;% html_table(fill = TRUE)\n\nВидим, что таких таблиц 5.\nДавайте сохраним первую из них:\n\ntable1 &lt;- tables[[3]]\n\nА теперь сохраним ее себе на компьютер с помощью знакомой уже функции write.xlsx():\n\nwrite.xlsx(table1, \"HPI.xlsx\")",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Импорт и экспорт данных</span>"
    ]
  },
  {
    "objectID": "ImportExport.html#импорт-файла-из-spss-файл-.sav",
    "href": "ImportExport.html#импорт-файла-из-spss-файл-.sav",
    "title": "6  Импорт и экспорт данных",
    "section": "6.7 Импорт файла из SPSS (файл .sav)",
    "text": "6.7 Импорт файла из SPSS (файл .sav)\nЭто наш любимый формат, поскольку мы с вами специалисты по социологическим исследованиям, любим SPSS и хотели бы соединить возможности этого пакета с возможностями R.\nЧтобы загрузить в R файл в формате .sav, мы должны:\n\nво-первых, загрузить файл. Это результаты недавнего исследования кафедры об изменениях климата в высокогорных районах Алтая.\nво-вторых, нам нужно загрузить библиотеку haven, с помощью которой мы можем импортировать данных из формата программы SPSS в R.\n\n\nlibrary(haven)\n\nПосле того, как мы загрузили библиотекy, импортируем базу данных с помощью функции read_sav:\n\ndf&lt;-read_sav(\"База_КлимРиск_2023.sav\", user_na = TRUE) # user_na позволяет активировать настройки, насающиеся пропущенных значений, определяемых пользователем, например, когда значение 99 закодировано как \"затрудняюсь ответить\" и в базе данных оно установлено как пропущенное.\n\nИтак, наши данные теперь сохранены в датафрейме с именем df. Мы видим, что в нем есть 202 переменных и 913 наблюдений.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Импорт и экспорт данных</span>"
    ]
  },
  {
    "objectID": "ImportExport.html#самостоятельная-работа",
    "href": "ImportExport.html#самостоятельная-работа",
    "title": "6  Импорт и экспорт данных",
    "section": "6.8 Самостоятельная работа",
    "text": "6.8 Самостоятельная работа\nУпражнение 1. Используя портал Открытых данных России, найти интересующие Вас данные в разных форматах - excel, csv. Загрузить данные по ссылке, внести в них изменения (создать новую переменную, что-то поменять, используя R) и сохранить новый файл в аналогичном формате.\nУпражнение 2. С официального сайта Алтайского края скачать Указ Губернатора о присвоении звания ветерана труда и загрузить его в рабочую папку.\nУпражнение 3. С официального сайта Алтайского края, из раздела, посвященного национальной политике, скачать список национально-культурных организаций и сохранить его в формате excel.\nУпражнение 4. Загрузить базу данных своего магистерского исследования из SPSS в R.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Импорт и экспорт данных</span>"
    ]
  },
  {
    "objectID": "About-R.html",
    "href": "About-R.html",
    "title": "1  Основные сведения об R",
    "section": "",
    "text": "1.1 Что такое R?\nR – это язык программирования и свободная программная среда для статистической обработки и визуализации данных.\nНесмотря на наличие огромного количества языков и различных программ для статистической обработки данных, R в течение двух десятилетий остается популярным языком и средой обработки и анализа данных для специалистов из разных областей знания.\nПозиции языка R как средства разработки среди других языков программирования довольно высоки. Так, по данным индекса Tiobe за 2022 год, R занимал12-е место в мире, и хотя в 2024 году его авторитет заметно упал (23-е место), его используют миллионы аналитиков и разработчиков по всему миру:\nПрежде всего, это язык, который используют ученые, специалисты, работающие в различных отраслях экономики, менеджеры для анализа реальных данных и разработки научно обоснованных систем принятия решений, поэтому место в общем рейтинге не так высоко. Если посмотреть сферы использования, то на первом месте - академическая среда, на втором - сфера здравоохранения, на третьем - правительственные учреждения.\nR используют банки и маркетинговые агентства, технические компании и информационные корпорации для разных целей - от обработки данных до прогнозирования и представления интерактивной инфографики.\nВот только небольшой список тех компаний, которые используют R в своей деятельности.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Основные сведения об R</span>"
    ]
  },
  {
    "objectID": "About-R.html#немного-истории",
    "href": "About-R.html#немного-истории",
    "title": "1  Основные сведения об R",
    "section": "1.2 Немного истории",
    "text": "1.2 Немного истории\nR был создан профессорами Россом Айхэка и Робертом Джентельменом (Ross Ihaka и Robert Gentleman) в 1992 году, сначала как язык программирования для обучения студентов статистике в университете Окленда (Новая Зеландия). Авторы вдохновлялись при создании языком S, используемым в лаборатории Bell, и ради шутки назвали язык R - по первым буквам собственных имен.\nВ июне 1995 года статистик Мартин Махлер убедил Айхэку и Джентельмена опубликовать R как язык со свободным исходным кодом под публичной лицензией GNU. Первая официальная версия была выпущена 29 февраля 2000 года.\nЧуть ранее, в 1997 году Куртом Хорником и Фрицем Лейшем была основана Сеть для архивирования кода R (CRAN, The Comprehensive R Archive Network), цель которой заключалась в хранении исходного кода, выполняемых файлов, документации и библиотек, создаваемых пользователями. На момент декабря 2022 года CRAN имел 103 зеркальных сервера и 18 976 библиотек.\nКоманда разработчиков (R Core Team) также была основана в 1997 году для дальнейшего развития языка. Сейчас в ней состоят ведующие разработчики, статистики, специалисты по компьютерным наукам, всего более 20 человек. В апреле 2003 года для развития проекта была создана некоммерческая организация R Foundation. Цель фонда заключается в предоставлении технической поддержки и коммуникации с создателями R, хранении и управлении технической документацией и интеллектуальной собственностью.\nПодробнее об истории создания R можно узнать из таймлайна: \nСоздатели R (Питер Далгаард - датский ученый, один из членов R Core Team):",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Основные сведения об R</span>"
    ]
  },
  {
    "objectID": "About-R.html#каковы-преимущества-r",
    "href": "About-R.html#каковы-преимущества-r",
    "title": "1  Основные сведения об R",
    "section": "1.3 Каковы преимущества R?",
    "text": "1.3 Каковы преимущества R?\nИх довольно много:\n\nВозможности для статистической обработки, от простых функций до сложных моделей.\n\nПочти все новое, что появляется в области статистики, можно найти в одной из библиотек R. Например, ANZ банк использует R для моделирования невыплат по ипотечному кредитованию, а The Bank Of America применяет R для формирования финансовой отчетности.\n\nЭто язык программирования с открытым исходным кодом (Open-source)\n\nЧто это значит? Это значит, что, во-первых, все написанное на R открыто для изучения и критики, а, во-вторых, каждый может внести вклад в его развитие и улучшение путем создания новых библиотек и новых функций для решения различных задач.\n\nПоддержка сообщества (Community)\n\nУ R - более 2 миллионов пользователей по всему миру, сообщество пользователей R не только внушительное по размеру, но и очень активное. Каким бы ни был ваш проект - учебным или крупномасштабным, всегда найдется тот, кто поможет разобраться в коде и принять правильное решение. Вы тоже можете найти себе единомышленников и подключаться к другим проектам.\nНекоторые полезные ссылки:\n\nhttps://community.rstudio.com/\nhttps://www.r-bloggers.com/\nhttps://stackoverflow.com/questions/tagged/r\nhttps://rweekly.org/ https://www.reddit.com/r/Rlanguage/\nhttps://r.awesome-programming.com/en/awesome/r-language-02/community\n\nКоммьюнити пользователей R это совсем не Man’s World: весьма активно женское сообщество RLadies, устраивающее митапы по всему миру и продвигающее свой особый, женский, взгляд на использование R для разработки и анализа данных.\n\nОгромная коллекция библиотек и полезных функций, позволяющих расширить возможности базового языка\n\nСамые авторитетные хранятся в CRAN (Comprehensive R Archive Network), их более 10 тысяч, однако, чтобы попасть туда, необходимо, чтобы документация и код соответствовали определенным требованиям, что требует времени. До того момента, когда библиотека загружена на CRAN, она, как правило, хранится в открытом доступе, например, на github, где каждый желающий может принять участие в ее тестировании и доработке.\nПример сетевого анализа библиотек и их взаимозависимостей представлен на рисунке ниже:\n\nПримеры полезных библиотек:\n\nобработка и всевозможные манипуляции с данными (tidytverse)\nработа с большими данными (sparklyr)\nглубокое обучение (keras, TensorFlow)\nмашинное обучение (H2O)\nвизуализация данных (ggplot2)\nсоздание отчетов, итерактивная графика и обучение (Rmarkdown, shiny).\n\n\nСовместимость с другими языками программирования\n\nБольшинство функций и библиотек написаны на самом языке R. Однако, для сложных вычислительных задач, могут использоваться и другие языки, такие как C, C++, FORTRAN. Для манипуляций с объектами, возможно использование других языков - .NET, Java, Python. Иными словами, возможности программирования становятся практически безграничными.\n\nСоздание привлекательных визуализаций\n\nВ современном мире анализ данных невозможен без качественной визуализации, особенно если его результаты планируется использовать в сфере политики и бизнеса. R является одним из лучших инструментов для создания качественной графики, и такие библиотеки как ggplot2, plotly, ggvis помогут создать очень детализированные и эстетически привлекательные визуализации.\nДля пользователей R, которые только начинают изучение языка, на сайте https://r-graph-gallery.com создана галерея визуализаций, разбитых по отдельным тематикам. На основе простых и более сложных примеров можно изучить код и адаптировать его под собственные задачи.\n\n\nИнтеграция с Hadoop и анализ больших данных\n\nЕсли перед вами стоит задача анализа больших данных, то с такими библиотеками как rmr, rhdfs, rhbase, RHIVE, RHIPE и Rhadoop возможно интегрировать R и Hadoop (проект фонда Apache Software Foundation для разработки и выполнения распределённых вычислений для работы с большими данными).\nВозможности по хранению данных Hadoop и вычислительные достоинства R используют многие в качестве оптимального решения для анализа больших данных. Например, компания Форд использует R и Hadoop для обработки данных обратной связи с потребителями, что позволяет им улучшить дизайн и обосновывать бизнес решения.\n\n\nСоздание интерактивных веб-приложений\n\nС помощью R и библиотеки shiny можно создавать интерактивные приложения, с помощью которых пользователи (ученики, заказчики, журналисты и пр.) могут познакомиться с вашими данными, провести какие-то виды анализа, сделать визуализацию, возможно изучить какие-то закономерности (часто используются как обучающий инструмент). Эти приложения можно хранить на сервере Shiny или в другом доступном месте.\n\nПримеры можно посмотреть в галерее на сайте shiny: (https://shiny.posit.co/r/gallery/):\n\n\nСовместимость с другими платформами\n\nR может работать с любой конфигурацией оборудования и поддерживает различные операционные системы, независимо от окружения выдает предсказуемые и однозначные результаты.\n\nВозможность запуска кода без компилирования (возможно этот пункт стоило бы поставить на одно из первых мест)\n\nR относится к интерпретируемым языкам, что означает, что ему не требуется компилятор для того, чтобы программа заработала. Иными словами, все команды, которые мы вводим, сразу же выполняются, без дополнительного компилирования (сборки), как это происходит в других языках, например в C, COBOL, Delphi или Fortran. Другими возможностями, которыми обладают интерпретируемые языки, являются кросс-платформенность (способность работать в разных операционных системах и аппаратных средах), простота тестирования и отладки программ.\n\nНапример, Если мы создадим несколько строк кода и запустим их, они выполнятся немедленно и мы увидим результат:\n\nКроме этих преимуществ есть и много других, о которых мы узнаем в процессе изучения курса.\nК слову сказать, данное пособие также создано с помощью R. При его создании использовались возможности open-source системы публикации научной и технической информациии Quarto.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Основные сведения об R</span>"
    ]
  }
]