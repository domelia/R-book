[
  {
    "objectID": "Regression.html",
    "href": "Regression.html",
    "title": "12  Регрессионный анализ в R",
    "section": "",
    "text": "12.1 Основы линейной регрессии\nНа предыдущих занятиях мы изучили основные типы и структуры данных, овладели основами создания простых функций на R, научились импортировать и экспортировать данные в разных форматах. Мы уже умеем проводить различные трансформации данных, визуализировать данные разного типа и проводить одномерный и двумерный анализ на категориальных данных. Мы даже справились со сложной задачей анализа данных с помощью метода главных компонент и анализа соответствий.\nМы продолжаем изучать многомерные методы, и следующий на очереди - регрессионный анализ и его отдельные разновидности.\nВ обобщенном виде уравнение регрессионного анализа может быть представлено как:\n\\[Y=X\\beta+\\epsilon,\\]\nгде \\(Y\\) - вектор значений зависимой переменной, \\(X\\) - матрица значений независимых переменных (предикторов), \\(\\beta\\) - вектор коэффициентов регрессии, используемых для подгонки к известным значениям зависимой переменной, \\(\\epsilon\\) - ошибки модели (остатки, разница между реальными и предсказанными значениями).\nПреимуществами и причинами популярности регрессионного анализа являются следующие:\nЧаще всего используются следующие виды регрессионного анализа:",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Регрессионный анализ в R</span>"
    ]
  },
  {
    "objectID": "Regression.html#основы-линейной-регрессии",
    "href": "Regression.html#основы-линейной-регрессии",
    "title": "12  Регрессионный анализ в R",
    "section": "",
    "text": "регрессионные модели могут включать множество предикторов одновременно, что позволяет оценивать вклад каждого при условии контроля над остальными параметрами;\nсуществует большое количество разновидностей регрессионного анализа, могут использоваться различные типы предикторов и зависимых переменных;\nдовольно легко интерпретировать результаты;\nдостаточно просты в применении и не слишком сложны с математической точки зрения.\n\n\n\nлинейная регрессия (зависимая переменная числовая)\nлогистическая регрессия (зависимая переменная бинарная)\nпорядковая регрессия (зависимая переменная - упорядоченная факторная)\nмультиномиальная регрессия (зависимая переменная категориальная)\n\n\n12.1.1 Загрузка данных\nВ качестве практического примера мы будем использовать учебный набор данных о взаимосвязи между объемами продаж и затратами на рекламу. Это небольшой набор, который поможет понять основные идеи метода и его различные реализации.\nКак всегда, мы начинаем с загрузки данных.\nСкачать данные\n\nlibrary(readr)\nAdvertising = read_csv(\"Advertising.csv\")\n\nПосле загрузки данных в R, первым делом нужно посмотреть сами данные, их структуру. Поскольку мы использовали функцию read_csv(), данные были загружены в формате tibble (tbl_df). Это практически датафрейм, вернее его усовершенствованная версия, в которой данные обрабатываются быстрее, и не происходит некоторых неприятных трансформаций (например, не меняются типы и имена данных).\nМы видим, что у нас всего 200 наблюдений и 4 переменных типа double (числовой формат, в котором происходит более точное округление десятичных знаков до 16 знаков после запятой - с 64-битной, то есть двойной точностью).\nВ нашем наборе переменная Sales (Продажи) будет являться зависимой переменной, и мы будем пытаться выявить взаимосвязи между продажами и тремя другими - независимыми переменными: TV, Radio, и Newspaper, обозначающими, соответственно, затраты на рекламу на телевидении, радио и в газетах.\n\n\n12.1.2 Предварительная визуализация данных\nПосле рассмотрения структуры данных, следующий шаг - это визуализация. Поскольку у нас только количественные (не категориальные) переменные, лучший способ их представить - это диаграммы рассеяния, которые мы можем сделать для каждого индивидуального предиктора.\nНапример, для рекламы на телевидении\n\nplot(Sales ~ TV, data = Advertising, col = \"dodgerblue\", pch = 20, cex = 1.5,\n     main = \"Продажи vs Реклама на телевидении\")\n\n\n\n\n\n\n\n\nСамостоятельная работа: сделайте аналогичные графики для других переменных.\nЧтобы сделать все графики сразу, можно воспользоваться функцией pairs().\n\npairs(Advertising)\n\n\n\n\n\n\n\n\nЧасто нам интересно посмотреть взаимосвязи только между зависимой переменной и предиктором, а функция pairs() выдает много лишнего.\nФункция featurePlot() из библиотеки caret (Classification And REgression Training), подходит для этой цели гораздо лучше.\n\nlibrary(caret)\nfeaturePlot(x = Advertising[ , c(\"TV\", \"Radio\", \"Newspaper\")], y = Advertising$Sales)\n\n\n\n\n\n\n\n\nМы видим, что есть явный рост продаж по мере увеличения рекламы на радио и телевидении, тогда как связь с рекламой в газетах не так очевидна.\n\n\n12.1.3 Простая линейная регрессия и функция lm()\nДавайте построим простую линейную модель для продаж, в которой в качестве предиктора будут выступать затраты на телевизионную рекламу.\n\nmod_1 = lm(Sales ~ TV, data = Advertising)\n\n\n12.1.3.1 Общие результаты и тестирование гипотез\nФункция summary() позволяет вывести на экран информацию о модели, полученную с помощью функции lm(), которая может быть полезной для тестирования гипотез, касающихся предикторов и оценки значимости регрессионных коэффициентов.\n\nsummary(mod_1)\n\n\nCall:\nlm(formula = Sales ~ TV, data = Advertising)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.3860 -1.9545 -0.1913  2.0671  7.2124 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 7.032594   0.457843   15.36   &lt;2e-16 ***\nTV          0.047537   0.002691   17.67   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.259 on 198 degrees of freedom\nMultiple R-squared:  0.6119,    Adjusted R-squared:  0.6099 \nF-statistic: 312.1 on 1 and 198 DF,  p-value: &lt; 2.2e-16\n\n\nДавайте разбираться!\nВывод начинается с повторения регрессионного уравнения под заголовком Call: lm(formula = Sales ~ TV, data = Advertising).\nЗатем в модели приводится распределение остатков. Остатки должны иметь нормальное распределение с абсолютными значениями минимума и максимума, также как и квартилями очень близкими друг к другу, что предполагает их примерно одинаковое расстояние от центра распределения.\nВ нашем случае это правило выполняется.\nСледующая часть вывода содержит таблицу с коэффициентами.\nЧтобы понять, что они означают и каким образом получаются, приведем формулу регрессии, но для случая с одной переменной:\n\\[y_i=\\alpha+\\beta x_i+\\varepsilon_i,\\]\nгде \\(y\\) - зависимая переменная, \\(i\\) - единица анализа, \\(\\alpha\\) - интерцепт (константа), \\(\\beta\\) - коэффициент регрессии, \\(x\\) - независимая переменная и \\(\\varepsilon\\) - ошибка.\nКогда мы работаем с выборочными данными, формула изменяется, так как вместо истинных значений у нас будут оценки:\n\\[\\hat{y}=\\hat{\\alpha}+\\hat{\\beta}x\\]\nОценка для интерцепта (\\(\\alpha\\)) - значение \\(y\\) когда \\(x = 0\\). В геометрическом смысле это точка пересечения регрессионной прямой с осью \\(OY\\). Иногда интерцепт может иметь смысл и подлежит интерпретации, но часто он может принимать несуществующие значения, выходящие за рамки возможных значений переменных, не описывается и не интерпретируется (допустим, мы пытаемся выявить зависимость веса от роста, получается, что интерцепт нам покажет, чем равен вес, когда рост равен нулю, что не имеет смысла).\nВ нашем примере интерцепт - это среднее значение продаж при нулевых затратах на рекламу.\nДалее следуют коэффициенты для предикторов. Для каждого предиктора коэффициент обозначает ожидаемое изменение в зависимой переменной при изменении предиктора на одну единицу.\nГеометрический смысл beta-beкоэффициента: это угол наклона регрессионной прямой:\n\nЧтобы найти коэффициенты \\(\\alpha\\) и \\(\\beta\\) нам нужно понять, как они вычисляются. Начнем с коэффициентов \\(\\beta\\):\n\\[\\hat{\\beta}=\\frac{cov(x,y)}{var(x)}\\]\nЧтобы узнать коэффициент \\(\\beta\\) для рекламы на телевидении, нам необходимо найти ковариацию между рекламой и продажами и дисперсию затрат на рекламу:\n\ncov_x_y&lt;-cov(Advertising$TV, Advertising$Sales)\nvar_x&lt;-var(Advertising$TV)\nbeta_x&lt;-cov_x_y/var_x\nbeta_x\n\n[1] 0.04753664\n\n\nКоэффициент показывает, на сколько увеличатся продажи, при увеличении затрат на рекламу на единицу.\nПосчитав коэффициент \\(\\beta\\), мы можем перейти к \\(\\alpha\\):\n\\[\\hat{\\alpha}=\\bar{y} - \\hat{\\beta}\\bar{x}\\]\nДля того, чтобы вычислить его вручную, нам необходимо знать средние значения \\(\\bar{y}\\) и \\(\\bar{x}\\).\n\ny_bar&lt;-mean(Advertising$Sales)\nx_bar&lt;-mean(Advertising$TV)\nintercept&lt;-y_bar-beta_x*x_bar\nintercept\n\n[1] 7.032594\n\n\nВсе сходится!\nТеперь мы можем посчитать предсказанные значения по продажам на основе нашей модели:\n\nyhat &lt;- intercept + beta_x * Advertising$TV\nhead(yhat) \n\n[1] 17.970775  9.147974  7.850224 14.234395 15.627218  7.446162\n\n\nКогда мы проводим регрессионный анализ, один из главных вопросов, которые мы себе задаем - можно ли на основе знаний о переменной \\(x\\) понять, как ведет себя \\(y\\). Говоря формальным языком, мы исследуем эту взаимосвязь, оценивая остаточные значения, ассоциированные с коэффициентами \\(\\alpha\\) и \\(\\beta\\), на основе тестирования гипотез о том, отличаются ли данные коэффициенты от нуля.\nНапример:\n\n\\(H_0: \\beta=0\\)\n\\(H_1: \\beta \\neq 0\\)\n\nИными словами, если коэффициент \\(\\beta\\) равен нулю, то переменная \\(x\\) никак не объясняет \\(y\\), так как \\(0 \\times x=0\\).\nПоскольку мы основываемся на допущении о том, что остатки имеют нормальное распределение, это нам позволяет рассчитать t-статистику для коэффициентов и проверить их статистическую значимость.\nХотя R все делает автоматически, давайте разберемся, как это происходит.\nПосчитаем разницу между реальными и предсказанными значениями по продажам:\n\nres &lt;- Advertising$Sales - yhat\nhead(res)\n\n[1]  4.1292255  1.2520260  1.4497762  4.2656054 -2.7272181 -0.2461623\n\n\nДалее мы должны посчитать разброс наблюдений вокруг регрессионной прямой, которую мы только что воспроизвели, а также стандартную ошибку остатков, которая используется для оценки ошибок коэффициентов и их статистической значимости.\nЧтобы найти стандартную ошибку остатков, нам требуется:\n\nнайти сумму квадратов отклонений остатков (\\(RSS\\))\nнайти количество степеней свободы (\\(df=N-2\\))\n\n\nres.sqr &lt;- res^2\nRSS &lt;- sum(res.sqr, na.rm=T)\ndf &lt;- length(Advertising$Sales) - 2\ndf\n\n[1] 198\n\nRSE &lt;- sqrt(RSS / df)\nRSE\n\n[1] 3.258656\n\n\nСобственно говоря, это мы и видим в выводе: Residual standard error: 3.259 on 198 degrees of freedom.\nЗная стандартную ошибку остатков, мы можем вычислить стандартные ошибки для наших коэффициентов. Для этого, мы должны сначала вычислить сумму квадратов отклонений по независимой переменной (затраты на рекламу ТВ):\n\nTSSx &lt;- sum((Advertising$TV - x_bar)^2)\nTSSx\n\n[1] 1466819\n\n\nЧтобы найти стандартную ошибку коэффициента \\(\\beta\\), нужно стандартную ошибку остатков разделить на квадратный корень из суммы квадратов отклонений по переменной \\(x\\):\n\\[SE_{\\beta}=\\frac{RSE}{\\sqrt{TSS_x}}\\]\n\nSEB &lt;- RSE / sqrt(TSSx)\nSEB\n\n[1] 0.002690607\n\n\nДля интерцепта алгоритм несколько отличается:\n\\[SE_{\\alpha}=RSE*\\sqrt{\\frac{1}{N}+\\frac{\\bar{x^2}}{TSS_x}}\\]\n\nSEA &lt;- RSE * sqrt((1 / 200)+(x_bar^2 / TSSx))\nSEA\n\n[1] 0.4578429\n\n\nЗная стандартные ошибки, мы можем теперь посчитать соответствующие t-статистики, чтобы оценить, отличаются ли наши коэффициенты от нуля. Для этого нужно значения коэффициентов разделить на их стандартные ошибки.\nДля коэффициента по переменной телевизионной рекламы:\n\nt.B &lt;- beta_x / SEB\nt.B\n\n[1] 17.66763\n\n\nДля интерцепта:\n\nt.A &lt;- intercept / SEA\nt.A\n\n[1] 15.36028\n\n\nЭти значения измеряются в стандартных отклонениях и показывают, насколько далеко наши коэффициенты находятся от нуля. Значения 17,6 и 15,4 очень большие, следовательно, наши коэффициенты статистически значимы, что и подтверждают соответствующие p-значения из вывода: 15.36   &lt;2e-16 *** и 17.67   &lt;2e-16 ***.\n\n\n12.1.3.2 Показатели качества модели\nКакие еще важные показатели мы должны принять во внимание, когда мы анализируем результаты регрессионного анализа?\nОбратимся к оставшейся части вывода.\nОсновной мерой, показывающей, насколько хорошо регрессионная модель объясняет данные, является коэффициент детерминации - \\(R^2\\). Для того, чтобы найти \\(R^2\\), нужны следующие промежуточные вычисления о некоторых компонентах дисперсии зависимой переменной:\n\nсумме квадратов остатков (\\(RSS\\))\nобщей сумме квадратов отклонений от среднего (\\(TSS\\))\nсумме квадратов отклонений, объясненной моделью (\\(ESS\\))\n\n\nTSS &lt;- sum((Advertising$Sales - y_bar)^2)\nTSS\n\n[1] 5417.149\n\nESS &lt;- TSS - RSS\nESS\n\n[1] 3314.618\n\nr.sqr &lt;- ESS / TSS\nr.sqr\n\n[1] 0.6118751\n\n\nТаким образом, 61,2% дисперсии зависимой переменной объясняется регрессионной моделью.\nНесмотря на то, что показатель \\(R^2\\) является довольно информативным, у него есть один существенный недостаток: он имеет свойство неоправданно возрастать, при включении дополнительных переменных в анализ, даже если они не оказывают существенного влияния на зависимую переменную. Иными словами, чем более комплексной будет модель, тем выше будет \\(R^2\\), что не очень хорошо.\nПоэтому вместо обычного \\(R^2\\) в качестве более точной оценки качества модели используется скорректированный показатель - adjusted\\(R^2\\). Проблему множественных предикторов этот показатель решает, путем внесения «наказаний» (пенальти) за включение в модель дополнительных переменных. Чтобы найти скорректированный \\(R^2\\) используется формула:\n\\[1-\\frac{(1-R^{2})(n-1)}{n-k-1},\\]\nгде \\(k\\) - количество предикторов в модели, не считая интерцепта (A).\n\nr.sqr.adj&lt;-1-(((1 - r.sqr) * (200 - 1)) / (200 - 1 - 1))\nr.sqr.adj\n\n[1] 0.6099148\n\n\nПоскольку у в модели один предиктор, значение уменьшилось незначительно.\nУ нас остался нерассмотренным только один показатель из вывода - F-статистика. F-критерий является «глобальным» тестом, показывающим, насколько лучше наша модель базовой модели - такой, в которую включен только один интерцепт.\nЕще одна интерпретация: этот тест показывает, что в нашей модели есть хотя бы один значимый предиктор.\nВ нашем выводе F-statistic: 312.1 on 1 and 198 DF,  p-value: &lt; 2.2e-16, что указывает на то, что модель с предиктором существенно лучше базовой модели объясняет зависимую переменную.\n\n\n12.1.3.3 Сравнение нескольких моделей\nДопустим, мы хотим создать несколько двумерных моделей и сравнить их. Это возможно с помощью функции mtable() из пакета memisc (Management of Survey Data and Presentation of Analysis Results - управление данными исследований и презентация результатов анализа). Для демонстрации создадим три модели, иллюстрирующие взаимосвязь между продажами и каждым типом рекламы.\nПервая модель у нас уже есть, создадим две других:\n\nmod_2 = lm(Sales ~ Radio, data = Advertising)\nmod_3 = lm(Sales ~ Newspaper, data = Advertising)\n\nБлагодаря функции ‘mtable()’ мы можем создать таблицу, в которой сведем всю важную информацию по всем трем моделям:\n\nlibrary(memisc)\nmtable&lt;-mtable(mod_1, mod_2, mod_3)\nmtable\n\n\nCalls:\nmod_1: lm(formula = Sales ~ TV, data = Advertising)\nmod_2: lm(formula = Sales ~ Radio, data = Advertising)\nmod_3: lm(formula = Sales ~ Newspaper, data = Advertising)\n\n===================================================\n                 mod_1       mod_2       mod_3     \n---------------------------------------------------\n  (Intercept)    7.033***    9.312***   12.351***  \n                (0.458)     (0.563)     (0.621)    \n  TV             0.048***                          \n                (0.003)                            \n  Radio                      0.202***              \n                            (0.020)                \n  Newspaper                              0.055**   \n                                        (0.017)    \n---------------------------------------------------\n  R-squared      0.612       0.332       0.052     \n  N            200         200         200         \n===================================================\n  Significance: *** = p &lt; 0.001; ** = p &lt; 0.01;   \n                * = p &lt; 0.05  \n\n\nВидим, что хотя во всех моделях интерцепты и коэффициенты предикторов являются значимыми, показатель \\(R^2\\) максимально высок в модели, где в качестве объясняющей переменной используется показатель затрат на рекламу на телевидении.\n\n\n\n12.1.4 Множественная регрессия\nРассмотрим случай, когда количество предикторов больше одного, то есть наша модель является моделью уже не простой, а множественной регрессии.\nФормула для нескольких предикторов приобретает вид:\n\\[\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1}x_1 + \\hat{\\beta_2}x_2 ...+... \\hat{\\beta_n}x_n\\]\nСинтаксис в R аналогичен тому, что мы использовали для двумерной регрессии. Создадим модель, в которую включим сразу все независимые переменные.\n\nmod_4 = lm(Sales ~ TV + Radio + Newspaper, data = Advertising)\n#mod_4 = lm(Sales ~ ., data = Advertising) можно использовать и такой синтаксис\n\nКроме функции summary() красивую таблицу с результатами можно создать с помощью функции tab_model() из библиотеки sjPlot\n\nsjPlot::tab_model(mod_4)\n\n\n\n\n \nSales\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n2.94\n2.32 – 3.55\n&lt;0.001\n\n\nTV\n0.05\n0.04 – 0.05\n&lt;0.001\n\n\nRadio\n0.19\n0.17 – 0.21\n&lt;0.001\n\n\nNewspaper\n-0.00\n-0.01 – 0.01\n0.860\n\n\nObservations\n200\n\n\nR2 / R2 adjusted\n0.897 / 0.896\n\n\n\n\n\n\n\nРезультаты показывают, что значимыми являются только коэффициенты для радио- и телерекламы, тогда как реклама в газетах не является значимым фактором, определяющим продажи.\nСкорректированный коэффициент детерминации (Adjusted R-squared), показывает, что эта модель гораздо лучше, чем любая модель с одним предиктором, и объясняет 89,6% дисперсии.\nДополнительно, в целях сравнения, давайте создадим более простую модель, без газет.\n\nmod_5 = lm(Sales ~ TV + Radio, data = Advertising)\nsjPlot::tab_model(mod_5)\n\n\n\n\n \nSales\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n2.92\n2.34 – 3.50\n&lt;0.001\n\n\nTV\n0.05\n0.04 – 0.05\n&lt;0.001\n\n\nRadio\n0.19\n0.17 – 0.20\n&lt;0.001\n\n\nObservations\n200\n\n\nR2 / R2 adjusted\n0.897 / 0.896\n\n\n\n\n\n\n\nКак видим, коэффициент детерминации не изменился (что неудивительно, ведь у удаленной переменной коэффициент регрессии равнялся нулю).\n\n12.1.4.1 Сравнение моделей с помощью дисперсионного анализа\nЧтобы сравнить, какая модель работает лучше, можно применить функцию anova(), запускающую дисперсионный анализ. В нашем случае, мы будем сравнивать модель со всеми предикторами mod_1 с сокращенной моделью mod_0. Наша задача будет заключаться в том, чтобы понять, какую роль играет переменная газетной рекламы в аддитивной модели.\n\nanova(mod_4, mod_5)\n\nAnalysis of Variance Table\n\nModel 1: Sales ~ TV + Radio + Newspaper\nModel 2: Sales ~ TV + Radio\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1    196 556.83                           \n2    197 556.91 -1 -0.088717 0.0312 0.8599\n\n\nМы видим, что разница между моделями в одну степень свободы (1 параметр - как раз наша переменная о рекламе в газетах).\n\nЧисло степеней свободы (df) − важный показатель регрессионного анализа, используемый в формулах метрик, показывающих качество модели:\n\nRes.Df - число степеней свободы, рассчитываемых для остатков (разности между предсказанными и реальными значениями).\nRes.Df - количество наблюдений - количество оцениваемых параметров.\nModel 1: Sales ~ TV + Radio\ndf= 197= 200-3 (2 предиктора + константа)\nModel 2: Sales ~ TV + Radio + Newspaper\ndf= 196= 200-4 (2 предиктора + константа)\n\n\nРезультаты дисперсионного анализа показывают, что качество модели не поменялось, и значит мы можем использовать более лаконичную (сокращенную) модель.\n\n\n12.1.4.2 Предсказание значений зависимой переменной для новых данных\nОбычно у регрессионного анализа две основные задачи - объяснение взаимосвязи между переменными и предсказание новых (неизвестных) значений зависимой переменной на основе модели. Для осуществления прогноза чаще всего используется функция predict(), обладающая большой гибкостью (может применяться с различными методами моделирования и типами данных).\nЕсли эту функцию использовать к модели, созданной на основе функции lm(), то она будет рассчитывать предсказанные значения для каждого наблюдения.\nДавайте посмотрим первые десять.\n\nhead(predict(mod_5), n = 10)\n\n        1         2         3         4         5         6         7         8 \n20.555465 12.345362 12.337018 17.617116 13.223908 12.512084 11.718212 12.105516 \n        9        10 \n 3.709379 12.551697 \n\n\nОтметим, что эффект функции predict()будет зависеть от того, какие данные даются на входе. Наша модель относится к классуlm, поэтому predict() запускает функциюpredict.lm() Если нам нужно что-то другое, можно посмотреть подробности с помощью ?predict.lm().\nМы также можем сгенерировать новые данные, и попробовать посчитать зависимую переменную на них.\nДавайте создадим новый набор с идентичными именами переменных.\n\nnew_obs = data.frame(TV = 150, Radio = 40, Newspaper = 1)\n\nТеперь мы можем использовать predict(), чтобы посчитать оценки и доверительные интервалы для новых данных.\nЕсли указать только модель и источник данный, R выдаст точечную оценку, то есть “предсказанное значение” \\(\\hat{y}\\).\n\npredict(mod_5, newdata = new_obs)\n\n       1 \n17.30409 \n\n\nЕсли указать дополнительно аргументinterval со значением \"confidence\", R покажет также 95% доверительные интервалы для среднего значения по данному наблюдению.\n\npredict(mod_1, newdata = new_obs, interval = \"confidence\")\n\n       fit      lwr      upr\n1 14.16309 13.70842 14.61776\n\n\nКроме того, мы можем изменить уровень и выбрать не доверительные интервалы, а предсказательные интервалы (доверительные интервалы прогноза). В чем отличие?\nПредсказательные интервалы показывают, в каком диапазоне значений будет находиться будущее наблюдение, тогда как доверительные интервалы показывают вероятный диапазон, в котором будет находится какой-либо статистический параметр, например, среднее в генеральной совокупности.\nПоскольку предсказательные интервалы рассчитываются в ситуации большей неопределенности, то они обычно шире, чем доверительные интервалы.\n\npredict(mod_1, newdata = new_obs, interval = \"prediction\", level = 0.95)\n\n       fit      lwr      upr\n1 14.16309 7.720898 20.60528\n\n\n\n\n12.1.4.3 Диагностика модели и оценка влияния наблюдений на результаты\nВ R доступны несколько функций, позволяющих оценить, насколько полученная модель хорошо воспроизводит исходные данные, и как различные наблюдения вносят вклад в предсказательные способности этой модели:\n\nresid() выдает остаток (разность между предсказанным и реальным значением)\nhatvalues() показывает leverage - отклонение в значениях по независимым переменным по каждому наблюдению. Данный показатель важен для понимания, как экстремальные значения по независимым переменным могут повлиять на результаты анализа.\n\n\nЧто такое hat - значения? hat - по-английски «шляпа», а также диакритический знак «циркумфлекс» (\\(\\hat{ }\\)), с помощью которого обозначаются значения зависимой переменной, предсказанные с помощью регрессионной модели.\nЭти предсказанные значения обозначаются как \\(\\hat{y}\\) и рассчитываются по формуле:\n\\[\\hat{y}=Xb\\]\nДля коэффициентов линейной регрессии используется следующая формула:\n\\[b = (X^{'}X)^{-1}X^{'}y\\]\nСледовательно, мы можем переписать уравнение для предсказанных значений как:\n\\[\\hat{y}=X(X^{'}X)^{-1}X^{'}y\\]\nТаким образом, предсказанные значения могут быть получены путем умножения \\(n \\times 1\\) вектора \\(y\\), содержащего наблюдаемые значения на \\(n \\times n\\) матрицы \\(H\\):\n\\[H=X(X^{'}X)^{-1}X^{'}\\]\nИли, более лаконично:\n\\[\\hat{y}=Hy\\]\nМатрица \\(H\\) часто называется hat-matrix - «матрица в шляпе», а ее диагональные значения как раз и являются значениями левериджа.\n\n\nrstudent() стьюдентизированные остатки по каждому наблюдению (остаток в регрессионной модели деленный на ее скорректированную стандартную ошибку)\ncooks.distance() рассчитывает важность каждого наблюдения\n\n\nddf &lt;- data.frame(residuals=residuals(mod_5), rstandard=rstandard(mod_5), rstudent=rstudent(mod_5), leverage=hatvalues(mod_5), cookd=cooks.distance(mod_5))\n\nКак мы можем это использовать?\nНапример, мы можем отобрать наблюдения, чьи стандартизированные остатки отклоняются более, чем на 2 стандартных отклонения в обе стороны:\n\nlibrary(dplyr)\nfilter(ddf, abs(rstandard) &gt; 2 | abs(rstudent) &gt; 2)\n\n    residuals rstandard  rstudent   leverage      cookd\n6   -5.312084 -3.215600 -3.295069 0.03465182 0.12372141\n26  -3.608021 -2.172231 -2.193135 0.02410291 0.03884690\n36  -4.192801 -2.530272 -2.565881 0.02870293 0.06306486\n79  -3.489203 -2.096437 -2.114833 0.02013318 0.03010147\n127 -3.990963 -2.403633 -2.433473 0.02479191 0.04895846\n131 -8.797700 -5.303997 -5.714235 0.02678270 0.25806539\n179 -4.213844 -2.541689 -2.577850 0.02772290 0.06140056\n\n\n\n Задание: проанализируйте в таблице исходных данных наблюдения с указанными номерами. Какие выводы можно сделать?\n\nВторой важный момент: анализ показателей leverage и Cook's distance.\nЗамечательная вещь по поводу левериджа заключается в том, что его значения помогают выявить экстремальные значения \\(x\\), которые могут влиять на результаты регрессионного анализа. Каким образом? Мы должны понять, какое значение левериджа нужно признать большим, то есть соответствующим значениям \\(x\\), расположенным максимально далеко от средних значений по всем другим наблюдениям. Общим является правило, согласно которому, любое наблюдение, чье значение левериджа в три раза превышает среднее значение, является нетипичным / странным / достойным внимания:\n\\[\\bar{h}=\\frac{\\sum_{i=1}^{n}h_{ii}}{n}=\\frac{p}{n}\\]\nИными словами, если:\n\\[h_{ii} &gt;3\\left( \\dfrac{p}{n}\\right),\\]\nто мы должны обратить внимание на это наблюдение. Сумма всех значений левериджа равняется количеству параметров модели: 3 - два предиктора + интерцепт (константа).\n\nhat_max = 3*3/200\nfilter(ddf, leverage&gt;hat_max)\n\n[1] residuals rstandard rstudent  leverage  cookd    \n&lt;0 строк&gt; (или 'row.names' нулевой длины)\n\n\nЧто мы видим? Мы видим, что по модели 5 у нас нет таких наблюдений, чей леверидж превышал бы максимально возможное значение.\nМы также можем отсортировать наблюдения по расстоянию Кука, чтобы понять, какие наблюдения являются наиболее влиятельными:\n\\[D_i=\\frac{(y_i-\\hat{y}_i)^2}{(k+1) \\times MSE}\\left[ \\frac{h_{ii}}{(1-h_{ii})^2}\\right],\\]\nгде \\(MSE\\) - среднеквадратическая ошибка регрессии, а \\(h_{ii}\\) - значения левериджа.\n\narrange(ddf, desc(cookd))[1:6,]\n\n    residuals rstandard  rstudent   leverage      cookd\n131 -8.797700 -5.303997 -5.714235 0.02678270 0.25806539\n6   -5.312084 -3.215600 -3.295069 0.03465182 0.12372141\n36  -4.192801 -2.530272 -2.565881 0.02870293 0.06306486\n179 -4.213844 -2.541689 -2.577850 0.02772290 0.06140056\n127 -3.990963 -2.403633 -2.433473 0.02479191 0.04895846\n26  -3.608021 -2.172231 -2.193135 0.02410291 0.03884690\n\n\nРекомендуется исключать из анализа наблюдения, расстояние Кука для которых превышает 1. В нашем анализе таких нет, но вот наблюдения 131 и 6 являются все-таки подозрительными, как имеющие наиболее расстояние Кука и самые большие остатки.\nАналогичную информацию можно получить с помощью специальных графиков:\n\npar(mfrow = c(2, 2))\nplot(mod_5)\n\n\n\n\n\n\n\n\nЧто показывают графики?\n1. Residuals vs Fitted (Остатки vs предсказанные значения)\nЭтот график показывает, есть ли в остатках регресии какие-либо нелинейные паттерны. Такое может случиться, если между предикторными переменными и зависимой переменной имеются нелинейные взаимосвязи, соответственно если эта нелинейность возникает на графике, значит модель плохо воспроизводит эти отношения. Если мы видим, что остатки равномерно распределены вокруг линии предсказанных значений без каких-либо серьезных колебаний, это хороший знак, значит у нас в модели таких нелинейных взаимосвязей нет. На нашем графике есть еле заметный «прогиб», но четким паттерном его назвать вряд ли возможно.\n2. Normal Q-Q residuals\nДанный график показывает, что остатки нормально распределены (то есть маленьких остатков много и их среднее значение приближается к нулю, в больших остатков мало). У нас с нормальностью остатков практически все в порядке, если не считать постоянно выбивающееся наблюдение 131.\n3. Scale-Location\nДанный график позволяет протестировать допущение о гомогенности дисперсии остатков (гомоскедастичности). Если мы видим, что остатки распределены вдоль линии равномерно, и их форма не напоминает «фен», то все хорошо.\n4. Residuals vs Leverage\nНу и, наконец, последний график визуализирует самые влиятельные наблюдения - одновременно через леверидж и расстояние Кука. Сомнительные наблюдения на всех графиках обозначены цифрами.\nБиблиотека olsrr (Tools for Building OLS Regression Models) также содержит несколько полезных функций, которые могут помочь в выявлении таких наблюдений.\n\nlibrary(olsrr)\nols_plot_cooksd_bar(mod_1)\n\n\n\n\n\n\n\n\n\nols_plot_cooksd_chart(mod_1)\n\n\n\n\n\n\n\n\n\nols_plot_dfbetas(mod_1)\n\n\n\n\n\n\n\n\nЧто дальше? Мы выяснили, что некоторые наблюдения являются нетипичными, что может приводить к искаженным вычислениям. Но мы можем попробовать удалить переменные, которые вызвали наибольшее количество вопросов, и сравнить результаты.\n\nAdvertising2&lt;-Advertising[-c(6,131),]\nmod_6 = lm(Sales ~ TV + Radio, data = Advertising2)\nmtable&lt;-mtable(mod_5, mod_6)\nmtable\n\n\nCalls:\nmod_5: lm(formula = Sales ~ TV + Radio, data = Advertising)\nmod_6: lm(formula = Sales ~ TV + Radio, data = Advertising2)\n\n=======================================\n                 mod_5       mod_6     \n---------------------------------------\n  (Intercept)    2.921***    3.052***  \n                (0.294)     (0.265)    \n  TV             0.046***    0.044***  \n                (0.001)     (0.001)    \n  Radio          0.188***    0.195***  \n                (0.008)     (0.007)    \n---------------------------------------\n  R-squared      0.897       0.915     \n  N            200         198         \n=======================================\n  Significance: *** = p &lt; 0.001;   \n                ** = p &lt; 0.01;   \n                * = p &lt; 0.05  \n\n\nПосле удаления экстремальных наблюдений, качество модели улучшилось (скорректированный \\(R^2=91.5%\\)), хотя общие выводы аналогичны.\n\n\n12.1.4.4 Мультиколлинеарность\nЕще один сложный термин))) Что такое мультиколлинеарность? Мультиколлинеарность случается тогда, когда один предиктор может предсказывать другой. Иными словами, мы хотели бы, чтобы предикторы хорошо предсказывали поведение зависимой переменной, но не друг друга, и если такое случается, то это и называется мультиколлинеарностью. Хотя слишком высокая мультиколлинеарность является редкостью, проверка на нее является одной из стандартных процедур регрессионного анализа. Отметим, что проблема мультиколлинеарности является важной, когда мы исследуем важность предикторов, пытаемся на основе интерпретации коэффициентов регрессии обнаружить значимые закономерности (например, доказать, что повышение уровня образования может привести к значительному увеличению доходов или что по мере развития ассоциаций между гражданами увеличивается уровень институционального доверия). Если же первостепенной задачей моделирования является предсказание (как бывает во многих задачах машинного обучения), то проблема мультиколлинеарности не является релевантной, и ее можно проигнорировать.\nКак мы можем проверить, если в нашей модели чрезмерная мультиколлинеарность?\nСамый простой способ - посмотреть на коэффициенты корреляции между предикторами:\n\nAdvertising %&gt;%\n  dplyr::select(TV, Radio, Newspaper) %&gt;%\n  cor()\n\n                  TV      Radio  Newspaper\nTV        1.00000000 0.05480866 0.05664787\nRadio     0.05480866 1.00000000 0.35410375\nNewspaper 0.05664787 0.35410375 1.00000000\n\n\nНаши независимые переменные связаны друг с другом довольно слабо. Специальной мерой, позволяющей проверить мультиколлинеарность, является \\(VIF\\)- variance inflation factor, показывающая увеличение в дисперсии коэффициентов после включения дополнительной переменной:\n\ncar::vif(mod_6)\n\n      TV    Radio \n1.006511 1.006511 \n\n\nVIF &lt; 3 обозначает слабую корреляцию между переменными (идеальные условия). Чаще всего в литературе приводится пороговое значение \\(VIF=5\\), и только переменные \\(VIF&lt;5\\) должны быть включены в модель.\nУ нас в модели с мультиколлинеарностью все в порядке.\n\n\n\n12.1.5 Линейная регрессия с категориальными предикторами\nНапомним, что категориальные переменные (также известные, как качественные, или факторные переменные) - это такие переменные, которые позволяют разделить наблюдения на группы. Их особенностями является ограниченное количество значений (уровней). Типичными являются примеры с полом (два уровня - мужчины и женщины) или национальностью, социальным статусом или уровнем образования (например, лица с общим средним, средним профессиональным и высшим образованием).\nОбычно регрессионный анализ проводится с количественными переменными, и когда исследователь желает включить в модель категориальную переменную, необходимы некоторые шаги, чтобы сделать результаты более интерпретируемыми.\nВ частности, категориальные переменные перекодируются в набор так называемых «dummy» (фиктивных) переменных, в результате создается матрица контрастов. Современные программы, в том числе и R, «умеют» это делать автоматически.\n\n Пример: воспользуемся набором данных Salaries из пакета car, в котором содержатся данные о зарплате ассистентов, ассоциированных профессоров и профессоров в одном из американских колледжей (данные - за 2008-2009 учебный год). Данные были собраны администрацией для того, чтобы отслеживать различия между зарплатой, получаемой преподавателями мужчинами и женщинами,\n\nЗагрузим данные:\n\nlibrary(car)\ndata(\"Salaries\")\nhead(Salaries, 3)\n\n      rank discipline yrs.since.phd yrs.service  sex salary\n1     Prof          B            19          18 Male 139750\n2     Prof          B            20          16 Male 173200\n3 AsstProf          B             4           3 Male  79750\n\n\n\n12.1.5.1 Категориальные переменные с двумя уровнями\nВспомним, что в регрессионном уравнении для того, чтобы предсказать переменную \\(y\\) на основе независимой переменной \\(x\\), нужно суммировать все основные компоненты:\n\\[y = b_0 + b_1*x\\]\nПри этом:\n\n\\(b_0\\) и \\(b_1\\) являются регрессионными коэффициентами, представляющими константу (интерцепт) и угол наклона регрессионной прямой (slope).\n\nДопустим, мы хотим проанализировать различия в заработной плате у мужчин и женщин.\nНа основе переменной пола, мы можем создать новую фиктивную переменную, которая будет принимать значения:\n\n1 если преподаватель мужчина\n0 если преподаватель женщина\n\nи использовать эту переменную в регрессионном уравнении. При этом интерпретация коэффициентов и самого уравнения будет следующей:\n\n\\(b_0\\) средняя зарплата у женщин,\n\\(b_0 + b_1\\) средняя зарплата у мужчин,\n\\(b_1\\) различия в среднем между зарплатой мужчин и женщин.\n\nСоздадим модель:\n\nmod_7  &lt;- lm(salary ~ sex, data = Salaries)\nsummary(mod_7)$coef\n\n             Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept) 101002.41   4809.386 21.001103 2.683482e-66\nsexMale      14088.01   5064.579  2.781674 5.667107e-03\n\n\nИсходя из выведенной информации, средняя зарплата у преподавателей женщин - 101002 долларов (за 9 месяцев), тогда как у мужчин 101002 + 14088 = 115090. Полученное p-значение для фиктивной переменной sexMale очень значимое, что указывает на то, что имеются статистические обоснования наличия различий в зарплате по полу.\nФункция contrasts()позволяет посмотреть код, который использовался для создания фиктивных переменных:\n\ncontrasts(Salaries$sex)\n\n       Male\nFemale    0\nMale      1\n\n\nПри такой кодировке женщины являются референтной группой, с которой сравниваются мужчины, и в целом, любая подобная кодировка является условной, ее результаты будут влиять только на интерпретацию коэффициентов регрессии.\nЕсли нас такая кодировка не устраивает, мы можем использовать функцию relevel() для смены уровней:\n\nSalaries &lt;- Salaries %&gt;%\n  mutate(sex = relevel(sex, ref = \"Male\"))\n\nПосле перекодировки результаты регрессионного анализа будут следующими:\n\nmod_8 &lt;- lm(salary ~ sex, data = Salaries)\nsummary(mod_7)$coef\n\n             Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept) 101002.41   4809.386 21.001103 2.683482e-66\nsexMale      14088.01   5064.579  2.781674 5.667107e-03\n\n\nПоскольку мы теперь сравниваем зарплату женщин с зарплатой мужчин, коэффициент переменной sexFemale негативный, что означает более низкий уровень зарплат у женщин, по сравнению с мужчинами.\nКоэффициент \\(b_0\\) равено 115090 (средняя зарплата у мужчин), тогда как коэффициент \\(b_1\\) - -14088, показывает, на сколько, в среднем, ниже зарплата у женщин. Соответственно, 115090 - 14088 = 101002 - средняя зарплата женщин.\n\n\n12.1.5.2 Категориальная переменная с более чем двумя уровнями\nЧто делать, если в качественной переменной, которую мы хотим использовать, более двух уровней? Наиболее типичным является подход, когда такая категориальная переменная трансформируется в n-1 бинарных переменных, каждая из которых имеет по два уровня. И эти n-1 новых переменных содержат ту же информацию, что исходная переменная. В результате такой кодировки создается таблица контрастов.\nНапример, в нашем наборе есть переменная rank, которая имеет три уровня: AsstProf, AssocProf и Prof. Мы можем создать две фиктивных переменных - AssocProf и Prof:\n\nесли rank = AssocProf, тогда в новом столбце AssocProf преподавателями, являющими ассоциированными профессорами, будет присвоено значение 1, а профессорам - 0.\nесли rank = Prof, тогда в новом столбце Prof все профессора получат значение 1, а ассоциированные профессора - 0.\n\nчто же с ассистентами? В обоих новых столбцах они получат значение 0.\n\nТакого рода кодировка в R осуществляется автоматически. С помощью функции model.matrix() мы можем посмотреть, как такая матрица контрастов может выглядеть:\n\nres &lt;- model.matrix(~rank, data = Salaries)\nhead(res[, -1])\n\n  rankAssocProf rankProf\n1             0        1\n2             0        1\n3             0        0\n4             0        1\n5             0        1\n6             1        0\n\n\nВ практике регрессионного анализа есть различные способы кодирования категориальных переменных (создания контрастов). По умолчанию в R первый уровень используется в качестве референтного, а остальные интерпретируются уже по отношению к этому уровню.\n\nПример, который мы только что рассмотрели, показывает, что дисперсионный анализ - ANOVA (analyse of variance) является специальным случаем линейной модели, в которой предикторами являются категориальные переменные. И поскольку R это тоже «понимает», мы можем извлечь из модели результаты дисперсионного анализа (предпочтительнее использовать функцию Anova() из пакета car (car означает Companion to Applied Regression - компаньон для прикладных задач регрессионного анализа).\n\nСоздадим модель, в которой мы будем предсказывать зарплату от всех других переменных в наборе (знак плюс означает, что мы будем рассматривать только главные эффекты, без интеракций):\n\nmod_9&lt;- lm(salary ~ yrs.service + rank + discipline + sex,\n             data = Salaries)\nAnova(mod_9)\n\nAnova Table (Type II tests)\n\nResponse: salary\n                Sum Sq  Df  F value    Pr(&gt;F)    \nyrs.service 3.2448e+08   1   0.6324    0.4270    \nrank        1.0288e+11   2 100.2572 &lt; 2.2e-16 ***\ndiscipline  1.7373e+10   1  33.8582 1.235e-08 ***\nsex         7.7669e+08   1   1.5137    0.2193    \nResiduals   2.0062e+11 391                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nПосле того, как мы приняли во внимание другие переменные (стаж - yrs.service, должность - rank, область знаний - discipline), стало понятно, что фактор пола уже не имеет значения и не вносит вклада в вариабельность заработной платы. Значимыми становятся должность и область знания.\nЧтобы вывести более подробные результаты анализа, лучше воспользоваться функцией summary():\n\nsummary(mod_9)\n\n\nCall:\nlm(formula = salary ~ yrs.service + rank + discipline + sex, \n    data = Salaries)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-64202 -14255  -1533  10571  99163 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   73122.92    3245.27  22.532  &lt; 2e-16 ***\nyrs.service     -88.78     111.64  -0.795 0.426958    \nrankAssocProf 14560.40    4098.32   3.553 0.000428 ***\nrankProf      49159.64    3834.49  12.820  &lt; 2e-16 ***\ndisciplineB   13473.38    2315.50   5.819 1.24e-08 ***\nsexFemale     -4771.25    3878.00  -1.230 0.219311    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 22650 on 391 degrees of freedom\nMultiple R-squared:  0.4478,    Adjusted R-squared:  0.4407 \nF-statistic: 63.41 on 5 and 391 DF,  p-value: &lt; 2.2e-16\n\n\nРезультаты показывают, что зарплата ассоциированного профессора в среднем на 14560.40 долларов выше, чем у ассистента, при прочих равных условиях, а у профессора - выше на 49159.64 долларов. Интересно, что зарплата значительно варьирует от специализации: на прикладных кафедрах (applied departments) наблюдается в среднем на 13473.38 долее высокая зарплата, по сравнению с теоретическими дисциплинами (theoretical departments).\n\n\n12.1.5.3 Интеракции\nИнтеракции происходят тогда, когда эффект одного из предикторов зависит от другой переменной в модели.\nЧтобы продемонстрировать эффект интеракции, рассмотрим взаимосвязь между должностью и областью знаний в примере про зарплату преподавателей:\n\\[\n\\begin{split}\ny_i &=\\beta_0 + \\beta_1*(rank) + \\beta_2*(discipline) + \\beta_3*(rank*discipline) +\\\\ & \\beta_4*(yrs.service) + \\beta_5*(sex) + \\varepsilon_i\n\\end{split}\n\\]\n\nmod_9 &lt;- lm(salary ~ yrs.service + sex + rank * discipline, data = Salaries)\nsummary(mod_9)\n\n\nCall:\nlm(formula = salary ~ yrs.service + sex + rank * discipline, \n    data = Salaries)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-64153 -14387  -1511  10675  99229 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               75387.60    4743.67  15.892  &lt; 2e-16 ***\nyrs.service                 -86.26     111.91  -0.771    0.441    \nsexFemale                 -4974.39    3897.08  -1.276    0.203    \nrankAssocProf              9603.29    6543.62   1.468    0.143    \nrankProf                  46972.83    5627.26   8.347 1.24e-15 ***\ndisciplineB                9987.33    5802.62   1.721    0.086 .  \nrankAssocProf:disciplineB  8023.36    8189.27   0.980    0.328    \nrankProf:disciplineB       3246.32    6446.03   0.504    0.615    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 22680 on 389 degrees of freedom\nMultiple R-squared:  0.4492,    Adjusted R-squared:  0.4393 \nF-statistic: 45.32 on 7 and 389 DF,  p-value: &lt; 2.2e-16\n\n\n\nОтметим, что хотя в формуле мы указали только интеракцию, в выводе содержатся также сведения и об индивидуальных эффектах. R включает эту информацию автоматически.\n\nИнтерпретируя результаты отметим, что эффект от взаимосвязи не значим, и единственным значимым предиктором в модели остается должность: значительная прибавка в зарплате отмечается только у профессоров, тогда как дисциплинарная принадлежность значима только на уровне статистической тенденции (\\(p=0,086\\)).\n\n\n12.1.5.4 Отбор переменных для модели\nПрежде чем перейти к моделированию, аналитик проводит тщательную работу по отбору переменных. Обычно, этому предшествует теоретический анализ, который позволит определить, какие показатели, важные для целевой переменной, необходимо включить в исследование, а затем - в модель.\nОднако, когда эксперимент уже проведен, наступает время проверки статистических гипотез. Очевидно, что не всегда все включаемые в модель параметры, в конце концов оказываются значимыми.\nКакие алгоритмы мы можем использовать для определения финальной, самой лучшей модели из возможных?\nОтбор переменных (variable selection) - это процесс выбора наиболее значимых переменных для включения в регрессионную модель. Методы отбора помогают улучшить производительность модели и избежать чрезмерной подгонки.\nВ рамках данного занятия мы рассмотрим следующие методы отбора:\n\nанализ всех возможных моделей / лучшей модели, определяемой на основе оценке качества модели\nпошаговые алгоритмы\n\nДля работы мы будем использовать пакет olsrr:\n\ninstall.packages(\"olsrr\")\nlibrary(olsrr)\n\n\n12.1.5.4.0.1 Анализ всех возможных моделей\nПрежде чем мы рассмотрим методы пошагового отбора, давайте вкратце рассмотрим регрессию по всем/лучшим подмножествам. Поскольку они оценивают все возможные комбинации переменных, эти методы требуют больших вычислительных затрат и могут вывести систему из строя, если использовать их с большим набором переменных.\nМетод All subset regression (все возможные варианты) представляет результаты по всем возможным комбинациям предикторов. Если у нас есть \\(k\\) потенциальных независимых переменных, не считая константы, то количество отдельных моделей, которые потребуется проанализировать, составит - \\(2^k\\). Например, если у нас 10 предикторов, то количество моделей - \\(2^10\\) - 1024, а если переменных 20 - то количество комбинаций превышает миллион.\n\nmodel &lt;- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)\nols_step_all_possible(model)\n\n   Index N      Predictors  R-Square Adj. R-Square Mallow's Cp\n3      1 1              wt 0.7528328     0.7445939   12.480939\n1      2 1            disp 0.7183433     0.7089548   18.129607\n2      3 1              hp 0.6024373     0.5891853   37.112642\n4      4 1            qsec 0.1752963     0.1478062  107.069616\n8      5 2           hp wt 0.8267855     0.8148396    2.369005\n10     6 2         wt qsec 0.8264161     0.8144448    2.429492\n6      7 2         disp wt 0.7809306     0.7658223    9.879096\n5      8 2         disp hp 0.7482402     0.7308774   15.233115\n7      9 2       disp qsec 0.7215598     0.7023571   19.602810\n9     10 2         hp qsec 0.6368769     0.6118339   33.472150\n14    11 3      hp wt qsec 0.8347678     0.8170643    3.061665\n11    12 3      disp hp wt 0.8268361     0.8082829    4.360702\n13    13 3    disp wt qsec 0.8264170     0.8078189    4.429343\n12    14 3    disp hp qsec 0.7541953     0.7278591   16.257790\n15    15 4 disp hp wt qsec 0.8351443     0.8107212    5.000000\n\n\n\n\n12.1.5.4.0.2 Подборка лучших моделей (Best Subset Regression)\nДанный метод позволяет отобрать модели, которые являются лучшими по обобщенным критериям модели, например, имеет наибольший \\(R^2\\) или меньшие \\(MSE\\) (средняя квадратичная ошибка) или \\(AIC\\) (информационный критерий Акаике).\n\nmodel &lt;- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)\nols_step_best_subset(model)\n\n   Best Subsets Regression    \n------------------------------\nModel Index    Predictors\n------------------------------\n     1         wt              \n     2         hp wt           \n     3         hp wt qsec      \n     4         disp hp wt qsec \n------------------------------\n\n                                                   Subsets Regression Summary                                                    \n---------------------------------------------------------------------------------------------------------------------------------\n                       Adj.        Pred                                                                                           \nModel    R-Square    R-Square    R-Square     C(p)        AIC        SBIC        SBC         MSEP       FPE       HSP       APC  \n---------------------------------------------------------------------------------------------------------------------------------\n  1        0.7528      0.7446      0.7087    12.4809    166.0294    74.2916    170.4266    296.9167    9.8572    0.3199    0.2801 \n  2        0.8268      0.8148      0.7811     2.3690    156.6523    66.5755    162.5153    215.5104    7.3563    0.2402    0.2091 \n  3        0.8348      0.8171       0.782     3.0617    157.1426    67.7238    164.4713    213.1929    7.4756    0.2461    0.2124 \n  4        0.8351      0.8107       0.771     5.0000    159.0696    70.0408    167.8640    220.8882    7.9497    0.2644    0.2259 \n---------------------------------------------------------------------------------------------------------------------------------\nAIC: Akaike Information Criteria \n SBIC: Sawa's Bayesian Information Criteria \n SBC: Schwarz Bayesian Criteria \n MSEP: Estimated error of prediction, assuming multivariate normality \n FPE: Final Prediction Error \n HSP: Hocking's Sp \n APC: Amemiya Prediction Criteria \n\n\n\n\n12.1.5.4.0.3 Пошаговый отбор (Stepwise Selection)\nПошаговая регрессия - это метод подбора регрессионных моделей, который предполагает итерационный отбор независимых переменных для использования в модели. Он может быть реализован с помощью прямого отбора, обратного исключения или комбинации обоих методов.\nМетод прямого отбора начинается с модели без предикторов и постепенно добавляет каждую новую переменную, проверяя ее статистическую значимость, а метод обратного исключения, напротив,ю начинается с полной модели и затем по очереди удаляет наименее статистически значимые переменные.\n\n Пример: Для иллюстрации возможностей пошагового отбора воспользуемся данными из области недвижимости (HousingData).Набор включает данные по 506 объектам недвижимости, оцененных по 14 показателям.\n\nПеременные:\n\nCRIM : уровень преступности\nZN : proportion of residential land zoned for lots over 25,000 sq.ft.\nINDUS : доля промышленных предприятий среди нежилых объектов\nCHAS : дамми переменная, показывает расположение относительно главной реки\nNOX : концентрация нитрита озота\nRM : среднее количество комнат\nAGE : доля зданий, построенных до 1940\nDIS : взвешенное расстояние до пяти значимых бостонских деловых центров\nRAD : индекс доступности хайвея\nTAX : налоги\nPTRATIO : соотношение между учителями и преподавателями (обеспеченность учителями)\nB : доля чернокожено населения\nLSTAT : доля населения с низкими доходами\nMEDV : медианная стоимость в 1000 долларов\n\nСкачать данные\nЗагрузим данные и создадим общую модель:\n\nHousingData&lt;-read.csv(\"HousingData.csv\")\nmodel &lt;- lm(MEDV ~ ., data = HousingData)\nsummary(model)\n\n\nCall:\nlm(formula = MEDV ~ ., data = HousingData)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.4234  -2.5830  -0.5079   1.6681  26.2604 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  32.680059   5.681290   5.752 1.81e-08 ***\nCRIM         -0.097594   0.032457  -3.007 0.002815 ** \nZN            0.048905   0.014398   3.397 0.000754 ***\nINDUS         0.030379   0.065933   0.461 0.645237    \nCHAS          2.769378   0.925171   2.993 0.002940 ** \nNOX         -17.969028   4.242856  -4.235 2.87e-05 ***\nRM            4.283252   0.470710   9.100  &lt; 2e-16 ***\nAGE          -0.012991   0.014459  -0.898 0.369504    \nDIS          -1.458510   0.211007  -6.912 2.03e-11 ***\nRAD           0.285866   0.069298   4.125 4.55e-05 ***\nTAX          -0.013146   0.003955  -3.324 0.000975 ***\nPTRATIO      -0.914582   0.140581  -6.506 2.44e-10 ***\nB             0.009656   0.002970   3.251 0.001251 ** \nLSTAT        -0.423661   0.055022  -7.700 1.19e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.487 on 380 degrees of freedom\n  (112 пропущенных наблюдений удалены)\nMultiple R-squared:  0.7671,    Adjusted R-squared:  0.7591 \nF-statistic: 96.29 on 13 and 380 DF,  p-value: &lt; 2.2e-16\n\n\nМетод ступенчатого включения (начинаем с нулевой модели и постепенно добавляем предикторы)\n\nols_step_forward_p(model)\n\n\n                               Stepwise Summary                               \n----------------------------------------------------------------------------\nStep    Variable        AIC         SBC         SBIC        R2       Adj. R2 \n----------------------------------------------------------------------------\n 0      Base Model    2864.955    2872.908    1744.164    0.00000    0.00000 \n 1      LSTAT         2549.957    2561.886    1429.569    0.55272    0.55158 \n 2      RM            2445.616    2461.522    1325.564    0.65852    0.65677 \n 3      PTRATIO       2391.749    2411.631    1272.076    0.70366    0.70138 \n 4      B             2377.429    2401.287    1257.753    0.71569    0.71276 \n 5      CHAS          2367.576    2395.411    1247.924    0.72411    0.72056 \n 6      DIS           2361.466    2393.277    1241.828    0.72973    0.72554 \n 7      NOX           2337.205    2372.992    1218.357    0.74716    0.74258 \n 8      ZN            2330.955    2370.719    1212.385    0.75240    0.74725 \n 9      CRIM          2328.804    2372.544    1210.391    0.75499    0.74925 \n 10     RAD           2324.806    2372.522    1206.697    0.75870    0.75240 \n 11     TAX           2313.853    2365.546    1196.546    0.76650    0.75978 \n----------------------------------------------------------------------------\n\nFinal Model Output \n------------------\n\n                         Model Summary                           \n----------------------------------------------------------------\nR                       0.875       RMSE                  4.412 \nR-Squared               0.767       MSE                  19.470 \nAdj. R-Squared          0.760       Coef. Var            20.042 \nPred R-Squared          0.743       AIC                2313.853 \nMAE                     3.067       SBC                2365.546 \n----------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                                 ANOVA                                  \n-----------------------------------------------------------------------\n                 Sum of                                                \n                Squares         DF    Mean Square       F         Sig. \n-----------------------------------------------------------------------\nRegression    25181.422         11       2289.220    113.998    0.0000 \nResidual       7671.047        382         20.081                      \nTotal         32852.468        393                                     \n-----------------------------------------------------------------------\n\n                                    Parameter Estimates                                     \n-------------------------------------------------------------------------------------------\n      model       Beta    Std. Error    Std. Beta      t        Sig       lower      upper \n-------------------------------------------------------------------------------------------\n(Intercept)     32.975         5.631                  5.856    0.000     21.904     44.046 \n      LSTAT     -0.440         0.052       -0.352    -8.532    0.000     -0.541     -0.339 \n         RM      4.167         0.455        0.318     9.149    0.000      3.271      5.063 \n    PTRATIO     -0.915         0.139       -0.217    -6.599    0.000     -1.187     -0.642 \n          B      0.009         0.003        0.093     3.201    0.001      0.004      0.015 \n       CHAS      2.788         0.920        0.077     3.031    0.003      0.980      4.596 \n        DIS     -1.421         0.197       -0.326    -7.201    0.000     -1.808     -1.033 \n        NOX    -18.468         3.895       -0.228    -4.741    0.000    -26.127    -10.809 \n         ZN      0.050         0.014        0.131     3.526    0.000      0.022      0.078 \n       CRIM     -0.098         0.032       -0.099    -3.029    0.003     -0.162     -0.034 \n        RAD      0.282         0.066        0.267     4.309    0.000      0.153      0.411 \n        TAX     -0.012         0.003       -0.228    -3.573    0.000     -0.019     -0.006 \n-------------------------------------------------------------------------------------------\n\n\nМетод пошагового исключения:\n\nols_step_backward_p(model)\n\n\n                               Stepwise Summary                               \n----------------------------------------------------------------------------\nStep    Variable        AIC         SBC         SBIC        R2       Adj. R2 \n----------------------------------------------------------------------------\n 0      Full Model    2316.815    2376.460    1199.720    0.76711    0.75915 \n 1      INDUS         2315.035    2370.704    1197.851    0.76698    0.75965 \n 2      AGE           2313.853    2365.546    1196.546    0.76650    0.75978 \n----------------------------------------------------------------------------\n\nFinal Model Output \n------------------\n\n                         Model Summary                           \n----------------------------------------------------------------\nR                       0.875       RMSE                  4.412 \nR-Squared               0.767       MSE                  19.470 \nAdj. R-Squared          0.760       Coef. Var            20.042 \nPred R-Squared          0.743       AIC                2313.853 \nMAE                     3.067       SBC                2365.546 \n----------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                                 ANOVA                                  \n-----------------------------------------------------------------------\n                 Sum of                                                \n                Squares         DF    Mean Square       F         Sig. \n-----------------------------------------------------------------------\nRegression    25181.422         11       2289.220    113.998    0.0000 \nResidual       7671.047        382         20.081                      \nTotal         32852.468        393                                     \n-----------------------------------------------------------------------\n\n                                    Parameter Estimates                                     \n-------------------------------------------------------------------------------------------\n      model       Beta    Std. Error    Std. Beta      t        Sig       lower      upper \n-------------------------------------------------------------------------------------------\n(Intercept)     32.975         5.631                  5.856    0.000     21.904     44.046 \n       CRIM     -0.098         0.032       -0.099    -3.029    0.003     -0.162     -0.034 \n         ZN      0.050         0.014        0.131     3.526    0.000      0.022      0.078 \n       CHAS      2.788         0.920        0.077     3.031    0.003      0.980      4.596 \n        NOX    -18.468         3.895       -0.228    -4.741    0.000    -26.127    -10.809 \n         RM      4.167         0.455        0.318     9.149    0.000      3.271      5.063 \n        DIS     -1.421         0.197       -0.326    -7.201    0.000     -1.808     -1.033 \n        RAD      0.282         0.066        0.267     4.309    0.000      0.153      0.411 \n        TAX     -0.012         0.003       -0.228    -3.573    0.000     -0.019     -0.006 \n    PTRATIO     -0.915         0.139       -0.217    -6.599    0.000     -1.187     -0.642 \n          B      0.009         0.003        0.093     3.201    0.001      0.004      0.015 \n      LSTAT     -0.440         0.052       -0.352    -8.532    0.000     -0.541     -0.339 \n-------------------------------------------------------------------------------------------\n\n\nПринудительное включение в модель по имени переменной:\n\nols_step_forward_p(model, include = c(\"AGE\", \"LSTAT\"))\n\n\n                               Stepwise Summary                               \n----------------------------------------------------------------------------\nStep    Variable        AIC         SBC         SBIC        R2       Adj. R2 \n----------------------------------------------------------------------------\n 0      Base Model    2549.805    2565.711    1428.520    0.55515    0.55288 \n 1      AGE           2795.421    2807.350    1673.446    0.16603    0.16390 \n 2      LSTAT         2549.805    2565.711    1428.520    0.55515    0.55288 \n 3      RM            2447.070    2466.952    1326.441    0.65899    0.65637 \n 4      PTRATIO       2393.728    2417.587    1273.685    0.70368    0.70063 \n 5      B             2379.428    2407.262    1259.450    0.71569    0.71202 \n 6      DIS           2363.230    2395.041    1243.534    0.72852    0.72431 \n 7      NOX           2346.870    2382.657    1227.647    0.74088    0.73618 \n 8      CHAS          2336.592    2376.355    1217.770    0.74883    0.74361 \n 9      ZN            2331.522    2375.262    1212.973    0.75330    0.74752 \n 10     CRIM          2329.117    2376.834    1210.769    0.75604    0.74967 \n 11     RAD           2325.744    2377.436    1207.701    0.75935    0.75242 \n----------------------------------------------------------------------------\n\nFinal Model Output \n------------------\n\n                         Model Summary                           \n----------------------------------------------------------------\nR                       0.871       RMSE                  4.480 \nR-Squared               0.759       MSE                  20.066 \nAdj. R-Squared          0.752       Coef. Var            20.346 \nPred R-Squared          0.734       AIC                2325.744 \nMAE                     3.107       SBC                2377.436 \n----------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                                 ANOVA                                  \n-----------------------------------------------------------------------\n                 Sum of                                                \n                Squares         DF    Mean Square       F         Sig. \n-----------------------------------------------------------------------\nRegression    24946.389         11       2267.854    109.576    0.0000 \nResidual       7906.079        382         20.697                      \nTotal         32852.468        393                                     \n-----------------------------------------------------------------------\n\n                                    Parameter Estimates                                     \n-------------------------------------------------------------------------------------------\n      model       Beta    Std. Error    Std. Beta      t        Sig       lower      upper \n-------------------------------------------------------------------------------------------\n(Intercept)     30.750         5.722                  5.374    0.000     19.499     42.001 \n        AGE     -0.015         0.015       -0.045    -1.015    0.311     -0.044      0.014 \n      LSTAT     -0.432         0.056       -0.346    -7.760    0.000     -0.542     -0.323 \n         RM      4.459         0.472        0.340     9.454    0.000      3.532      5.387 \n    PTRATIO     -0.982         0.139       -0.233    -7.043    0.000     -1.256     -0.708 \n          B      0.010         0.003        0.097     3.300    0.001      0.004      0.016 \n        DIS     -1.392         0.209       -0.319    -6.666    0.000     -1.802     -0.981 \n        NOX    -20.174         4.054       -0.250    -4.976    0.000    -28.144    -12.203 \n       CHAS      3.123         0.930        0.086     3.360    0.001      1.295      4.951 \n         ZN      0.037         0.014        0.097     2.623    0.009      0.009      0.065 \n       CRIM     -0.093         0.033       -0.094    -2.832    0.005     -0.158     -0.028 \n        RAD      0.101         0.044        0.096     2.290    0.023      0.014      0.188 \n-------------------------------------------------------------------------------------------\n\n\nПринудительное включение по индексу:\n\nols_step_forward_p(model, include = c(5, 7))\n\n\n                               Stepwise Summary                               \n----------------------------------------------------------------------------\nStep    Variable        AIC         SBC         SBIC        R2       Adj. R2 \n----------------------------------------------------------------------------\n 0      Base Model    2770.218    2786.123    1647.004    0.22167    0.21769 \n 1      NOX           2773.716    2785.645    1651.853    0.21073    0.20872 \n 2      AGE           2770.218    2786.123    1647.004    0.22167    0.21769 \n 3      RM            2518.979    2538.861    1397.239    0.59071    0.58757 \n 4      LSTAT         2446.035    2469.894    1324.883    0.66161    0.65813 \n 5      PTRATIO       2389.609    2417.444    1269.358    0.70825    0.70449 \n 6      DIS           2357.931    2389.742    1238.409    0.73215    0.72800 \n 7      CHAS          2345.772    2381.559    1226.591    0.74160    0.73692 \n 8      B             2336.592    2376.355    1217.770    0.74883    0.74361 \n 9      ZN            2331.522    2375.262    1212.973    0.75330    0.74752 \n 10     CRIM          2329.117    2376.834    1210.769    0.75604    0.74967 \n 11     RAD           2325.744    2377.436    1207.701    0.75935    0.75242 \n----------------------------------------------------------------------------\n\nFinal Model Output \n------------------\n\n                         Model Summary                           \n----------------------------------------------------------------\nR                       0.871       RMSE                  4.480 \nR-Squared               0.759       MSE                  20.066 \nAdj. R-Squared          0.752       Coef. Var            20.346 \nPred R-Squared          0.734       AIC                2325.744 \nMAE                     3.107       SBC                2377.436 \n----------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                                 ANOVA                                  \n-----------------------------------------------------------------------\n                 Sum of                                                \n                Squares         DF    Mean Square       F         Sig. \n-----------------------------------------------------------------------\nRegression    24946.389         11       2267.854    109.576    0.0000 \nResidual       7906.079        382         20.697                      \nTotal         32852.468        393                                     \n-----------------------------------------------------------------------\n\n                                    Parameter Estimates                                     \n-------------------------------------------------------------------------------------------\n      model       Beta    Std. Error    Std. Beta      t        Sig       lower      upper \n-------------------------------------------------------------------------------------------\n(Intercept)     30.750         5.722                  5.374    0.000     19.499     42.001 \n        NOX    -20.174         4.054       -0.250    -4.976    0.000    -28.144    -12.203 \n        AGE     -0.015         0.015       -0.045    -1.015    0.311     -0.044      0.014 \n         RM      4.459         0.472        0.340     9.454    0.000      3.532      5.387 \n      LSTAT     -0.432         0.056       -0.346    -7.760    0.000     -0.542     -0.323 \n    PTRATIO     -0.982         0.139       -0.233    -7.043    0.000     -1.256     -0.708 \n        DIS     -1.392         0.209       -0.319    -6.666    0.000     -1.802     -0.981 \n       CHAS      3.123         0.930        0.086     3.360    0.001      1.295      4.951 \n          B      0.010         0.003        0.097     3.300    0.001      0.004      0.016 \n         ZN      0.037         0.014        0.097     2.623    0.009      0.009      0.065 \n       CRIM     -0.093         0.033       -0.094    -2.832    0.005     -0.158     -0.028 \n        RAD      0.101         0.044        0.096     2.290    0.023      0.014      0.188 \n-------------------------------------------------------------------------------------------\n\n\nВыбор на основе коэффициента детерминации:\n\nols_step_forward_adj_r2(model)\n\n\n                               Stepwise Summary                               \n----------------------------------------------------------------------------\nStep    Variable        AIC         SBC         SBIC        R2       Adj. R2 \n----------------------------------------------------------------------------\n 0      Base Model    2864.955    2872.908    1744.164    0.00000    0.00000 \n 1      LSTAT         2549.957    2561.886    1429.569    0.55272    0.55158 \n 2      RM            2445.616    2461.522    1325.564    0.65852    0.65677 \n 3      PTRATIO       2391.749    2411.631    1272.076    0.70366    0.70138 \n 4      B             2377.429    2401.287    1257.753    0.71569    0.71276 \n 5      CHAS          2367.576    2395.411    1247.924    0.72411    0.72056 \n 6      DIS           2361.466    2393.277    1241.828    0.72973    0.72554 \n 7      NOX           2337.205    2372.992    1218.357    0.74716    0.74258 \n 8      ZN            2330.955    2370.719    1212.385    0.75240    0.74725 \n 9      CRIM          2328.804    2372.544    1210.391    0.75499    0.74925 \n 10     RAD           2324.806    2372.522    1206.697    0.75870    0.75240 \n 11     TAX           2313.853    2365.546    1196.546    0.76650    0.75978 \n----------------------------------------------------------------------------\n\nFinal Model Output \n------------------\n\n                         Model Summary                           \n----------------------------------------------------------------\nR                       0.875       RMSE                  4.412 \nR-Squared               0.767       MSE                  19.470 \nAdj. R-Squared          0.760       Coef. Var            20.042 \nPred R-Squared          0.743       AIC                2313.853 \nMAE                     3.067       SBC                2365.546 \n----------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                                 ANOVA                                  \n-----------------------------------------------------------------------\n                 Sum of                                                \n                Squares         DF    Mean Square       F         Sig. \n-----------------------------------------------------------------------\nRegression    25181.422         11       2289.220    113.998    0.0000 \nResidual       7671.047        382         20.081                      \nTotal         32852.468        393                                     \n-----------------------------------------------------------------------\n\n                                    Parameter Estimates                                     \n-------------------------------------------------------------------------------------------\n      model       Beta    Std. Error    Std. Beta      t        Sig       lower      upper \n-------------------------------------------------------------------------------------------\n(Intercept)     32.975         5.631                  5.856    0.000     21.904     44.046 \n      LSTAT     -0.440         0.052       -0.352    -8.532    0.000     -0.541     -0.339 \n         RM      4.167         0.455        0.318     9.149    0.000      3.271      5.063 \n    PTRATIO     -0.915         0.139       -0.217    -6.599    0.000     -1.187     -0.642 \n          B      0.009         0.003        0.093     3.201    0.001      0.004      0.015 \n       CHAS      2.788         0.920        0.077     3.031    0.003      0.980      4.596 \n        DIS     -1.421         0.197       -0.326    -7.201    0.000     -1.808     -1.033 \n        NOX    -18.468         3.895       -0.228    -4.741    0.000    -26.127    -10.809 \n         ZN      0.050         0.014        0.131     3.526    0.000      0.022      0.078 \n       CRIM     -0.098         0.032       -0.099    -3.029    0.003     -0.162     -0.034 \n        RAD      0.282         0.066        0.267     4.309    0.000      0.153      0.411 \n        TAX     -0.012         0.003       -0.228    -3.573    0.000     -0.019     -0.006 \n-------------------------------------------------------------------------------------------\n\n\nВизуализация модели:\n\nk &lt;- ols_step_forward_adj_r2(model)\nplot(k)\n\n\n\n\n\n\n\n\n\n\n12.1.5.4.0.4 Иерархический отбор\nКогда для отбора переменных используются p-значения, возможно использования иерархического отбора. Этот метод предполагает, что поиск значимых переменных ограничен следующей переменной. Если какая-то переменная не отбирается по причине не подходящего p-значения, то и ни одна последующая переменная не рассматривается для включения.\n\nols_step_forward_p(model, 0.1, hierarchical = TRUE)\n\n\n                               Stepwise Summary                               \n----------------------------------------------------------------------------\nStep    Variable        AIC         SBC         SBIC        R2       Adj. R2 \n----------------------------------------------------------------------------\n 0      Base Model    2864.955    2872.908    1744.164    0.00000    0.00000 \n 1      CRIM          2799.295    2811.224    1677.300    0.15779    0.15564 \n 2      ZN            2743.805    2759.711    1620.779    0.27213    0.26841 \n 3      INDUS         2709.933    2729.815    1585.906    0.33548    0.33037 \n 4      CHAS          2691.217    2715.075    1566.162    0.36951    0.36303 \n----------------------------------------------------------------------------\n\nFinal Model Output \n------------------\n\n                         Model Summary                           \n----------------------------------------------------------------\nR                       0.608       RMSE                  7.251 \nR-Squared               0.370       MSE                  52.571 \nAdj. R-Squared          0.363       Coef. Var            32.635 \nPred R-Squared          0.347       AIC                2691.217 \nMAE                     5.208       SBC                2715.075 \n----------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                                ANOVA                                  \n----------------------------------------------------------------------\n                 Sum of                                               \n                Squares         DF    Mean Square      F         Sig. \n----------------------------------------------------------------------\nRegression    12139.449          4       3034.862    56.996    0.0000 \nResidual      20713.019        389         53.247                     \nTotal         32852.468        393                                    \n----------------------------------------------------------------------\n\n                                  Parameter Estimates                                    \n----------------------------------------------------------------------------------------\n      model      Beta    Std. Error    Std. Beta      t        Sig      lower     upper \n----------------------------------------------------------------------------------------\n(Intercept)    26.633         0.901                 29.557    0.000    24.862    28.405 \n       CRIM    -0.220         0.044       -0.221    -5.036    0.000    -0.305    -0.134 \n         ZN     0.076         0.018        0.200     4.233    0.000     0.041     0.112 \n      INDUS    -0.436         0.067       -0.329    -6.527    0.000    -0.567    -0.305 \n       CHAS     6.697         1.461        0.185     4.583    0.000     3.824     9.570 \n----------------------------------------------------------------------------------------\n\n\nПошаговая регрессия может оказаться хорошей идеей, особенно, когда количество предикторов велико и нужно отобрать только самые значимые. Между тем, исследователи отмечают большое количество «подводных камней» и статистических проблем, которые могут возникнут в процессе применения регрессионного анализа, таких как переобученность данных, смещенные оценки, ошибки I рода (Harrell, 2015). Кроме того, в процессе применения пошаговых методов возникает опасная иллюзия итого, что компьютер автоматически отбирает правильные переменные, на самом деле это происходит без связи с теоретическими основаниями и гипотезами исследования. Более того, модель, которая была отобрана на основе какого-то критерия, на самом деле может оказаться нестабильной и малоинформативной. Во многих случаях правильным было бы опираться на теорию и предыдущие исследования, тогда как методы отбора могут рассматриваться в качестве поисковых техник.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Регрессионный анализ в R</span>"
    ]
  },
  {
    "objectID": "Regression.html#логистическая-регрессия",
    "href": "Regression.html#логистическая-регрессия",
    "title": "12  Регрессионный анализ в R",
    "section": "12.2 Логистическая регрессия",
    "text": "12.2 Логистическая регрессия\nЛогистическая регрессия применяется в том случае, если наша зависимая перменная имеет вид 0-1, то есть является дихотомической и имеет значения 1 и 0. По сути, такой регрессионный анализ решает задачу классификации, то есть определения принадлежности к одному из двух классов (“победит” или “проиграет”, примут на работу или нет и т.д.).\n\n Пример: В качестве примера, мы будем рассматривать данные о приеме в высшие учебные заведения. В частности, нас будет интересовать, как результаты выпускных экзаменов GRE (Graduate Record Exam scores) и средние оценки GPA (grade point average), а также престиж учебного заведения связаня с допуском в высшее учебное заведение. Зависимой является переменная admit/don’t admit, которая уже закодирована в формате 0-1.\n\n\ndata &lt;- read.csv(\"univ_prestige.csv\")\nhead(data)\n\n  admit gre  gpa rank\n1     0 380 3.61    3\n2     1 660 3.67    3\n3     1 800 4.00    1\n4     1 640 3.19    4\n5     0 520 2.93    4\n6     1 760 3.00    2\n\n\nМодель логистической регрессии имеет вид:\n\\[\n\\log\\left(\\frac{p(x)}{1 - p(x)}\\right) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots  + \\beta_p x_p.\n\\]\nОткуда, путем перестановки, мы можем вывести вероятность принадлежности к группе 1:\n\\[\np(x) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots  + \\beta_p x_p)}} = \\sigma(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots  + \\beta_p x_p)\n\\]\nОбычно основное уравнение представляется в виде сигмоиды (логистической функции):\n\\[\n\\sigma(x) = \\frac{e^x}{1 + e^x} = \\frac{1}{1 + e^{-x}}\n\\]\n\nПодгонка модели осуществляется путем максимизации функции правдоподобия, что практически никогда не происходит вручную, и мы предоставим возможность это сделатьR\nНачнем с того, что переведем ранг заведения в факторную переменную:\n\ndata$rank &lt;- factor(data$rank)\n\n\nmodel_glm &lt;- glm(admit ~ gre + gpa + rank, data = data, family = \"binomial\")\n\nРезультаты логистической регрессии очень похожи на то, что мы видели в линейной регрессии, только вместо lm() мы используем glm(). Другая особенность - в атрибуте family = \"binomial\", что означает, что у нас будет зависимая переменная, состоящая из двух классов. Если использовать glm() с family = \"gaussian\" то получится обычная линейная регрессия.\nДавайте посмотрим на общие результаты\n\nsummary(model_glm)\n\n\nCall:\nglm(formula = admit ~ gre + gpa + rank, family = \"binomial\", \n    data = data)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -3.989979   1.139951  -3.500 0.000465 ***\ngre          0.002264   0.001094   2.070 0.038465 *  \ngpa          0.804038   0.331819   2.423 0.015388 *  \nrank2       -0.675443   0.316490  -2.134 0.032829 *  \nrank3       -1.340204   0.345306  -3.881 0.000104 ***\nrank4       -1.551464   0.417832  -3.713 0.000205 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 499.98  on 399  degrees of freedom\nResidual deviance: 458.52  on 394  degrees of freedom\nAIC: 470.52\n\nNumber of Fisher Scoring iterations: 4\n\n\nВывод в целом напоминает то, что мы видели в линейной регрессии. Видим, что все наши зависимые переменные являются значимыми. Переменные ранга имеют отрицательные коэффициенты, так как сравниваются со значением 1 - группа учебных заведений с наиболее высокими позициями.\nОднако, стоит помнить, что коэффициенты в логистической регрессии не простые, они представляют собой логарифм шансов. Что это значит?\nДавайте посмотрим на таблицу с нашей зависимой переменной:\n\ntable(data$admit)\n\n\n  0   1 \n273 127 \n\n\nВсего допущено 127 человек из 400, то есть вероятность допуска составит: 127/400=0.3175, а отношение шансов (допуска к недопуску): 0.3175/(1-0.1375)=0.3681159. Логарифм данного выражения составит:\n\nlog(0.3175/(1-0.3175))\n\n[1] -0.7652847\n\n\nЭто именно то, что мы бы получили, если бы создали модель только с одним интерцептом:\n\nmodel_null&lt;-glm(admit ~ 1, data = data, family = \"binomial\")\nsummary(model_null)\n\n\nCall:\nglm(formula = admit ~ 1, family = \"binomial\", data = data)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -0.7653     0.1074  -7.125 1.04e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 499.98  on 399  degrees of freedom\nResidual deviance: 499.98  on 399  degrees of freedom\nAIC: 501.98\n\nNumber of Fisher Scoring iterations: 4\n\n\nВернемся к коэффициентам нашей большой модели:\n\nизменения в одну единицу по переменной gre, логарифм шансов допуска увеличится на 0.002.\nизменение на одну единицу в gpa, логарифм шансов допуска увеличится на 0.804.\n\nИндикаторные переменные для ранга имеют слегка иную интерпретацию. Например, посещая школу, входящую во вторую группу по престижности, по сравнению с группой 1 изменяет логарифм шансов на -0.675.\nВнизу таблицы с коэффициентами располагаются индексы подгонки (AIC).\nЧтобы перевести коэффициенты в обычное отношение шансов, применяется экспоненциальная функция. Можно соединить это действие с вычислением доверительных интервалов:\n\nexp(cbind(OR = coef(model_glm), confint(model_glm)))\n\n                   OR       2.5 %    97.5 %\n(Intercept) 0.0185001 0.001889165 0.1665354\ngre         1.0022670 1.000137602 1.0044457\ngpa         2.2345448 1.173858216 4.3238349\nrank2       0.5089310 0.272289674 0.9448343\nrank3       0.2617923 0.131641717 0.5115181\nrank4       0.2119375 0.090715546 0.4706961\n\n\nКак интерпретировать отношение шансов?\nДля количественных переменных меняется мало что:\n\nизменение на одну единицу gre на 0,2% увеличивает шансы быть принятыми\nизменение среднего балла на единицу увеличивает шансы на 123%\nа вот учеба в школе ранга 2 снижает шансы на 50%, ранга 3 - на 73.8%, ранга 4 - на 78,8%.\n\n\nОбщая формула перевода отношения шансов в проценты: (OR-1) * 100\n\nСледующий этап - посмотреть, как работает функция predict() вместе с glm():\n\nhead(predict(model_glm))\n\n         1          2          3          4          5          6 \n-1.5671256 -0.8848442  1.0377118 -1.5273305 -2.0081113 -0.5323458 \n\n\nПо умолчанию predict.glm() использует type = \"link\".\n\nhead(predict(model_glm, type = \"link\"))\n\n         1          2          3          4          5          6 \n-1.5671256 -0.8848442  1.0377118 -1.5273305 -2.0081113 -0.5323458 \n\n\nЭто означает, что R возвращает:\n\\[\n\\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 + \\cdots  + \\hat{\\beta}_p x_p\n\\] для каждого наблюдения.\nВажно понимать, что это не предсказанные вероятности, и для того, чтобы их получить:\n\\[\n\\hat{p}(x) = \\hat{P}(Y = 1 \\mid X = x)\n\\]\nмы должны написать type = \"response\"\n\nhead(predict(model_glm, type = \"response\"))\n\n        1         2         3         4         5         6 \n0.1726265 0.2921750 0.7384082 0.1783846 0.1183539 0.3699699 \n\n\nСоответственно, это вероятности, но не результаты классификации. Для того, чтобы их получить, мы должны сравнить вероятности с пороговым значением.\n\nmodel_glm_pred = ifelse(predict(model_glm, type = \"response\") &gt; 0.5, 1, 0)\nhead(model_glm_pred)\n\n1 2 3 4 5 6 \n0 0 1 0 0 0 \n\n\nЧто мы сделали?\n\\[\n\\hat{C}(x) =\n\\begin{cases}\n      1 & \\hat{f}(x) &gt; 0 \\\\\n      0 & \\hat{f}(x) \\leq 0\n\\end{cases}\n\\]\nгде\n\\[\n\\hat{f}(x) =\\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 + \\cdots  + \\hat{\\beta}_p x_p.\n\\]\nТот код, который мы запустили, делает следующее:\n\\[\n\\hat{C}(x) =\n\\begin{cases}\n      1 & \\hat{p}(x) &gt; 0.5 \\\\\n      0 & \\hat{p}(x) \\leq 0.5\n\\end{cases}\n\\]\nгде\n\\[\n\\hat{p}(x) = \\hat{P}(Y = 1 \\mid X = x).\n\\]\nПосчитав классификации, мы можем также посчитать метрики для ошибок.\n\ntab = table(predicted = model_glm_pred, actual = data$admit)\ntab\n\n         actual\npredicted   0   1\n        0 254  97\n        1  19  30\n\nlibrary(caret)\nconfusionMatrix = confusionMatrix(tab, positive = \"1\")\nc(confusionMatrix$overall[\"Accuracy\"], \n  confusionMatrix$byClass[\"Sensitivity\"], \n  confusionMatrix$byClass[\"Specificity\"])\n\n   Accuracy Sensitivity Specificity \n  0.7100000   0.2362205   0.9304029 \n\n\n\nМы можем также предсказать результаты допуска в вуз для новых данных.\nПопробуем их сгенерировать на основе исходных данных:\n\nnewdata1 &lt;- with(data, data.frame(gre = mean(gre), gpa = mean(gpa), rank = factor(1:4)))\n\nПредскажем результаты зачисления:\n\nnewdata1$rankP &lt;- predict(model_glm, newdata = newdata1, type = \"response\")\nnewdata1\n\n    gre    gpa rank     rankP\n1 587.7 3.3899    1 0.5166016\n2 587.7 3.3899    2 0.3522846\n3 587.7 3.3899    3 0.2186120\n4 587.7 3.3899    4 0.1846684\n\n\nВидим, что при средних оценках, учащиеся, обучавшиеся в престижных школах имеют большую вероятность поступить, чем те, кто училися не в очень престижных заведениях.\nМы также можем захотеть узнать, насколько хорошо наша модель соответствует действительности. Это может быть особенно полезно при сравнении конкурирующих моделей.\nАналогом \\(R^2\\) для логистической регрессии является \\(R^2 Макфаддена\\):\n\nwith(summary(model_glm), 1 - deviance/null.deviance)\n\n[1] 0.08292194\n\n\nЕсли не хочется вычислять вручную, можно воспользоваться готовой функцией pR2:\n\n#install.packages('pscl')\nlibrary(pscl)\n\npR2(model_glm)['McFadden']\n\nfitting null model for pseudo-r2\n\n\n  McFadden \n0.08292194 \n\n\nЕще одной популярной псевдомерой является \\(R^2 Nagelkerke\\):\n\n#install.packages('fmsb')\nlibrary(fmsb)\n\nNagelkerkeR2(model_glm)\n\n$N\n[1] 400\n\n$R2\n[1] 0.1379958",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Регрессионный анализ в R</span>"
    ]
  },
  {
    "objectID": "Regression.html#мультиномиальная-логистическая-регрессия",
    "href": "Regression.html#мультиномиальная-логистическая-регрессия",
    "title": "12  Регрессионный анализ в R",
    "section": "12.3 Мультиномиальная логистическая регрессия",
    "text": "12.3 Мультиномиальная логистическая регрессия\nЧто делать, если наша зависимая переменная имеет не две, а более категорий? Для этого случая больше подходит мультиномиальная логистическая регрессия.\n\\[\nP(Y = k \\mid { X = x}) = \\frac{e^{\\beta_{0k} + \\beta_{1k} x_1 + \\cdots +  + \\beta_{pk} x_p}}{\\sum_{g = 1}^{G} e^{\\beta_{0g} + \\beta_{1g} x_1 + \\cdots + \\beta_{pg} x_p}}\n\\]\nМы не будем погружаться в технические детали, но попробуем реализовать этот подход на практике. мы воспользуемся знакомым нам набором данных iris.\nЧтобы выполнить мультиномиальный регрессионный анализ нам потребуется функция multinom из библиотеки nnet, где используется синтаксис, похожий на lm() и glm(). Лучше добавить trace = FALSE, чтобы не выводилась информация об оптимизационных процессах во время обучения.\n\nlibrary(nnet)\nmodel_multi = multinom(Species ~ ., data = iris)\n\n# weights:  18 (10 variable)\ninitial  value 164.791843 \niter  10 value 16.177348\niter  20 value 7.111438\niter  30 value 6.182999\niter  40 value 5.984028\niter  50 value 5.961278\niter  60 value 5.954900\niter  70 value 5.951851\niter  80 value 5.950343\niter  90 value 5.949904\niter 100 value 5.949867\nfinal  value 5.949867 \nstopped after 100 iterations\n\nsummary(model_multi)\n\nCall:\nmultinom(formula = Species ~ ., data = iris)\n\nCoefficients:\n           (Intercept) Sepal.Length Sepal.Width Petal.Length Petal.Width\nversicolor    18.69037    -5.458424   -8.707401     14.24477   -3.097684\nvirginica    -23.83628    -7.923634  -15.370769     23.65978   15.135301\n\nStd. Errors:\n           (Intercept) Sepal.Length Sepal.Width Petal.Length Petal.Width\nversicolor    34.97116     89.89215    157.0415     60.19170    45.48852\nvirginica     35.76649     89.91153    157.1196     60.46753    45.93406\n\nResidual Deviance: 11.89973 \nAIC: 31.89973 \n\n\nЗаметим, что на выходе у нас коэффициенты только для двух классов, так же как и в обычной регрессии у нас есть только коэффициент для одного класса.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Регрессионный анализ в R</span>"
    ]
  },
  {
    "objectID": "Regression.html#непараметрическая-регрессия.-метод-k-ближайших-соседей",
    "href": "Regression.html#непараметрическая-регрессия.-метод-k-ближайших-соседей",
    "title": "12  Регрессионный анализ в R",
    "section": "12.4 Непараметрическая регрессия. Метод k-ближайших соседей",
    "text": "12.4 Непараметрическая регрессия. Метод k-ближайших соседей\nВсе методы, которые мы рассматривали до этого момента, являются параметрическими. Это можно представить в виде обобщающей формулы.\n\\[\nf(x) = \\mathbb{E}[Y \\mid X = x]\n\\]\nНапример, типичная форма для множественной линейной регрессии:\n\\[\nf(x) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_p x_p\n\\]\nЗадача аналитика в этом случае заключается в оценке параметров модели и предсказания на их основе.\nНепараметрические методы основываются на самих данных, а не на параметрах. В этом случае используется понятие локальности.\nРассуждения при этом примерно такие: чему будет равняться y, если x равен…?\n\\[\n\\hat{f}(x) = \\text{average}(\\{ y_i : x_i = x \\})\n\\]\nПоскольку не всегда это требование выполняется, то условия чуть-чуть меняются:\n\\[\n\\hat{f}(x) = \\text{average}( \\{ y_i : x_i \\text{ equal to (or very close to) x} \\} )\n\\]\nОдним из конкретных примером использования непараметрического подхода является метод ближайших соседей:\n\\[\n\\hat{f}_k(x) = \\frac{1}{k} \\sum_{i \\in \\mathcal{N}_k(x, \\mathcal{D})} y_i\n\\]\n\n\n12.4.1 KNN в R\nПосмотрим, как работает этот метод на данных набора HousingData:\n\nlibrary(FNN)\nlibrary(MASS)\ndata(Boston)\n\nСоздаем тренировочную и тестируемые выборки:\n\nset.seed(42)\nboston_idx = sample(1:nrow(Boston), size = 250)\ntrn_boston = Boston[boston_idx, ]\ntst_boston  = Boston[-boston_idx, ]\n\n\nX_trn_boston = trn_boston[\"lstat\"]\nX_tst_boston = tst_boston[\"lstat\"]\ny_trn_boston = trn_boston[\"medv\"]\ny_tst_boston = tst_boston[\"medv\"]\n\nСоздадим дополнительный набор для переменной lstat по которым мы будем предсказывать medv для создания графики.\n\nX_trn_boston_min = min(X_trn_boston)\nX_trn_boston_max = max(X_trn_boston)\nlstat_grid = data.frame(lstat = seq(X_trn_boston_min, X_trn_boston_max, \n                                    by = 0.01))\n\nЧтобы применить метод KNN в качестве разновидности регрессионного анализа, нам понадобится функция knn.reg() из библиотеки FNN.\nЕе общая архитектура следующая:\n\nknn.reg(train = ?, test = ?, y = ?, k = ?)\n\nДанные\n\ntrain: предикторы (тренировочные данные)\ntest: предикторы на тестовых данных, \\(x\\), по которым мы хотели бы сделать предсказания\ny: зависимая переменная (на тренировочных данных)\nk: количество “соседей”\n\nРезультат:\n\nвывод функции knn.reg() представляет собой \\(\\hat{f}_k(x)\\)\n\n\npred_001 = knn.reg(train = X_trn_boston, test = lstat_grid, y = y_trn_boston, k = 1)\npred_005 = knn.reg(train = X_trn_boston, test = lstat_grid, y = y_trn_boston, k = 5)\npred_010 = knn.reg(train = X_trn_boston, test = lstat_grid, y = y_trn_boston, k = 10)\npred_050 = knn.reg(train = X_trn_boston, test = lstat_grid, y = y_trn_boston, k = 50)\npred_100 = knn.reg(train = X_trn_boston, test = lstat_grid, y = y_trn_boston, k = 100)\npred_250 = knn.reg(train = X_trn_boston, test = lstat_grid, y = y_trn_boston, k = 250)\n\nМы сделали предсказания на основе lstat, для различных значений k. Отметим, что 250 это общее количество наблюдений в тренировочном датасете.\n\n\n\n\n\n\n\n\n\n\nОранжевые “кривые” представляют собой \\(\\hat{f}_k(x)\\) где \\(x\\) это значения, которые мы определили через lstat_grid.\n\nмы видим, что k = 1 приводит к большой переобученности, так как k = 1 это очень комплексная, вариативная модель. В свою очередь, k = 250 страдает недообученностью данных, так как k = 250 это очень простой пример с маленькой дисперсией, то есть по сути, всегда будет предсказываться одно и то же значение.\n\n\n12.4.2 Выбор параметра \\(k\\)\nДилемма: - низкое значение k = слишком сложная модель - высокое значение k = слишком жесткая модель.\nГде золотая середина?\n-мы хотим минимизировать \\(\\hat{f}_k\\):\n\\[\n\\text{EPE}\\left(Y, \\hat{f}_k(X)\\right) =\n\\mathbb{E}_{X, Y, \\mathcal{D}} \\left[  (Y - \\hat{f}_k(X))^2 \\right]\n\\]\nПроведем тестирование на ошибку RMSE:\n\nrmse = function(actual, predicted) {\n  sqrt(mean((actual - predicted) ^ 2))\n}\n\n\n# создадим вспомогательную функцию, чтобы \"вытащить\" предсказанные значения\nmake_knn_pred = function(k = 1, training, predicting) {\n  pred = FNN::knn.reg(train = training[\"lstat\"], \n                      test = predicting[\"lstat\"], \n                      y = training$medv, k = k)$pred\n  act  = predicting$medv\n  rmse(predicted = pred, actual = act)\n}\n\n\n# определяем возможные значения k\nk = c(1, 5, 10, 25, 50, 250)\n\n\n# Получаем train RMSEs\nknn_trn_rmse = sapply(k, make_knn_pred, \n                      training = trn_boston, \n                      predicting = trn_boston)\n# Получаем test RMSEs\nknn_tst_rmse = sapply(k, make_knn_pred, \n                      training = trn_boston, \n                      predicting = tst_boston)\n\n# Определяем лучшее значение k\nbest_k = k[which.min(knn_tst_rmse)]\n\n# Найдем значения для переобученности, недообученности и для лучшего случая\nfit_status = ifelse(k &lt; best_k, \"Over\", ifelse(k == best_k, \"Best\", \"Under\"))\n\n\n# Суммируем результаты\nknn_results = data.frame(\n  k,\n  round(knn_trn_rmse, 2),\n  round(knn_tst_rmse, 2),\n  fit_status\n)\ncolnames(knn_results) = c(\"k\", \"Train RMSE\", \"Test RMSE\", \"Fit?\")\n\n# Отобразим результаты\nknitr::kable(knn_results, escape = FALSE, booktabs = TRUE)\n\n\n\n\nk\nTrain RMSE\nTest RMSE\nFit?\n\n\n\n\n1\n1.65\n8.32\nOver\n\n\n5\n4.98\n5.83\nOver\n\n\n10\n5.26\n5.05\nOver\n\n\n25\n5.51\n4.79\nBest\n\n\n50\n5.94\n5.05\nUnder\n\n\n250\n9.61\n8.75\nUnder\n\n\n\n\n\nВопрос на засыпку: почему при k=1 ошибка на тренировочной выборке не равна 0?\n\n\n12.4.3 Сравнение с линейной регрессией\nЕсли у нас линейная зависимость: - lm() работает хорошо - knn “подгоняет автоматически”\nЕсли связь независимая: - lm() работает плохо - или работает лучше при определенных условиях - knn “делает все автоматически”\n\n\n\n\n\n\n\n\n\nТе же шаги, но быстрее, можно осуществить с помощью библиотеки caret:\n\nmodel_knn_caret &lt;- train(\n  medv ~ .,\n  data = trn_boston,\n  method = 'knn',\n  preProcess = c(\"center\", \"scale\"), tuneLength = 20 #этот параметр позволяет рассчитать разное количество соседей\n)\n\nmodel_knn_caret\n\nk-Nearest Neighbors \n\n250 samples\n 13 predictor\n\nPre-processing: centered (13), scaled (13) \nResampling: Bootstrapped (25 reps) \nSummary of sample sizes: 250, 250, 250, 250, 250, 250, ... \nResampling results across tuning parameters:\n\n  k   RMSE      Rsquared   MAE     \n   5  5.933835  0.6382250  3.799103\n   7  5.846760  0.6503739  3.810948\n   9  5.794543  0.6596814  3.805512\n  11  5.808902  0.6644311  3.820602\n  13  5.847843  0.6650228  3.846754\n  15  5.917803  0.6610201  3.904047\n  17  5.993814  0.6563786  3.954787\n  19  6.051791  0.6564321  4.001023\n  21  6.142314  0.6497636  4.053036\n  23  6.235960  0.6442404  4.138807\n  25  6.326002  0.6388550  4.205069\n  27  6.403704  0.6345582  4.262726\n  29  6.476211  0.6296321  4.305763\n  31  6.549668  0.6248754  4.352380\n  33  6.615244  0.6199260  4.392180\n  35  6.671859  0.6178162  4.432505\n  37  6.738799  0.6115130  4.475878\n  39  6.800152  0.6062191  4.517187\n  41  6.842031  0.6027472  4.542485\n  43  6.881135  0.6002764  4.584084\n\nRMSE was used to select the optimal model using the smallest value.\nThe final value used for the model was k = 9.\n\n\n\nknnPredict &lt;- predict(model_knn_caret, newdata = tst_boston)\nrsq_knn_cv &lt;- cor(knnPredict, tst_boston$medv) ^ 2\nrsq_knn_cv\n\n[1] 0.7549495",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Регрессионный анализ в R</span>"
    ]
  },
  {
    "objectID": "Regression.html#самостоятельная-работа",
    "href": "Regression.html#самостоятельная-работа",
    "title": "12  Регрессионный анализ в R",
    "section": "12.5 Самостоятельная работа",
    "text": "12.5 Самостоятельная работа\n\nПровести регрессионный анализ данных об успеваемости студентов и определяющих их фактора.\n\nНезависимые переменные:\n\nHours Studied: Общее количество часов, потраченных на учебу каждым студентом.\nPrevious Scores - предшествующие результаты: Баллы, полученные студентами на предыдущих экзаменах.\nExtracurricular Activities - Внеклассная деятельность: Участвует ли студент во внеклассных мероприятиях (да или нет).\nSleep Hours - Часы сна: Среднее количество часов сна студента в сутки.\nSample Question Papers Practiced: Количество пробных экзаменационных работ, которые студент практиковал.\n\nЦелевая переменная:\n\nPerformance Index: Показатель общей успеваемости каждого студента. Индекс успеваемости представляет собой академическую успеваемость студента и округляется до ближайшего целого числа. Индекс варьируется от 10 до 100, при этом более высокие значения свидетельствуют о более высокой успеваемости.\n\nСкачать данные\n\nПровести анализ методом логистической регрессии на данных по климату. В качестве зависимой переменной будет выступать вопрос про оценку опасности проживания вблизи ледников (вопрос 19) , а в качестве объясняющих - пол, возраст и переменная проживания в определенном районе (type).",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Регрессионный анализ в R</span>"
    ]
  },
  {
    "objectID": "Statistical-Inference.html",
    "href": "Statistical-Inference.html",
    "title": "10  Статистический вывод и тестирование исследовательских гипотез",
    "section": "",
    "text": "10.1 Общее понятие и логика статистического вывода\nСтатистический вывод (statistical inference) представляет собой процесс получения логических выводов о генеральной совокупности и ее свойствах на основании данных выборочного исследования.\nНа основании выборки исследователь тестирует те или иные гипотезы, часто:\nРезультатом статистического вывода является статистическое суждение, основывающееся на анализе статистических показателей двух типов:\nВ конечном итоге, принимая во внимание полученные результаты, аналитик принимает решение о принятии или отвержении своей исследовательской гипотезы.\nЛогика статистического вывода представляет собой порядок действий аналитика при проведении статистического анализа. В целом, она не зависит от конкретной проблемы и используемых статистических методов, однако, на практике, благодаря большому репертуару статистических инструментов, конечно, имеет свои особенности.\nТипичными являются следующие этапы статистического вывода.\nЭтапы:",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Статистический вывод и тестирование исследовательских гипотез</span>"
    ]
  },
  {
    "objectID": "Statistical-Inference.html#общее-понятие-и-логика-статистического-вывода",
    "href": "Statistical-Inference.html#общее-понятие-и-логика-статистического-вывода",
    "title": "10  Статистический вывод и тестирование исследовательских гипотез",
    "section": "",
    "text": "о различии статистических совокупностей (представления о распределении семейных обязанностей различаются у мужчин и женщин),\nо наличии закономерностей (на основе анализа совокупности явлений) об отсутствии случайностей (например, случайности распределения данных в последовательности)\n\n\n\nединичной (точечной) оценки (например, среднего значения, факторной нагрузки)\nинтервала (например, доверительного интервала для среднего значения, коэффициента корреляции или другого статистического параметра).\n\n\n\n\n\n\n\n\nСовет\n\n\n\nСтатистический анализ = анализ описательных статистик + статистический вывод\n\n\n\n\n\n\nФормулировка статистических гипотез (нулевых и альтернативных), позволяющих подтвердить существующую теорию или доказать авторскую.\nВыбор статистического критерия (метода анализа), позволяющего подтвердить гипотезы и расчет его статистических значений.\nОпределение статистической значимости (p-value) и доверительных интервалов.\nВывод о сохранении нулевых или подтверждении выдвинутых гипотез.\n\n\n10.1.1 Статистические гипотезы\nСтатистическая гипотеза - предположение о виде распределения и свойствах случайной величины, которое можно подтвердить или опровергнуть применением статистических методов к данным выборки.\nНулевая гипотеза (\\(H_0\\), null hypothesis) – содержит предположение об отсутствии различий, влияния фактора, различия значения выборочной характеристики от заданной величины (например, нуля) и т. п. Как правило, Н0 не является для исследователя содержательной гипотезой, т. е. предметом и целью доказательства.\nАльтернативная гипотеза (\\(H_1\\), alternative hypothesis) – другое проверяемое предположение, конкурирующая гипотеза (о наличии различий, взаимосвязей, отсутствии случайности, отличии от нуля и пр.). Обычно, за исключением некоторых случаев, профессиональный интерес исследователя сводится именно к верификации альтернативной гипотезы.\nНулевая гипотеза сохраняется или отвергается исходя из того, насколько вероятным оказывается наблюдаемый результат.\nДля оценки статистических гипотез используются статистические критерии (математические правила), для которых имеются рассчитанные распределения и по которым эти вероятности можно посчитать.\n\nПримеры: критерии согласия (Пирсона, Колмогорова-Смирнова), проверки на однородность (например, тест Ливиня), параметрические (t-критерий, коэффициент корреляции Пирсона – содержат в формулах средние и дисперсии) и непараметрические критерии (Манна-Уитни, Уилкоксона, часто имеют ранговый характер).\n\n\nС каждым критерием связана некоторая статистика \\(S\\), которая измеряет отклонение в наблюдаемом процессе от ситуации, соответствующей \\(H_0\\).\n\nВ силу случайности извлекаемых выборок случайными оказываются и значения статистики \\(S\\), вычисляемые в соответствии с этими выборками. То есть, если мы много раз будем извлекать выборки из генеральной совокупности, значения статистики будут отличаться, и разница между ними будет носить случайный характер.\n\n\nПри справедливости проверяемой гипотезы \\(H_0\\) статистика \\(S\\) подчиняется некоторому распределению \\(g(𝑆│𝐻_0 )\\), например, нормальному распределению, t-распределению, распределению \\(\\chi^2\\) и др.\nВ этом распределении выделяется два множества: случайных отклонений и критических значений. Если статистика попадает в область критических значений – нулевая гипотеза отклоняется, в противном случае – нет.\n\n\n\n\n\n\n\n\n\n\nРисунок 10.1: Области критических значений\n\n\n\n\n\n10.1.2 Доверительная вероятность и доверительный интервал\nЧтобы отклонить нулевую гипотезу, мы выбираем субъективное суждение относительно уровня риска, который мы готовы принять для того, чтобы ошибиться. Этот риск оказывается отраженным в понятиях доверительного интервала и доверительной вероятности.\nДоверительный интервал (confidence interval, \\(CI\\)) – диапазон, в котором находятся истинные средние значения в генеральной совокупности с определенной доверительной вероятностью.\nДоверительная вероятность (confidence level, \\(CL\\)) – вероятность того, что доверительный интервал содержит значение оцениваемого параметра.\nТипичные значения доверительной вероятности – 90%, 95% (чаще всего), 99%. Чем больше доверительная вероятность, тем шире (и иногда бесполезнее) интервал.\nПример: с 95% вероятностью можно утверждать, что данного мнения о реализации национального проекта придерживаются 48% до 73% жителей региона.\n\n\n\n\n10.1.3 Ошибки при оценке статистических гипотез\nОшибка первого рода – показывает вероятность того, что мы найдем различия там, где их на самом деле нет! Нездоровые сенсации, большой вред (Пример: поставили диагноз, а болезни нет, или человека признали виновным, а вины нет)\nМожем контролировать путем подбора порога значений, ниже которого будем считать, что различий нет, то есть уровня значимости.\nТипичный порог – \\(\\alpha\\) = 0,05.\nОшибка второго рода – различия есть, но мы их не нашли. Близорукость, слепота критерия, мы ее не можем контролировать! Вред небольшой.\nМинимизировать ошибку второго рода можно путем подбора статистического критерия.\nОшибку первого рода можно совершить, только, если мы отвергли \\(H_0\\), а ошибку второго рода – если мы приняли \\(H_0\\). Сразу две ошибки совершить нельзя!\nОптимальная величина α (критический уровень значимости) должна удовлетворять двум противоречивым требованиям:\n\nона должна быть достаточно мала, чтобы обеспечить высокое доверие к выводу об отклонении \\(H_0\\)\nно она должна быть достаточно велика, чтобы реже допускать ошибки 2-го рода\n\nПри этом вероятность ошибки 𝛽 уменьшается при увеличении значения 𝛼, а для фиксированного значения 𝛼:\n\nпри увеличении объема выборки;\nпри уменьшении выборочной дисперсии.\n\n\n\n\n\n\n\nРисунок 10.2: Ошибки первого и второго рода\n\n\n\n\n\n10.1.4 Односторонние и двусторонние критерии. Определение уровня значимости\nВ случае одностороннего критерия (one-tailed, one-sided) полученное значение статистики \\(S^*\\) сравнивают с критическим значением \\(𝑆_{(1−\\alpha)}\\) при заданном уровне значимости \\(\\alpha\\) или делают вывод на основе «достигнутого уровня значимости» (p-value): вероятности возможного превышения полученного значения статистики при справедливости \\(H_0\\).\nОдносторонний критерий применяется для оценки направленных гипотез, в которых содержатся утверждения «больше (выше)» или «меньше (ниже)».\nПример: уровень доверия к некоммерческим организациям у женщин выше, чем у мужчин.\n\n\n\n\n\n\nРисунок 10.3: Графическая интерпретация одностороннего статистического критерия\n\n\n\nВ случае двустороннего критерия (two-tailed, two-sided) критическая область состоит из двух частей. И проверяемая гипотеза \\(H_0\\) отклоняется, если \\(S^∗&gt;𝑆_{(\\alpha/2)}\\) или \\(S^∗&lt;S_{1-\\alpha /2}\\).\nДвусторонний критерий применяется для оценки ненаправленных гипотез (действуют в обе стороны), в которых содержатся утверждения «отличается» или «не равен».\nПример: уровень доверия к некоммерческим организациям у женщин у мужчин различается.\n\n\n\n\n\n\nРисунок 10.4: Графическая интерпретация двустороннего статистического критерия\n\n\n\n\n\n10.1.5 Мощность статистического критерия\nМощность статистического критерия — это способность критерия обнаружить эффект, в случае если этот эффект действительно существует. С точки зрения статистики, это вероятность справедливого опровержения нулевой гипотезы.\nПри проверке любой статистической гипотезы желательно использовать наиболее мощный критерий, который для заданной вероятности \\(𝛼\\) ошибки 1-го рода обеспечивает минимальную вероятность \\(𝛽\\) ошибки 2-го рода относительно любой конкурирующей гипотезы \\(H_1\\).\nЖелательно всегда, если позволяют данные, применять более мощный критерий, так как это позволяет избежать ошибки 2-го рода.\n\n\n\n\n\n\nРисунок 10.5: Плотности распределения статистик при справедливости соответственно гипотез \\(H_0\\) и \\(H_1\\) в случае двустороннего критерия.\n\n\n\n\n\n10.1.6 Выбор метода для анализа\nВыбор метода, с помощью которого будут анализироваться данные, осуществляется еще на этапе разработки программы научного исследования на основе его цели и задач, определяющих общий дизайн, методологические подходы и основные показатели исследования. Сразу, на «берегу», если исследователь делает выбор в пользу «количественной» аналитический стратегии, решается вопрос относительно объема и принципов формирования выборочной совокупности, необходимой для получения достаточных данных для доказательства исследовательских гипотез, а также обрисовываются контуры инструментария исследования, в который оказываются вплетены не только тщательно операционализированные и эмпирически интерпретированные понятия, но и подразумеваемые уровни измерения соцологических явлений и феноменов, от которых будет зависеть выбор конкретного статистического метода. Так, например, в исследовании перспектив развития гражданского общества в регионах России, может ставиться исследовательский вопрос об участии граждан в деятельности общественных, благотворительных организаций.\nОт того, в какой форме будет задан вопрос, как будет сформулирована исследовательская задача: от простого установления факта такого участия (что может быть достигнуто вопросом: «Участвуете ли Вы в деятельности какой-либо общественной организации или благотвориктельного общества?“) до количественного измерения вклада в деятельность НКО путем определения временных или трудозатрат («Сколько часов своего личного времени Вы тратите на общественную деятельность?», «Если перевести затраченное время в денежный эквивалент, согласно получаемой Вами зарплате на основном месте работы, сколько Вы тратите на помощь общественной организации?») будет зависеть, к какому уровню измерения (категориальному или уровню отношений) будут отнесены данные, и какой статистический метод может быть использован для проверки гипотез о том, какие слои населения в большей степени принимают участие в деятельности общественных организаций, или какие профессионалы вносят больший вклад, участвуя в программах pro bono.\nЧтобы выбрать метод, с помощью которого можно было бы проверить статистические гипотезы, нужно выполнить ряд простых действий:\n\nУточнить тип данных (количественные или качественные)\nВ случае количественных данных уточнить тип распределения (нормальное или отличное от нормального)\nУточнить количество сравниваемых групп (если они есть)\nУточнить, связаны ли группы сравнения между собой, т. е. являются ли единицы наблюдения в группах разными носителями признака (независимые группы), или это одни респонденты, которых опрашивали несколько раз (связанные группы).\n\nОтветы на эти вопросы будут определять выбор статистического метода.\nПримерная схема принятия решения для случая, когда планируется сравнивать выраженность количественного признака в одной или нескольких группах, может быть следующей:\n\n\n\n\n\nflowchart LR\n  A[Сколько групп сравнивается?] --&gt;F[1]\n  subgraph Сравнение с заданным значением\n  F--&gt;|Параметрические|O[z-критерий \\n t-критерий]\n  F--&gt;|Непараметрические|P[Критерий \\n Уилкоксона]\n  end\n  A--&gt;D[2]\n  A--&gt;J[3+]\n  subgraph Зависимые группы\n  D--&gt;|Параметрические|T[t-критерий \\n для парных \\n выборок]\n  D--&gt;|Непараметрические|L[Критерий \\n Уилкоксона]\n  J --&gt;|Параметрические|K[Дисперсионный анализ \\n для повторных \\n измерений]\n  J--&gt;|Непараметрические|M[Критерий \\n Фридмана]\n  end\n  A--&gt;R[2]\n  A--&gt;Q[3+]\n  subgraph Независимые группы \n  R--&gt;|Параметрические|S[t-критерий для \\n независимых выборок]\n  R--&gt;|Непараметрические|U[U-критерий \\n Манна-Уитни]\n  Q--&gt;|Параметрические|W[Однофакторный \\n дисперсионный анализ]\n  Q--&gt;|Непараметрические|Y[Непараметрический \\n дисперсионный анализ\\n Краскела-Уоллиса]\n  end\n\n\n\n\n\n\nЕсли в центре анализа находятся качественные данные, то рассуждения аналитика могут выстраиваться таким образом:\n\n\n\n\n\nflowchart LR\n  classDef dark color:#fff,fill:#0d5caa\n  A[Сколько групп сравнивается]--&gt;B[Одна]\n  A--&gt;C[Две]\n  A--&gt;D[Три и больше]\n  subgraph Сравнение с заданным значением\n  B--&gt;E[z-критерий]\n  end\n  subgraph Зависимые группы\n  C--&gt;F[Критерий Мак-Немара]\n  D--&gt;G[Q-критерий Кокрена]\n  end\n  A--&gt;K[Две]\n  A--&gt;L[Три и больше]\n  subgraph Независимые группы\n  K--&gt;I[Хи-квадрат Пирсона]\n  L--&gt;J[Хи-квадрат Пирсона с поправкой на правдоподобие]\n  end\n  class C,K,F,I dark\n\n\n\n\n\n\nРассмотрим каждый из приведенных критериев подробнее.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Статистический вывод и тестирование исследовательских гипотез</span>"
    ]
  },
  {
    "objectID": "Statistical-Inference.html#сопоставление-количественных-результатов-с-заданным-значением",
    "href": "Statistical-Inference.html#сопоставление-количественных-результатов-с-заданным-значением",
    "title": "10  Статистический вывод и тестирование исследовательских гипотез",
    "section": "10.2 Сопоставление количественных результатов с заданным значением",
    "text": "10.2 Сопоставление количественных результатов с заданным значением\n\n10.2.1 z-критетий и t-критерий Стьюдента для одной выборки\nВ том случае, когда мы имеем какой-либо количественный показатель и хотим сравнить со средним значением в генеральной совокупности, которое известно заранее (из ранее проведенных исследований, регламентов, переписей населения и др. источников), мы можем для этого использовать z и t-критерии.\nОба теста преследуют весьма сходные цели - сравнить выборочные средние значения с гипотетическими значениями, и являются параметрическими, что означает соблюдение требования о непрерывности распределения и нормальности данных.\nИх отличия кроются в используемых статистических распределениях (z - критерий использует нормальное распределение, тогда как t-критерий, естественно, t-распределение с более «тяжелыми» хвостами, что делает его более консервативным), а также отношение к стандартному отклонению. Если стандартное отклонение в генеральной совокупности известно, предпочтительнее использовать z-критерий, если его нет - одновыборочный критерий Стьюдента.\n\n10.2.1.1 z-критерий\nДля того, чтобы корректно использовать z-критерий для тестирования гипотез, необходимо выполнять следующие требования:\n\nпоказатель, который мы исследуем, в генеральной совокупности должен иметь нормальное распределение (оценить на практике практически нереально, но мы можем сделать проверку выборочного распределения в качестве «прокси»);\nнаблюдения в выборке должны быть независимыми, а сама выборка - случайной\nмы должны знать стандартное отклонение в генеральной совокупности\nразмер выборки должен быть достаточно большим, по меньшей мере \\(n &gt; 30\\).\n\nКакие гипотезы мы формулируем, когда используем z-критерий?\nЧто касается нулевой гипотезы, то здесь все просто:\n\\(H_0: \\bar{x}=\\mu_0\\) – выборочное среднее равно предполагаемому среднему в генеральной совокупности\nА вот альтернативная гипотеза может быть сформулирована тремя возможными способами:\n\n\\(H_{a1} : \\bar{x}&lt;\\mu_0\\) – выборочное среднее меньше предполагаемого среднего значения (левостронний тест)\n\\(H_{a2} : \\bar{x}&gt;\\mu_0\\) – выборочное среднее больше предполагаемого среднего значения (правосторонний тест)\n\\(H_{a3} : \\bar{x} \\neq \\mu_0\\) – выборочное среднее отличается от предполагаемого среднего значения (двусторонний тест)\n\nФормула для расчета z-критерия: \\[z=\\frac{\\bar{x}-\\mu_0}{\\sigma \\ \\sqrt{n}},\\]\nгде: \\(\\bar{x}\\) – выборочное среднее \\(\\mu_0\\) – гипотетическое среднее, с которым мы сравниваем выборочное среднее \\(n\\) – объем выборки \\(\\sigma\\) – гипотетическое стандартное отклонение\n\n Пример: Предположим, вы хотите оценить уровень удовлетворенности студентов материально-техническим оснащением на вашем университетском кампусе. Ваша гипотеза состоит в том, что средний уровень удовлетворенности составляет 6.5, что было определено в результате прошлогоднего исследования, и хотите проверить, верно ли это. Вы знаете, что стандартное отклонение оценок составляет 1,4.\n\n\nВы случайным образом отобрали 100 студентов и попросили их оценить свой уровень удовлетворенности по шкале от 1 до 10. После сбора данных, вы рассчитали среднюю оценку уровня удовлетворенности студентов, которая оказалась равной 6,8, со стандартным отклонением 1,2.\n\nСгенерируем аналогичные данные с помощью функции rnorm():\n\nset.seed(123)\ndata&lt;-rnorm(n=100, mean = 6.8, sd = 1.2)\n\nПоскольку мы заранее определили, что наши данные происходят из нормального распределения, довольно бессмысленно их проверять на нормальность. В доказательство сделаем гистограмму:\n\nhist(data)\n\n\n\n\n\n\n\nРисунок 10.6: Проверка данных на нормальность графическим способом\n\n\n\n\n\nУстановим пакет BSDA:\n\ninstall.packages(\"BSDA\")\n\nПосчитаем статистику теста:\n\nlibrary(\"BSDA\")\nz.test(data,\nalternative = \"two.sided\",\nmu = 6.5,\nsigma.x = 1.4,\nconf.level = 0.95\n)\n\n\n    One-sample z-Test\n\ndata:  data\nz = 2.9178, p-value = 0.003526\nalternative hypothesis: true mean is not equal to 6.5\n95 percent confidence interval:\n 6.634092 7.182882\nsample estimates:\nmean of x \n 6.908487 \n\n\nРезультаты теста показывают, что оценка удовлетворенности студентов значимо отличается от гипотетической.\nВизуально это можно представить в виде графика, показывающего, насколько далеко полученное z-значение находится от нулевой отметки, символизирующей отсутствие отличий эмпирического и теоретического средних значений. Поскольку мы использовали ненаправленную гипотезу (средние отличаются, но не понятно, как именно) и двусторонний тест, то и вероятности мы будем считать тоже как бы «в обе стороны». По тесту у нас получилось p-значение = 0,003 (вероятность ошибочно отвергнуть нулевую гипотезу составляет 0.3%) и мы разбиваем его на две части - по 0,0015 (около 0,15%) с каждой стороны.\n\n\n\n\n\n\n\n\nРисунок 10.7: Графическая интерпретация z-теста (двусторонний критерий)\n\n\n\n\n\nЕсли бы мы ставили гипотезу о том, что получившееся среднее превышает гипотетическую величину (мы предполагали бы, что оценка удовлетворенности выросла), то есть использовали бы направленную гипотезу и, соответственно, односторонний критерий, то результаты были бы следующими:\n\n\n\n\n\n\n\n\nРисунок 10.8: Графическая интерпретация z-теста (односторонний критерий)\n\n\n\n\n\n\n\n10.2.1.2 Одновыборочный t-критерий\nОдновыборочный t-критерий «ведет себя» в целом аналогично z-критерию: он также применяется к данным, подчиняющимся закону нормального распределения.\nНулевая гипотеза:\n\n\\(H_0:m=μ\\)\n\nАльтернативные гипотезы:\n\n\\(H_a:m≠μ\\) (двусторонний критерий)\n\\(H_a:m&gt;μ\\) (правосторонний)\n\\(H_a:m&lt;μ\\) (левосторонний)\n\nФормула:\n\\[t = \\frac{m-\\mu}{s/\\sqrt{n}}\\]\nРазберем возможности применения критерия на следующем примере.\n\n Пример 2: Минимальный размер оплаты труда в Алтайском крае с 1 января 2024 года составляет 20 454 руб. Вы работаете в научно-исследовательском центре и занимаетесь социально-экономическими исследованиями. В результате опроса населения были получены следующие данные о заработной плате жителей одного из сел. Докажите, что среднее значение в выборочной совокупности отличается от установленного минимального размера заработной платы в регионе.\n\nРассмотрим следующие данные:\n\ndata&lt;-c(18431, 21211, 18200, 17502, 25581, 29684, 30319, 27533, 15328, 30801, 18650, 22702, 17807, 35468, 17693, 17966, 21690, 19580, 24581, 25817, 28493, 33954, 22030, 22300, 16290, 15371, 26745, 20320, 21226, 20522)\n\nВычислим t-критерий, сравнив выборочное среднее с минимальным размером оплаты труда\n\nt_test &lt;- t.test(data, mu = 20454)\nt_test \n\n\n    One Sample t-test\n\ndata:  data\nt = 2.3188, df = 29, p-value = 0.02765\nalternative hypothesis: true mean is not equal to 20454\n95 percent confidence interval:\n 20730.00 24856.33\nsample estimates:\nmean of x \n 22793.17 \n\n\nРезультаты анализа указывают на то, что средняя зарплата в данном населенном пункте достоверно отличается от минимального размера оплаты труда в регионе (\\(t_{29}=2,3188, p=0,028\\)). Мы видим, что сравниваемое значение 20454 не попадает в 95% интервал, границы которого определяются значениями 20730,0 и 24856,33. Откуда, кстати, берутся эти значения?\nЭто станет понятным, если взглянуть на формулу доверительных интервалов для t-критерия: \\[\\left( \\bar x + t_{n-1, \\alpha / 2} \\cdot \\frac{s}{\\sqrt{n}},\n  \\bar x + t_{n-1, 1 - \\alpha / 2} \\cdot \\frac{s}{\\sqrt{n}} \\right)\\],\nгде:\n\n\\(\\bar{x}\\) - среднее значение\n\\(t_{n-1, \\alpha / 2}\\) - квантиль \\(\\alpha /2\\) t-распределения с \\(n-1\\) степенями свободы\n\\(s = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})^2}\\) - выборочное стандартное отклонение\n\\(n\\) - размер выборки\n\nПопробуем воспроизвести доверительные интервалы, используя возможности библиотеки distributions3:\n\n#install.packages(\"distributions3\")\nlibrary(distributions3)\nT_9&lt;-StudentsT(df=29)\nmean(data) + quantile(T_9, 0.05 / 2) * sd(data) / sqrt(30)\n\n[1] 20730\n\nmean(data) + quantile(T_9, 1 - 0.05 / 2) * sd(data) / sqrt(30)\n\n[1] 24856.33\n\n\nМы получили аналогичные результаты, что нас утверждает в мысли, что мы на правильном пути. Посмотрим, как результаты проверки гипотезы с помощью t-критерия можно представить графически:\n\n\n\n\n\n\n\n\nРисунок 10.9\n\n\n\n\n\n\n\n\n10.2.2 Критерий Уилкоксона\nВ том случае, когда по каким-то причинам мы не можем применять параметрические критерии (например, из-за погрешностей в распределении - слишком большой асимметрии), альтернативой z- и t-критериям может являться критерий знаковых рангов Уилкоксона (Wilcoxon signed-rank test). Он был разработат Фрэнком Уилкоксонов в 1945 году, и является одним из самых первых «непараметрических» тестов.\nВ отличие от рассмотренных выше тестов в нем сравниваются не средние значения, а медианы, что делает тест устойчивым к экстремальным значениям.\nНулевая и альтернативная гипотезы также формулируются в терминах выборочной медианы (\\(m\\)), которая сравнивается с другим медианным значением из генеральной совокупности, которое определяет исследователь:\n\n\\(H_0 : m = m_0\\)\n\\(H_1 : m \\neq m_0\\) (двусторонняя)\n\\(H_1 : m &gt; m_0\\) (правосторонняя)\n\\(H_1 : m &lt; m_0\\) (левосторонняя)\n\nКак и в другом тесте здесь есть свои допущения:\n\nраспределение оценок (различий между эмпирическим и теоретическим значением) должно быть симметричным (то есть должны быть разные данные - те, которые отклоняются от тестируемого значения в положительную сторону и в отрицательную сторону, в противном случае, тест проводить не стоит);\nвыборка должна быть случайной, а наблюдения независимыми друг от друга.\n\nОдновыборочный тест Уилкоксона основан на следующей тестовой статистике. Могут быть использованы два способа подсчета, которые приводят к идентичным результатам.\nОбозначим первый как \\(W_1\\) (также известный как \\(T\\)), а второй как \\(W_2\\). Для того, чтобы посчитать статистику критерия по каждому способу, нужно осуществить следующие действия:\n\nДля каждого значения выявить знак отличия с теоретическим значением: \\(sign_d=sgn(score−m_0)\\) . Показатель \\(sign\\) равен 1, если различия больше нуля, -1, если различия меньше нуля, и 0 - если равны нулю.\nДля каждого значения посчитать абсолютную разницу с теоретическим значением: \\(|score−m_0|\\).\nУдалить значения, различия по которым равны нулю (по этому поводу есть несколько спорных мнений - удалять или не удалять, а сдвигать значения на небольшую константу, но мы будем следовать «классическому объяснению»). После удаления «нулей» окончательный объем выборки становится равным \\(N_r\\).\nПрисвоить ранги \\(R_d\\) всем абсолютным разностям в \\(N_r\\). Наименьшему значению присваивается ранг 1, а наибольшему - ранг, соответствующий \\(N_r\\). Если есть повторные ранги (ties), то они заменяются на средние значения.\n\nДалее необходимо посчитать сумму рангов, соответствующих положительным различиям: \\(W_1=\\Sigma R^+_d\\)\nи\n\\(W_2=\\Sigma sign_d × R_d\\) (то есть просто умножить разницу на знак, а затем суммировать все произведения).\nРаспределение статистики \\(W_1\\) при условии, что верна нулевая гипотеза (\\(H_0\\)) и \\(N_r\\) достаточно большое, приближается к нормальному со средним значением \\(\\mu W_1\\) и стандартным отклонением \\(\\sigma W_1\\), которые рассчитываются по следующим формулам:\n\\[\\mu_{W_1} = \\frac{N_r(N_r + 1)}{4}\\] и\n\\[\\sigma_{W_1} = \\sqrt{\\frac{N_r(N_r + 1)(2N_r + 1)}{24}}\\]\nСледовательно, стандартизированная z-статистика принимает следующий вид:\n\\[z = \\frac{W_1 - \\mu_{W_1}}{\\sigma_{W_1}}\\]\nРаспределение статистики \\(W_2\\) при достаточно большом количестве выборки также имеет нормальное распределение со средним значением \\(0\\) и стандартным отклонением \\(\\sigma W_2\\):\n\\[ \\sigma_{W_2} = \\sqrt{\\frac{N_r(N_r + 1)(2N_r + 1)}{6}}\\]\nСтандартизированная статистика рассчитывается в этом случае по формуле:\n\\[z = \\frac{W_2}{\\sigma_{W_2}}\\]\nЕсли выборки маленькие (менее 50 наблюдений), следует использовать точные распределения.\nВернемся к нашему первому примеру и попробуем применить к этим же данным тест Уилкоксона:\n\nset.seed(123)\ndata&lt;-rnorm(n=100, mean = 6.8, sd = 1.2)\n\nres &lt;- wilcox.test(data, mu = 6.5)\n# Printing the results\nres\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  data\nV = 3506, p-value = 0.0007482\nalternative hypothesis: true location is not equal to 6.5",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Статистический вывод и тестирование исследовательских гипотез</span>"
    ]
  },
  {
    "objectID": "Statistical-Inference.html#сравнение-исследуемого-признака-в-двух-и-более-независимых-выборках",
    "href": "Statistical-Inference.html#сравнение-исследуемого-признака-в-двух-и-более-независимых-выборках",
    "title": "10  Статистический вывод и тестирование исследовательских гипотез",
    "section": "10.3 Сравнение исследуемого признака в двух и более независимых выборках",
    "text": "10.3 Сравнение исследуемого признака в двух и более независимых выборках\n\n10.3.1 t-критерий\nТ-критерий для независимых выборок (t-тест, t-критерий Стьюдента) предназначен для сравнения средних в двух независимых группах с целью предоставления статистического обоснования того, что и в основной популяции (генеральной совокупности) соответствующие средние так же различаются. Своим именем этот критерий обязан английскому химику и математику Уильяму Сили Госсету, работавшему в конце XIX века на пивоварне Гиннесс в Дублине. Владельцы пивоварни поставили перед ученым весьма амбициозную задачу - сохранить репутацию лучших пивоваров и вывести процесс производства пива на новый уровень, что потребовало проведения исследований в области контроля качества. Как известно, производство пива - процесс не только высокотехнологичный, требующий четкости следования процедурам, но и результат случайного стечения обстоятельств, поскольку натуральные продукты, из которых производится пиво - хмель, ячмень, солод, как и другие сельскохозяйственные культуры, имеют большую степень вариабельности вкусовых качеств из-за влияния состава почвы, климата и других факторов. Задача Госсета как подмастерья пивовара заключалась не только в оценке качества этих продуктов, но и том, чтобы сделать это наиболее экономным и точным образом.\n\n\n\n\n\n\nРисунок 10.10: Памятная табличка в Дублине, на складе пивоварни Гиннесс\n\n\n\nРаботая с небольшими выборками, Госсет заметил, что распределение средних значений отклонялось от нормального распределения, и, значит, он не мог использовать обычные статистические методы, основанные на нормальном распределении для того, чтобы принимать решения.\nВ 1904 году Госсет опубликовал внутренний отчет, в котором математически обосновал свой «закон ошибок» среднего (Law of Error) и возможности его применения для нужд пивоварни. В своей работе он описал, что «чем больше наблюдений, по которым рассчитано среднее значение, тем меньше (вероятная) ошибка». Госсет также отметил, что в сравнении с нормальным распределением, «кривая, представляющая частоту ошибок становится выше и уже по мере того, как объем выборки уменьшается. Управление Гинесса предложило Госсету проконсультироваться с другими специалистами, и так состоялась встреча Госсета с Карлом Пирсоном, под руководством которого в 1908 году в журнале «Биометрика» и появляется знаменитая работа Госсета об оценке ошибки среднего, напсанная под псевдонимом Стьюдент. Госсет не мог публиковаться под своим собственным именем, поскольку речь шла о данных, составляющих коммерческую тайну, а завод Гинесса очень строго относился к своим данным. Поэтому, вероятно, под влиянием названия на блокноте, который он использовал для ведения записей (The Student’s Science Notebook), Госсет выбрал такой псевдоним, который он использовал в 19 из своих 21 научных работ (Brown).\n Примеры: сравнение результатов академической успеваемости в двух группах учащихся, доходов у мужчин и женщин, доверия к социальным институтам среди городских и сельских жителей и др.\nТребования:\n\nЗависимая переменная должна быть непрерывной, измеренной по интервальной шкале или шкале отношений Независимая переменная - категориальная (номинальная), состоящая из двух и более групп (но мы сравниваем только две)\nДля анализа отбираются только случаи, где есть валидные значения по обеим переменным (отсутствующие значения удаляются)\nСравниваемые группы должны быть независимыми (должно выполняться требование независимости наблюдений). Что это значит?\n\n\nМежду наблюдениями в разных группах нет никаких взаимосвязей, то есть:\nСубъекты (респонденты) в первой группе не могут быть одновременно и во второй группе (респонденты в каждой группе - разные)\nСубъекты (респонденты) одной группы никаким образом не могут влиять на субъектов (респондентов) в другой группе (например, в одной группе - родители, в другой - дети, или в одной начальники, а в другой - подчиненные и т. д.) Независимость двух выборок означает, что средние значения «будут совершенно некоррелируемыми для бесконечного множества пар выборок».\n\n\nВыборка должна быть случайной\nЗависимая переменная в каждой группе должна быть нормально распределена Ненормальное распределение, особенно с «тяжелыми хвостами» или слишком большой асимметрией значительно снижает мощность теста (его способность отвергать нулевую гипотезу) В случае если выборка по размеру средняя или большая, нарушениями нормальности можно пренебречь, так как они меньше влияют на величину ошибки \\(p\\).\nГомогенность дисперсий (то есть дисперсии в группах должны быть практически равными) Когда это требование нарушается и размеры групп не совпадают, значению \\(p\\) нельзя доверять.К счастью, R позволяет рассчитать модифицированную статистику t-критерия по формулам, которые не основываются на допущении о равенстве дисперсий. Этот альтернативный критерий носит название t-критерия Уэлча, он также известен под названиями t-критерия для неравных дисперсий (Unequal Variance t-Test или Separate Variances t-Test).\nВ измерениях не должно быть выбросов (можно проверить по ящичной диаграмме).\n\nНулевая гипотеза (\\(H_0\\)) и альтернативная гипотеза (\\(H_1\\)) при использовании t-критерия Стьюдента может быть выражена двумя аналогичными способами:\n\n\\(H_0: \\mu_1 = \\mu_2\\) («средние в двух группах равны»)\n\\(H_1: \\mu_1 \\neq \\mu_2\\) («средние в двух группах не равны»)\n\nИЛИ\n\n\\(H_0: \\mu_1- \\mu_2 = 0\\) («различия между средними равны 0»)\n\\(H_1:\\mu_1 - \\mu_2 \\neq 0\\) («различия между средними в двух группах не равны 0»)\n\nгде \\(µ_1\\) и \\(µ_2\\) это средние в генеральной совокупности для группы 1 и группы 2, соответственно. Заметим, что вторая группа гипотез выводится просто путем переноса \\(\\mu_2\\) в левую часть уравнения (неравенства) - или путем ее вычитания из обеих частей.\nКогда доказано, что две выборки происходят из групп генеральной совокупности с равными дисперсиями \\(\\sigma^2_1=\\sigma^2_2\\)), статистика t-критерия рассчитывается по формулам:\n\\[t=\\frac{\\bar{x_1}-\\bar{x_2}}{s_p/\\sqrt{n_\\sigma}}\\]\nгде\n\\[s_p=\\sqrt{\\frac{(n_1-1)*s^2_1+(n_2-1)*s^2_2}{n_1+n_2-2}}\\]\nа\n\\[n_\\sigma=\\frac{1}{\\frac{1}{n_1}+\\frac{1}{n_2}}\\]\nгде \\(\\bar{x_1}\\),\\(\\bar{x_2}\\) – средние значения в сравниваемых выборках, \\(n_1, n_2\\) – количество наблюдений в первой и второй группах, \\(s_1, s_2\\) – стандартные отклонения в первой и второй группах, \\(s_p\\) – объединенное стандартное отклонение.\nРаспределение статистики t-критерий является t-распределением Стьюдента с \\(df\\) - степенями свободы. Если гипотеза о равенстве дисперсий подтверждается, то количество степеней свободы подсчитывается по формуле \\(n_1+n_2-2\\).\nКогда независимые выборки (и соответствующие им группы в генеральной совокупности) имеют неравные дисперсии (то есть, \\(𝜎_1^2≠𝜎_2^2\\)), t-критерий рассчитывается по формуле (известной также как t-критерий Уэлча):\n\\[t=\\frac{\\bar{x_1}-\\bar{x_2}}{\\sqrt{\\frac{s^2_1}{n_1}+\\frac{s^2_2}{n_2}}}\\] где (\\(\\bar{x_1}, \\bar{x_2}\\),– средние значения в сравниваемых выборках,\\(n_1, n_2\\) - количество наблюдений в первой и второй группах, \\(s_1, s-2\\) – стандартные отклонения в первой и второй группах.\nКоличество степеней свободы при этом высчитывается по формуле: \\[df=\\frac{(\\frac{s^2_1}{n_1}+\\frac{s^2_2}{n_2})^2}{\\frac{1}{n_1-1}(\\frac{s^2_1}{n_1})^2+\\frac{1}{n_2-1}(\\frac{s^2_2}{n_2})^2}\\] Полученная статистика \\(t\\) сравнивается с критическими значениями из таблиц с t-распределения Стьюдента для количества степеней свободы и выбранного уровня значимости α (как правило 0,05). Если рассчитанное значение t больше табличного, мы отвергаем нулевую гипотезу (\\(H_0\\)) о равенстве средних значений.\nРассмотрим возможности анализа данных с помощью t-критерия Стьюдента на следующем примере.\n\n Пример: В нашем исследовании по климату есть переменные, оценивающие важность для коренных народов, проживать на территории традиционного проживания, соблюдать обычаи и этнические традиции, осуществлять традиционную хозяйственную деятельность. Это табличный вопрос В18. Создадим усредненное значение по пяти подвопросам данного блока и используем его в качестве обобщенной оценки важности сохранения традиционных основ жизнедеятельности коренных народов, проживающих на высокогорных территориях. Проведем сравнительный анализ средних оценок в различных возрастных группах - в группе молодежи до 35 лет и среди жителей старше 35-летнего возраста.\n\nЗагрузим данные:\n\nlibrary(haven)\ndf&lt;-read_sav(\"База_КлимРиск_2023.sav\")\n\nПосчитаем новую переменную - среднее значение по переменным B18_1 - B18_5 и сделаем гистограмму:\n\nlibrary(dplyr)\ndf&lt;-df |&gt; \n  rowwise() |&gt;  \n  mutate(V18_mean=mean(c_across(starts_with(\"V18\")), na.rm = TRUE))\nhist(df$V18_mean, col=\"steelblue\")\n\n\n\n\n\n\n\n\nМы видим, что наши данные весьма далеки от совершенства, и есть значительная отрицательная асимметрия, происходящая оттого, что большинство опрошенных дали самые высокие оценки значимости по всем пяти подвопросам. Строго говоря, с такими данными применять t-критерий не совсем корректно, и любой тест на нормальность это подтвердит.\n\nshapiro.test(df$V18_mean)\n\n\n    Shapiro-Wilk normality test\n\ndata:  df$V18_mean\nW = 0.80376, p-value &lt; 2.2e-16\n\n\n\nt(psych::describe(df$V18_mean))\n\n                   X1\nvars       1.00000000\nn        884.00000000\nmean       3.44560709\nsd         0.68243009\nmedian     3.80000000\ntrimmed    3.56541902\nmad        0.29652000\nmin        1.00000000\nmax        4.00000000\nrange      3.00000000\nskew      -1.24143852\nkurtosis   0.92310844\nse         0.02295261\n\n\nТак и есть, у нас асимметрия превышает допустимые пределы от +1 до -1 (Hair et al., 2022). Но исключительно в учебных целях мы продолжим.\nСоздадим группирующую переменную по возрасту:\n\ndf&lt;-df |&gt; \n  mutate(age_bin=case_when(\n    age&lt;35 ~ \"До 35 лет\",\n    age&gt;=35 ~ \"Старше 35 лет\"\n      ))\n\nДля проверки гомогенности дисперсий в группах проведем лест Ливиня, преимуществом которого является возможность использования, когда в данных есть отклонения от нормальности:\n\nlibrary(car)\nleveneTest(V18_mean~age_bin, data = df)\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value  Pr(&gt;F)  \ngroup   1  5.7383 0.01681 *\n      858                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nСудя по тесту, у нас значимые различия в дисперсиях, а значит, если уж использовать t-критерий, то его модифицированную версию Уэлча:\n\nt.test(V18_mean~age_bin, data = df)\n\n\n    Welch Two Sample t-test\n\ndata:  V18_mean by age_bin\nt = -3.362, df = 414.04, p-value = 0.0008457\nalternative hypothesis: true difference in means between group До 35 лет and group Старше 35 лет is not equal to 0\n95 percent confidence interval:\n -0.28310252 -0.07419598\nsample estimates:\n    mean in group До 35 лет mean in group Старше 35 лет \n                   3.314108                    3.492757 \n\n\nТест показал наличие значимых отличий в оценке важности традиционной деятельности по возрасту (\\(t_{414,04}=-3,362, p = 0,001\\)), указывающих на то, что современная молодежь, проживающая вблизи ледников, в меньшей степени ориентирована на сохранение традиционных ценностей и устоев своего народа.\nСоздадим визуализацию к результатам теста:\n\nlibrary(ggpubr)\ndf |&gt;  \n  filter(!is.na(age_bin)) |&gt;  \n  ggboxplot(x = \"age_bin\", y = \"V18_mean\",\n          color = \"age_bin\", palette = \"jco\", order = c(\"До 35 лет\", \"Старше 35 лет\"), add = \"jitter\", xlab = \"Возраст\", )+stat_compare_means(method = \"t.test\", label.y = 4.5)\n\n\n\n\n\n\n\nРисунок 10.11: Ящичная диаграмма по результатам рассчета t-критерия (ggpubr)\n\n\n\n\n\nЛибо как вариант:\n\nlibrary(ggstatsplot)\nggbetweenstats(df, age_bin, V18_mean, xlab = \"Возраст\")\n\n\n\n\n\n\n\nРисунок 10.12: Ящичная диаграмма по результатам рассчета t-критерия (ggstatsplot)\n\n\n\n\n\nНесмотря на то, что различия статистически достоверны, величина эффекта представляющая собой стандартизованную разницу между средними, невысока: Hedges’ g (метрика, позволяющая оценить различия в средних) составляет всего 0,26, что можно считать очень незначительной разницей (согласно негласным нормам средний эффект наблюдается при g = 0,5, значительный - при g выше 0,8).\n\n\n10.3.2 U-критерий Манна-Уитни\nКритерий Манна-Уитни (U критерий, также называемый критерием Манна-Уитни-Вилкоксона, MWW/MWU, критерием ранговых сумм Вилкоксона, тестом Вилкоксона-Манна-Уитни) – непараметрический тест, проверяющий нулевую гипотезу о том, что в случайных значениях двух групп X и Y вероятность того, что X больше Y равна вероятности, что Y больше, чем X.\nПредложен в 1945 году американским химиком и статистиком Фрэнком Уилкоксоном, доработан австрийским и американским математиком Генри Манном и Дональдом Уитни в 1947 году.\nU-критерий Манна-Уитни используется для сравнения выраженности показателей в двух несвязных (независимых) выборках, является непараметрическим аналогом t-критерия Стьюдента.\nТребования:\n\nВсе наблюдения из двух групп должны быть независимыми друг от друга\nДанные должны быть измеренными по крайней мере в порядковой шкале (взяв два значения мы должны точно сказать, какое из них больше)\nНулевая гипотеза \\(H_0\\) предполагает, что распределения в двух группах являются идентичными Альтернативная гипотеза \\(H_1\\) заключается в том, что распределения не идентичны.\n\nДля применения U-критерия Манна — Уитни нужно:\n\nСоставить единый ранжированный ряд из обеих сопоставляемых выборок, расставив их элементы по степени нарастания признака и приписав меньшему значению меньший ранг. Общее количество рангов получится равным: \\(N=n_1+n_2\\), где \\(n_1\\) — количество элементов в первой выборке, а \\(n_2\\) — количество элементов во второй выборке.\nРазделить единый ранжированный ряд на два, состоящие соответственно из единиц первой и второй выборок. Подсчитать отдельно сумму рангов, пришедшихся на долю элементов первой выборки, и отдельно — на долю элементов второй выборки.\nОпределить значение U-критерия Манна — Уитни в каждой группе по формуле:\n\n\\[U_x=T_x-\\frac{n_x(n_x+1)}{2},\\] где \\(T_x\\) - сумма рангов.\nМеньшее значение и будет итоговым значением критерия.\n\nПо таблице для избранного уровня статистической значимости определить критическое значение критерия для данных \\(n_1\\) и \\(n_2\\). Если полученное значение \\(U\\) меньше табличного или равно ему, то признается наличие существенного различия между уровнем признака в рассматриваемых выборках (принимается альтернативная гипотеза). Если же полученное значение \\(U\\) больше табличного, принимается нулевая гипотеза. Достоверность различий тем выше, чем меньше значение \\(U\\).\n\nПроведем тест Манна-Уитни с примером выше:\n\nwilcox.test(V18_mean~age_bin, data = df)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  V18_mean by age_bin\nW = 62591, p-value = 0.0001254\nalternative hypothesis: true location shift is not equal to 0\n\n\nРезультаты аналогичны полученным с помощью t-критерия.\n\nlibrary(ggstatsplot)\nggbetweenstats(df, age_bin, V18_mean, xlab = \"Возраст\", type=\"nonparametric\")\n\n\n\n\n\n\n\nРисунок 10.13: Ящичная диаграмма по результатам рассчета U-критерия (ggstatsplot)\n\n\n\n\n\n\n\n10.3.3 Однофакторный дисперсионный анализ\nДисперсионный анализ (Analysis of Variance, ANOVA) – статистический метод выявления различий между выборочными средними для двух или больше совокупностей.\nСуществуют различные разновидности дисперсионного анализа, различающиеся по количеству группирующих (факторных) и зависимых переменных, используемых в анализе, типам выборок (независимые наблюдения или повторные эксперименты), наличию кластеров внутри выборки и др.\nСамым простым является однофакторный дисперсионный анализ (One-way или one-factor ANOVA), в котором используются одна зависимая и одна независимая переменные.\n\n Например, нас может интересовать влияние принадлежности к тому или иному социальному классу (низший, средний и высший класс) на показатели здоровья, такие как доступ к медицинским услугам или распространенность хронических заболеваний, или мы могли бы сравнить, как лица с различным уровнем религиозности (высокорелигиозные, со средней религиозностью или совсем не религиозности) оценивают уровень социальной справедливости или проявляют электоральное поведение, а также как уровень потребления медиаконтента (например, заядлые ТВ-зрители или типичные пользователи социальных сетей или читатели определенных газет) взаимосвязан с установками в отношении социальных проблем, так как проблемы миграции или гендерного неравенства.\n\nПри этом, мы предполагаем, что исходные значения зависимой переменной можно разложить на несколько компонентов, определяющих различия между ними:\n\\[x_{ij}=\\mu + F_i+ \\epsilon_{ij}, \\] где\n\n\\(x_{ij}\\) - значение зависимой переменной, полученной на \\(i-м\\) уровне фактора с порядковым номером \\(j\\);\n\\(\\mu\\) - общее среднее значение;\n\\(F_i\\) - эффект, обусловленный влиянием \\(i-го\\) уровня фактора;\n\\(\\epsilon_{ij}\\) - остаточный член, возмущение, вызванное влиянием неконтролируемых факторов, то есть вариацией переменной внутри уровня.\n\nГипотезы:\n\n\\(H_0\\): \\(\\mu_1=\\mu_2=\\mu_3=...= \\mu_k\\)\n\\(H_1\\): не все средние равны (хотя бы между одной парой средних имеются различия)\n\nДисперсионный анализ сравнивает дисперсии двух видов — внутри групп (связанную со случайными, неконтролируемыми различиями между испытуемыми) и между группами (связанную с влиянием группирующей переменной, или фактора). Как мы увидели выше, при сравнении двух групп t-статистика измеряет разность средних стандартной ошибкой. Дисперсионный анализ измеряет квадрат разности средних квадратами стандартной ошибки, т.е. результат для двух выборок равен квадрату, рассчитанной по этим же данным t-статистики.\nМежгрупповая (факторная) дисперсия рассчитывается по формуле:\n\\[MS_b=\\frac{SSb}{k-1},\\] где\n\n\\(SS_b=\\Sigma n_i(\\bar{x_i}-\\bar{x})\\) - межгрупповая сумма квадратов отклонений среднего значения в каждой группе от общего среднего;\n\\(k-1\\) - степень свободы (количество уровней группирующей переменной минус единица).\n\nВнутригрупповая (остаточная) дисперсия:\n\\[MS_w=\\frac{SSw}{N-k},\\] где\n\\(SS_w=\\Sigma(x-\\bar{x_i})\\) - внутригрупповая сумма квадратов отклонений \\(N-k\\) - количество степеней свободы\nОбщая сумма квадратов отклонений есть сумма межгрупповых и внутригрупповых квадратов отклонений:\n\\[SS=SS_w+SS_b\\] Результаты вычислений можно представлять в виде следующей таблицы:\n\nОсновной в дисперсионном анализе является статистика \\(F\\), показываются соотношение между межгрупповой и внутригрупповой дисперсиями. Если межгрупповая дисперсия существенно выше, чем внутригрупповая, то между изучаемыми группами (уровнями) существуют статистически значимые различия.\n\\[F=\\frac{MS_b}{MS_w}\\]\n\n\n\n\n\n\nРисунок 10.14: Межгрупповая и внутригрупповая дисперсии\n\n\n\n\n10.3.3.1 Множественные сравнения (post hoc tests – апостериорные тесты)\nСам по себе дисперсионный анализ показывает, что есть различия хотя бы между одной парой средних значений (наличие общего эффекта). Но между какими именно группами? Обычно на вычислении \\(F\\) все не заканчивается, а только начинается.\nПервое, что может прийти в голову: почему бы не сравнить группы попарно с помощью того же t-критерия? Однако, все не так просто, поскольку когда групп много, возникает проблема одновременной проверки множественных гипотез.\nВкратце, эта проблема заключается в том, что при одновременной проверке большого числа гипотез на том же наборе данных вероятность сделать неверное заключение в отношении хотя бы одной из этих гипотез значительно превышает изначально принятый уровень значимости (\\(\\alpha\\)).\nТак, если мы будем сравнивать десять групп по тридцать испытуемых в каждой, то мы практически всегда найдем отличие лучшей группы от худшей на уровне значимости меньше 0.05, даже если группы эти набирались совершенно случайно и ни о каком воздействии, которое могло бы привести к систематическому сдвигу среднего значения, речи не шло. Например, вы можете сравнить пассажиров десяти вагонов поезда по тесту, измеряющему социальную дистанцию к какой-либо группе и убедиться, что значимость отличия лучшего вагона от худшего по Т-критерию, как правило, меньше 0.05.\nМожно сказать, что 10(10−1)/2 (число сочетаний из 10 по 2) попарных сравнений «всех со всеми» практически гарантируют хотя бы одну ошибку первого рода, т.е. отвержение гипотезы \\(H_0\\), когда она на самом деле верна.\nКогда мы сравниваем три группы (например, А и В, А и С, В и С), вероятность совершить ошибку хотя бы в одном из этих трех сравнений составит:\n\\[P' = 1 - (1 - \\alpha)^m =  1- (1 - 0.05)^3 = 0.143,\\] Если же количество сравнений 45, как в примере с 10 вагонами (10*9/2=45), то вероятность ошибки начинает превышать 90%:\n\\[P' = 1 - (1 - \\alpha)^m =  1- (1 - 0.05)^{45} = 0.90,\\]\nЧто делать?\nДля устранения эффекта множественных сравнений существует большой репертуар методов, позволяющих снизить вероятность ошибочного решения. Они различаются как своей консервативностью, так и условиями применения.\nОдним из самых простых и известных способов контроля над групповой вероятностью ошибки является Метод Бонферрони (назван так в честь предложившего его итальянского математика Карло Эмилио Бонферрони; Carlo Emilio Boferroni). Он заключается в умножении полученных при сравнении групп p-значений на количество сравниваемых групп.\n\n Пример: Предположим, Предположим, что мы применили определенный статистический критерий 3 раза (например, сравнили при помощи критерия Стьюдента средние значения групп А и В, А и С, и В и С) и получили следующие три Р-значения: 0.01, 0.02 и 0.005. Чтобы применить метод Бонферрони, мы должны умножить каждое из p-значений на 3, а затем сравнить с выбранным уровнем значимости:\n\n0.01 * 3 = 0.03 &lt; 0.05: гипотеза отклоняется;\n0.02 * 3 = 0.06 &gt; 0.05: гипотеза принимается;\n0.005 * 3 = 0.015 &lt; 0.05: гипотеза отклоняется.\n\n\nХотя метод Бонферрони очень прост в реализации, он обладает одним существенным недостатком: при возрастании числа проверяемых гипотез мощность этого метода резко снижается. Другими словами, при возрастании числа гипотез нам будет все сложнее и сложнее отвернуть даже те из них,которые должны быть отвергнуты. Например, при проверке 10 гипотез, применение поправки Бонферрони привело бы к снижению исходного уровня значимости до 0.05/10 = 0.005. Соответственно, для отклонения той или иной гипотезы, соответствующие Р-значения должны были бы оказаться меньше 0.005, и такого условия достичь маловероятно. В связи с этим метод Бонферрони не рекомендуется использовать, если число проверяемых гипотез превышает 7-8.\nДля преодоления проблем, связанных с низкой мощностью метода Бонферрони, в 1978 г. Стур Холм (Holm 1978) предложил гораздо более мощную его модификацию (часто этот метод называют еще методом Холма-Бонферрони). Этот модифицированный метод основан на алгоритме, который включает следующие шаги:\n\nИсходные Р-значения упорядочиваются по возрастанию: \\(p_{(1)} \\leq p_{(2)} \\leq \\dots \\leq p_{(m)}\\). Эти Р-значения соответствуют проверяемым гипотезам \\(H_{(1)}, H_{(2)}, \\dots H_{(m)}\\).\nЕсли \\(p_{(1)} \\geq \\alpha/m\\), все нулевые гипотезы \\(H_{(1)}, H_{(2)}, \\dots H_{(m)}\\) принимаются и процедура останавливается. Иначе следует отвергнуть гипотезу \\(H_{(1)}\\) и продолжить.\nЕсли \\(p_{(2)} \\geq \\alpha/(m-1)\\), нулевые гипотезы \\(H_{(2)}, H_{(3)}, \\dots H_{(m)}\\) принимаются и процедура останавливается. Иначе гипотеза \\(H_{(2)}\\) отвергается и процедура продолжается.\n…\nЕсли \\(p_{(m)} \\geq \\alpha\\), нулевая гипотеза $H_{(m)} принимается и процедура останавливается.\n\nОписанную процедуру называют нисходящей (англ. step-down): она начинается с наименьшего P-значения в упорядоченном ряду и последовательно «спускается» вниз к более высоким значениям. На каждом шаге соответствующее значение \\(p_{(i)}\\) сравнивается со скорректированным уровнем значимости \\(\\alpha / (m+i-1)\\). Как и в случае с поправкой Бонферрони, вместо корректировки уровня значимости мы можем скорректировать непосредственно Р-значения - конечный результат (в смысле принятия или отклонения той или иной гипотезы) окажется идентичным. Соответствующая поправка выполняется в виде \\(q_i = p_{(i)} (m + i -1)\\). Так, для рассмотренного выше примера с тремя Р-значениями получаем:\n\n\\(q_1 = p_{(1)}(m - 1 + 1) = 0.005*3 = 0.015\\)\n\\(q_2 = p_{(2)}(m - 2 + 1) = 0.01*2 = 0.02\\)\n\\(q_3 = p_{(2)}(m - 3 + 1) = 0.02*1 = 0.02\\)\n\nИменно последний подход реализован в R-функции p.adjust():\n\n# Скорректированные Р-значения:\np.adjust(c(0.01, 0.02, 0.005), method = \"holm\")\n\n[1] 0.020 0.020 0.015\n\n\nНа практике, в различных программах статистической обработки данных используются разные критерии для множественных сравнений.\nТак, критерии диапазона выявляют однородные подмножества средних, не различающихся между собой. Парные множественные сравнения проверяют разности между каждой парой средних значений и выдают матрицу, в которой звездочками обозначены групповые средние, значимо различающиеся на уровне \\(\\alpha\\).\nЕсли требование о равенстве дисперсий выполняется, то рекомендуется использовать такие апостериорные тесты как Тьюки-b, С-Н-К (Стьюдента-Ньюмена-Келса), Дункана, Р-Э-Г-У F ( F-критерий Райана-Эйнота-Габриэля-Уэлша), Р-Э-Г-У Q (критерий диапазона Райана-Эйнота-Габриэля-Уэлша) и Уоллера-Дункана, Бонферрони, Тьюки LSD, Шидака, Габриэля, Гохберга, Даннетта, Шеффе и НЗР (наименьшей значимой разности).\nЕсли требование о равенстве дисперсий не выполняется: Тамхейна T2, Даннетта T3, Геймса-Хоуэлла и Даннетта C.\nПродолжим наш пример с климатическими данными, однако, на этот раз, разделим выборку не на две, а на три возрастные группы, используя уже имеющуюся в наборе переменную age_cat3.\nПоскольку нам нужна факторная переменная, прежде всего восстановим метки значений для этой переменной:\n\ndf$age_cats3&lt;-sjlabelled::as_label(df$age_cats3)\n\nПроведем тест на гомогенность дисперсий (тест Ливиня из библиотеки rstatix - не забываем, что ее нужно предварительно установить):\n\ndf %&gt;% \n  rstatix::levene_test(V18_mean ~ age_cats3)\n\n# A tibble: 1 × 4\n    df1   df2 statistic      p\n  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n1     2   857      2.62 0.0732\n\n\nТест показывает отсутствие значимых различий, а значит, мы можем использовать «обычную» практику расчетов, в ппротивномслучае было бы лучше воспользоваться специальной формулой Уэлча, которая есть и для дисперсионного анализа.\nПроведем дисперсионный анализ, используя функцию anova_test из уже упомянутого пакета rstatix. В качестве аргумента мы должны задать формулу: V18_mean ~ age_cats3, означающую, что мы хотим исследовать зависимость значений V18_mean от уровней факторной переменной age_cats3:\n\nres.aov &lt;- df %&gt;% rstatix::anova_test(V18_mean ~ age_cats3)\nres.aov\n\nANOVA Table (type II tests)\n\n     Effect DFn DFd    F        p p&lt;.05   ges\n1 age_cats3   2 857 7.29 0.000725     * 0.017\n\n\nЗначение F составило 7,29, весьма далеко от случайного отклонения:\n\n\n\n\n\n\n\n\n\n\npwc &lt;- df %&gt;% rstatix::tukey_hsd(V18_mean ~ age_cats3)\npwc\n\n# A tibble: 3 × 9\n  term      group1    group2      null.value estimate conf.low conf.high   p.adj\n* &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;            &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 age_cats3 До 30 лет 31-49 лет            0   0.245    0.0930    0.397  4.82e-4\n2 age_cats3 До 30 лет 50 лет и с…          0   0.204    0.0442    0.364  7.87e-3\n3 age_cats3 31-49 лет 50 лет и с…          0  -0.0409  -0.163     0.0807 7.09e-1\n# ℹ 1 more variable: p.adj.signif &lt;chr&gt;\n\n\nВизуализация результатов (первый вариант):\n\n# Прежде, чем создать визуализацию, создадим список пар групп, которые будем сравнивать\nmy_comparisons &lt;- list( c(\"До 30 лет\", \"31-49 лет\"), c(\"До 30 лет\", \"50 лет и старше\"), c(\"31-49 лет\", \"50 лет и старше\"))\ndf %&gt;% \n  filter(!is.na(age_cats3)) %&gt;% \n  ggviolin(x = \"age_cats3\", y = \"V18_mean\",\n          color = \"age_cats3\", fill = \"age_cats3\", add = \"boxplot\", add.params = list(fill = \"white\"), palette = \"jco\",  xlab = \"Возраст\", ylab = \"Оценка важности сохранения \\n традиционных основ жизнедеятельности\", legend.title = \"Возрастные группы\" )+stat_compare_means(comparisons = my_comparisons)\n\n\n\n\n\n\n\nРисунок 10.15: Визуализация результатов однофакторного дисперсионного анализа (ggpubr)\n\n\n\n\n\nВизуализация результатов (вариант c помощью пакета ggstatsplot). Отметим, что по умолчанию функция основывается на предположении об отсутствии равенства дисперсий и использует критерий Геймса-Хоуэлла, поэтому результаты отличаются p-значениями, но общий вывод не меняется: основные различия пролегают между оценками младшей группы и всеми остальными.\n\nlibrary(ggstatsplot)\nggbetweenstats(\n  df,\n  x    = age_cats3,\n  y    = V18_mean,\n  type = \"parametric\",\n  xlab = \"Возраст\"\n)\n\n\n\n\n\n\n\nРисунок 10.16: Визуализация результатов однофакторного дисперсионного анализа (ggstatsplot)\n\n\n\n\n\n\n\n\n10.3.4 Однофакторный непараметрический дисперсионный анализ Краскела-Уоллиса\nАдекватное применение дисперсионного анализа основывается на допущении о том, что зависимая переменная является непрерывной, имеет нормальное распределение и достаточно большую выборку (желательно \\(n_j&gt; 30\\) где \\(j=1, 2, ..., k\\), а \\(k\\) обозначает количество независимых сравниваемых групп). Дополнительно, ANOVA требует равенства дисперсий в сравниваемых выборках. Этот метод достаточно устойчив к отклонением, если выборки небольшие, но одинаковые. Когда есть проблемы с нормальностью или выборки маленькие и неравные, а также когда данные измерены в порядковой шкале, лучше использовать непараметрический аналог\nОдним из часто используемых непараметрических аналогов, позволяющих сравнить более двух независимых группn, является критерий Краскела-Уоллиса (Kruskal Wallis test). Этот тест сравнивает распределения в k группах (k &gt; 2), при этом исходные данные также заменяются рангами. Является обобщением U-критерия Манна-Уитни для количества групп более двух.\nНулевая и альтернативная исследовательские гипотезы формулируются следующим образом:\n\n\\(H_0\\): Медианы в \\(k\\) группах населения являются равными\n\\(H_1\\): Медианы в \\(k\\) группах не равны\n\nПроцедура тестирования предполагает проведение следующих шагов:\n\nсведение всех \\(k\\) выборок в один комбинированный набор\nранжирование всех значений от 1 до \\(N\\), где \\(N = n_1+n_2 + ...+ n_k\\) и присвоение рангов\nподсчет суммы рангов в каждой группе\nвычисление статистики критерия\nопределение уровня значимости и формулировка вывода.\n\nТребования\n\nВыборки являются случайными.\nНаблюдения не зависят друг от друга\nКак минимум порядковый уровень измерения для зависимой переменной и порядковый или номинальный – для независимого фактора.\nНет требований о характере распределения или нормальности данных.\n\nФормула для расчета H-критерия Краскела-Уоллиса:\n\\[H=\\frac{12}{N(N+1)}\\Sigma^k_{i=1}\\frac{R^2_i}{n_i}-3(N+1)\\]\nгде:\n\n\\(k\\) количество сравниваемых групп\n\\(N\\) - общий объем выборки\n\\(n_i\\) - объем выборки в группе \\(i\\)\n\\(R_i\\) - сумма рангов в группе \\(i\\).\n\n\nkruskal.test(V18_mean ~ age_cats3, data = df)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  V18_mean by age_cats3\nKruskal-Wallis chi-squared = 17.074, df = 2, p-value = 0.0001961\n\n\nМы также можем сделать парные сравнения:\n\npairwise.wilcox.test(df$V18_mean, df$age_cats3,  p.adjust.method = \"BH\")\n\n\n    Pairwise comparisons using Wilcoxon rank sum test with continuity correction \n\ndata:  df$V18_mean and df$age_cats3 \n\n                До 30 лет 31-49 лет\n31-49 лет       0.00014   -        \n50 лет и старше 0.00428   0.18172  \n\nP value adjustment method: BH \n\n\n\nlibrary(ggstatsplot)\nggbetweenstats(\n  df,\n  x    = age_cats3,\n  y    = V18_mean,\n  type = \"non-parametric\",\n  xlab = \"Возраст\"\n)\n\n\n\n\n\n\n\nРисунок 10.17: Визуализация результатов непараметрического однофакторного дисперсионного анализа (ggstatsplot)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Статистический вывод и тестирование исследовательских гипотез</span>"
    ]
  },
  {
    "objectID": "Statistical-Inference.html#анализ-зависимых-выборок",
    "href": "Statistical-Inference.html#анализ-зависимых-выборок",
    "title": "10  Статистический вывод и тестирование исследовательских гипотез",
    "section": "10.4 Анализ зависимых выборок",
    "text": "10.4 Анализ зависимых выборок\nНапомним, что зависимые (иногда их еще называют связанные, парные) выборки представляют собой данные, в которых каждому наблюдению в одном датасете соответствует другое, связанное с ним наблюдение в другом наборе данных.\nНапример, могут сравниваться:\n\nизмерения в разные периоды времени (до и после лечения, обучения, смены работы и пр.)\nусловия, в которых были проведены измерения (лекарство натощак или после завтрака, выполнение теста с предварительным тренингом и без, результаты адаптации мигрантов через программу интеграции и без нее и др.)\nрезультаты разных измерений (разные тесты тревожности, например)\nспаренные пары, имеющие сходные характеристики (например, при сравнении результатов различных образовательных программ).\n\nДля анализа результатов таких исследований требуются особые статистические методы, которые мы будем обсуждать в данном разделе.\n\n\n\n\n\n\nРисунок 10.18: Связь между наблюдениями в зависимых выборках. Источник: www.statology.org\n\n\n\n\n10.4.1 t-критерий для зависимых выборок\nАналогично ситуациям, рассмотренным выше, t-критерий для зависимых выборок также используется для того, чтобы сравнить средние значения, но при этом предполагается, что группы связаны друг с другом каким-то образом. Чаще всего речь идет об одних и тех же испытуемых (респондентах, наблюдениях), у которых измеряется один и тот же показатель через некоторый промежуток времени. Однако, это могут быть и разные люди (объекты исследования), но выборки все равно проектируются как связанные.\n\n\n\n\n\n\nРисунок 10.19: Отличия между различными критериями\n\n\n\nБудучи параметрической процедурой, в которой оцениваются неизвестные параметры, t-критерий основывается на некоторых допущениях, проверка которых обычно осуществляется перед проверкой статистических гипотез. Поскольку речь идет о зависимых выборках, в качестве наблюдений учитываются не исходные данные, а различия между двумя наборами значений, и каждое допущение касается именно этих различий, а не исходных данных. Таких допущений четыре:\n\nЗависимая переменная должна быть непрерывной (интервальная шкала / шкала отношений). В отдельных случаях могут использоваться дискретные шкалы (например, в ходе измерения социальных установок с помощью шкал Лайкерта).\nНаблюдения должны быть независимы друг от друга (различия между парами по одной строке не должны зависеть от различий по другим строкам).\nЗависимая переменная (то есть различия между значениями) должна иметь нормальное распределение.\nУ зависимой переменной не должно быть выбросов.\n\nКак всегда, формулируем гипотезы:\n\\(H_0:μ_d=0\\) – разница в средних значениях двух спаренных выборок равна нулю\n\\(H_1:μ_d≠0\\) - (двухсторонний) разница в средних значениях двух выборок – не равна нулю \\(H_1:μ_d&gt;0\\) - (правосторонний) – разница больше нуля\n\\(H_1:μ_d&gt;0\\) - (левосторонний) – разница меньше нуля\nПорядок вычислений:\n\nПосчитать различия в значениях между двумя измерениями – \\(D\\)\nПосчитать среднее значение получившихся различий \\(d_1, d_2, … , d_n\\):\n\n\\[\\bar{d}=\\frac{d_1+d_2+...+ d_n}{n}\\] - Посчитать значение t-критерия по формуле:\n\\[t = \\frac{\\bar{d}}{s/\\sqrt{n}},\\] где \\(s\\) - стандартное отклонение различий (\\(d\\)), \\(n\\) - объем выборки (количество пар значений, по которым высчитываются \\(d\\))/\nДалее мы можем вычислить p-значение для абсолютного значения статистики критерия (\\(|t|\\)) на основе сведений о количестве степеней свободы (\\(df\\)): \\(df=n−1\\) (приблизительно с помощью таблиц критических значений или точно на основе специальной программы).\n\n Пример: Одной из извечных российских проблем являются дороги. Для повышения безопасности и комфорта дорожного движения созден специальный национальный проект - «Безопасные качественные дороги», постоянно проводятся профилактические мероприятия, направленные на повышение уровня информированности граждан о правилах дорожного движения, ответственности и сознательности, как водителей, так и пешеходов. Допустим, мы хотели бы выяснить, как меняется ситуация на дорогах, происходит ли снижение количества погибших в ДТП за последние пять лет.\n\nСкачать данные\nЗагрузим данные Росстата о количестве погибших в ДТП на 100 тыс. населения:\n\nlibrary(readxl)\ndtp&lt;-read_excel(\"dtp.xls\")\n\nВычислим различия между двумя рядами значений и сделаем проверку на нормальность - графически и с помощью теста Шапиро-Уилка.\n\nnames(dtp)&lt;-c(\"region\", \"dtp2018\", \"dtp2023\")\ndtp$d&lt;-dtp$dtp2018-dtp$dtp2023\n# Сделаем гистограмму\nhist(dtp$d)\n\n\n\n\n\n\n\nshapiro.test(dtp$d)\n\n\n    Shapiro-Wilk normality test\n\ndata:  dtp$d\nW = 0.87308, p-value = 4.961e-07\n\n\nДа, пока результаты не радуют, мы видим, что есть существенные отклонения от нормальности. Если посмотреть на данные, то можно увидеть, что ситуацию осложняет Республика Калмыкия, где произошло большое увеличение количества ДТП. Попробуем убрать этот регион и посмотреть, что получится.\n\nlibrary(dplyr)\ndtp2&lt;-dtp %&gt;% \n  filter(region!=\"Республика Калмыкия\")\nhist(dtp2$d)\n\n\n\n\n\n\n\nshapiro.test(dtp2$d)\n\n\n    Shapiro-Wilk normality test\n\ndata:  dtp2$d\nW = 0.989, p-value = 0.6942\n\n\nСовсем другое дело!\nВ нашей таблице данные не совсем «чистые», один и тот же показатель указан в двух столбцах, однако такая ситуация встречается довольно часто.\nВ целом, функция останется прежней, но к ней добавится аргумент paired = TRUE, который и позволит провести измерения с зависимыми выборками:\n\nres &lt;- t.test(dtp2$dtp2018, dtp2$dtp2023, paired = TRUE) \nres\n\n\n    Paired t-test\n\ndata:  dtp2$dtp2018 and dtp2$dtp2023\nt = 8.6723, df = 84, p-value = 2.711e-13\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 1.938852 3.092597\nsample estimates:\nmean difference \n       2.515725 \n\n\nПоскольку мы сравниваем 2018 год и 2023, то положительная средняя разница означает, что в среднем количество погибших в ДТП сокращается. Статистика t-критерия (8,67) при количестве степеней свободы 84 указывает на достоверные различия (p&lt;0,001).\nДля того, чтобы визуализировать отличия, переструктурируем наши данные в «длинный формат» таким образом, чтобы вся информация о ДТП была в одном столбце, а период, в который собирались статистические данные, - в другом.\n\nlibrary(tidyr)\ndtp3&lt;-dtp2 |&gt;  \n  pivot_longer(cols=dtp2018:dtp2023, \nnames_to=\"year\",\nvalues_to = \"dtp_number\")\n\nВоспользуемся функцией ggwithinstats из пакета ggstatsplot`, чтобы отобразить различия между периодами:\n\nlibrary(ggstatsplot)\nggwithinstats(\n  data = dtp3,\n  x = year,\n  y = dtp_number\n)\n\n\n\n\n\n\n\n\n\n\n10.4.2 Критерий Уилкоксона для зависимых выборок\nЕсли все же проблемы с распределением непреодолимы или данные не являются непрерывными, лучше воспользоваться непараметрическим аналогом для зависимых выборок - критерием знаковых рангов Уилкоксона.\nВыше мы уже разбирали формулу данного критерия, здесь же подчеркнем, что мы будем сравнивать два распределения, соответствующие одной и той же группе респондентов (объектов, наблюдений).\n\n\n\n\n\n\nРисунок 10.20\n\n\n\nПрименим тест Уилкоксона для анализа данных о погибших в ДТП. Попробуем воспроизвести все действия вручную:\n\ndtp2$rank&lt;-rank(abs(dtp2$d),  ties.method = \"average\")# посчитаем ранги\ndtp2$sign&lt;-sign(dtp2$d)#найдем знак различий\ndtp2$signed_ranks&lt;-dtp2$rank*dtp2$sign#умножим ранги на знак\n\nПосчитаем сумму положительных рангов:\n\ndtp2 %&gt;% \n  filter(signed_ranks&gt;0) %&gt;% \n  summarise(sum_ranks=sum(signed_ranks))\n\n# A tibble: 1 × 1\n  sum_ranks\n      &lt;dbl&gt;\n1      3321\n\n\nВ R мы можем использовать знакомую уже нам функцию wilcox.test(), но указываем аргумент paired=TRUE.\n\nwilcox.test(dtp2$dtp2018, dtp2$dtp2023, paired = TRUE, alternative = \"two.sided\")\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  dtp2$dtp2018 and dtp2$dtp2023\nV = 3321, p-value = 6.071e-11\nalternative hypothesis: true location shift is not equal to 0\n\n\nВизуализируем это:\n\nggwithinstats(\n  data = dtp3,\n  x = year,\n  y = dtp_number,\n  type = \"non-parametric\"\n)\n\n\n\n\n\n\n\n\n\n\n10.4.3 ANOVA с повторными измерениями\nДисперсионный анализ с повторными измерениями расширяет идею t-критерия для зависимых выборок на случай, когда количество сравниваемых групп превышает две.\nВычисления сходны с теми, что мы рассматривали на примере «обычной» однофакторной ANOVA.\nРазличия будут заключаться в том, как мы будем подсчитывать среднюю межгрупповую сумму квадратов отклонений, для повторных измерений у в качестве группирующей переменной будет использоваться фактор времени:\n\\[F=\\frac{MS_{time}}{MS_{error}}\\]\nили фактор условия: \\[F=\\frac{MS_{condition}}{MS_{error}}\\]\nНапример, мы хотим сравнить значения, соответствующие различным временным отрезкам.\nТогда разница группового (внутри отдельных временных диапазонов) и общего среднего составит:\n\\[S_{time}=SS_b=\\sum_{i=1}^{k}(\\bar{x_i}-\\bar{x})^2\\] Разница индивидуальных значений и групповых средних:\n\\[SSw=\\sum_{1}(x_{i1}-\\bar{x_1})^2+\\sum_{2}(x_{i2}-\\bar{x_2})^2+ ... + \\sum_{k}(x_{ik}-\\bar{x_k})^2\\] Разница средних по всем измерениям и общего среднего, k – количество уровней:\n\\[SS_{subjects}=k*\\sum(\\bar{x_i}-\\bar{x})^2\\]\nСумма квадратов ошибки:\n\\[SS_{error}=SS_w-SS_{subjects}\\]\nТогда средний квадрат отклонений по времени будет:\n\\[MS_{time}=\\frac{SS_{time}}{k-1}\\] Соответственно, будет определяться и средняя ошибка:\n\\[MS_{error}=\\frac{SS_{error}}{(n-1)(k-1)}\\]\nРассмотрим возможности применения данного метода на следующем примере:\n\nПример: В исследовании изучалось влияние на восприятие алкогольных напитков различной информации. Сравнивались три напитка – вода, вино и пиво. Участники эксперимента смотрели положительные, нейтральные и негативные информационные материалы, а затем оценивали свое отношение к напитку по шкале от -100 до 100.\n\nСкачать данные\n\n\n\n\n\n\n\n\n\n\nРисунок 10.21: Примерно так могли выглядеть информационные материалы эксперимента\n\n\n\n\nalcohol_attitudes&lt;-read.csv(\"Alcohol_Attitudes.csv\")\n\nНаши данные представлены в «широком» формате, переведем их в длинный, оставив только переменные, касающиеся пива:\n\nalc_long&lt;-alcohol_attitudes |&gt;   \n  select(participant, contains(\"beer\")) |&gt;  \n  pivot_longer(cols=contains(\"beer\"), names_to = \"emotion\", values_to = \"score\")\n\nПосмотрим описательные статистики по группам:\n\nalc_long |&gt; \n  group_by(emotion) |&gt; \n  summarise(n=n(), mean=mean(score), sd=sd(score))\n\n# A tibble: 3 × 4\n  emotion      n  mean    sd\n  &lt;chr&gt;    &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 beerneg     20  4.45  17.3\n2 beerneut    20 10     10.3\n3 beerpos     20 21.0   13.0\n\n\nВидим, что оценки разные и следуют логике увеличения по мере смены эмоции с негативной на позитивную.\nСделаем проверку на нормальность:\n\nlibrary(rstatix)\nalc_long |&gt; \n  group_by(emotion) |&gt; \n  shapiro_test(score)\n\n# A tibble: 3 × 4\n  emotion  variable statistic      p\n  &lt;chr&gt;    &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n1 beerneg  score        0.896 0.0354\n2 beerneut score        0.961 0.573 \n3 beerpos  score        0.951 0.375 \n\n\nВидим, что в группе с негативной эмоцией есть отклонения от нормальности, но мы пока закроем на это глаза.\nСопроводим графической проверкой:\n\nggqqplot(alc_long, \"score\", facet.by = \"emotion\")\n\n\n\n\n\n\n\n\n\nres.aov &lt;- anova_test(data = alc_long, dv = score, wid=participant, within = emotion)\nget_anova_table(res.aov)\n\nANOVA Table (type III tests)\n\n   Effect DFn DFd      F        p p&lt;.05   ges\n1 emotion   2  38 17.574 3.95e-06     * 0.207\n\n\nРезультаты показывают, что оценки отношения к напитку были статистически различными в разных условиях эксперимента, F(2, 38) = 17.57 p &lt; 0.001, однако обобщенная величина эффекта не очень велика eta2[g] = 0.21.\nПроведем парные сравнения:\n\npwc &lt;- alc_long |&gt; \n  pairwise_t_test(\n    score ~ emotion, paired = TRUE,\n    p.adjust.method = \"bonferroni\"\n    )\npwc\n\n# A tibble: 3 × 10\n  .y.   group1   group2    n1    n2 statistic    df       p   p.adj p.adj.signif\n* &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;       \n1 score beerneg  beern…    20    20     -2.13    19 4.6 e-2 1.39e-1 ns          \n2 score beerneg  beerp…    20    20     -5.12    19 6.02e-5 1.81e-4 ***         \n3 score beerneut beerp…    20    20     -4.14    19 5.51e-4 2   e-3 **          \n\n\nРазличия значимы между оценками, полученными в результате предъявления позитивных материалов, и двумя другими группами (нейтральной и негативной), тогда как различия между негативными и нейтральными материалами не значимы.\nДобавим результаты статистического анализа в исходный график:\n\npwc &lt;- pwc %&gt;% add_xy_position(x = \"emotion\")\nbxp + \n  stat_pvalue_manual(pwc) +\n  labs(\n    subtitle = get_test_label(res.aov, detailed = TRUE),\n    caption = get_pwc_label(pwc)\n  )\n\nNULL\n\n\nНу, или вот так:\n\nggwithinstats(\n  data = alc_long,\n  x = emotion,\n  y = score\n)\n\n\n\n\n\n\n\n\n\n\n10.4.4 Критерий Фридмана - непараметрический аналог дисперсионного анализа для зависимых выборок\nТест Фридмана – разработан Милтоном Фридманом, американским экономистом и нобелевским лауреатом.\n\n\n\n\n\n\nРисунок 10.22: Милтон Фридман - нобелевский лауреат за исследования в области потребления, монетаризма и политики стабилизации\n\n\n\nЯвляется аналогом ANOVA с повторными измерениями и непараметрического дисперсионного анализа для связанных выборок.\nАлгоритм следующий:\n\nпредставить данные в виде матрицы $ {x_{ij} }_{n*k}$ с \\(n\\) количеством строк и \\(k\\) количеством столбцов\nзаменить исходные значения на ранги (\\(r\\)) по каждой строке.\nпосчитать сумму рангов (\\(R\\)) по каждому столбцу\nподставить значения в формулу:\n\n\\[Q=\\frac{12}{N*k*(k+1)}*\\sum R^2+(3*N*(k+1))\\]\n\nВ тех случаях, когде \\(n\\) или \\(k\\) достаточно большие (например, \\(n&gt;15\\) или \\(k&gt;4\\) статистика критерия может быть аппроксимирована распределением \\(\\chi^2\\). Если \\(n\\) или \\(k\\) маленькие, статистика \\(\\chi^2\\) может быть некорректной и лучше воспользоваться специальными таблицами.\n\n\nres.fried &lt;- alc_long %&gt;% friedman_test(score ~ emotion |participant)\nres.fried\n\n# A tibble: 1 × 6\n  .y.       n statistic    df        p method       \n* &lt;chr&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;        \n1 score    20      15.1     2 0.000521 Friedman test\n\n\nДля определения величины эффекта можно воспользоваться критерием W Кендалла:\n\\[W=\\frac{Q}{N(k-1)},\\]\nгде \\(Q\\) - статистика теста по Фридману, \\(N\\) объем выборки, \\(k\\) количество измерений по каждому субъекту (M. T. Tomczak and Tomczak 2014).\nЗначение коэффициента \\(W\\) может принимать значения от 0 (отсутствие взаимосвязи между оценками и уровням измерений) до 1 (тесная взаимосвязь).\nИнтерпретация коэффициента аналогична \\(d\\) Коэна:\n\n0.1 - &lt; 0.3 (маленький эффект)\n0.3 - &lt; 0.5 (средний эффект)\n\n= 0.5 (большой эффект).\n\n\nДоверительные интервалы рассчитываются с помощью бутстрэпа (многократного расщепления выборки на наборы данных).\n\nalc_long %&gt;% friedman_effsize(score ~ emotion |participant)\n\n# A tibble: 1 × 5\n  .y.       n effsize method    magnitude\n* &lt;chr&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;     &lt;ord&gt;    \n1 score    20   0.378 Kendall W moderate \n\n\nВ данном случае наблюдается средний эффект.\nПроведем парные сравнения с помощью теста Уилкоксона:\n\npwc &lt;- alc_long %&gt;%\n  wilcox_test(score ~ emotion, paired = TRUE, p.adjust.method = \"bonferroni\")\npwc\n\n# A tibble: 3 × 9\n  .y.   group1   group2      n1    n2 statistic        p p.adj p.adj.signif\n* &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;    &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;       \n1 score beerneg  beerneut    20    20      37   0.036    0.109 ns          \n2 score beerneg  beerpos     20    20       6   0.000579 0.002 **          \n3 score beerneut beerpos     20    20       7.5 0.00046  0.001 **          \n\n\nРезультаты аналогичны тем, что были найдены в ходе применения параметрических процедур.\nВизуализируем это:\n\npwc &lt;- pwc %&gt;% add_xy_position(x = \"emotion\")\nggboxplot(alc_long, x = \"emotion\", y = \"score\", fill=\"emotion\", add = \"point\") +\n  stat_pvalue_manual(pwc) +\n  labs(\n    subtitle = get_test_label(res.fried,  detailed = TRUE),\n    caption = get_pwc_label(pwc)\n  )\n\n\n\n\n\n\n\n\nВторой вариант визуализации:\n\nggwithinstats(\n  data = alc_long,\n  x = emotion,\n  y = score,\n  type=\"non-parametric\"\n)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Статистический вывод и тестирование исследовательских гипотез</span>"
    ]
  },
  {
    "objectID": "Statistical-Inference.html#анализ-качественных-данных",
    "href": "Statistical-Inference.html#анализ-качественных-данных",
    "title": "10  Статистический вывод и тестирование исследовательских гипотез",
    "section": "10.5 Анализ качественных данных",
    "text": "10.5 Анализ качественных данных\nРассмотрев случаи, когда зависимая переменная являлась количественной, перейдем к большому классу исследовательских ситуаций, когда мы исследуем взаимосвязи между категориальными показателями.\nК слову, это огромное количество случаев, встречающихся в ходе анализа социологических данных.\n\n10.5.1 Сравниваем независимые группы\n\n10.5.1.1 Одновыборочный z-критерий: сравниваем значение одной доли с теоретическим\nПроводя социологическое исследование, мы можем быть заинтересованы в том, чтобы сравнить результаты, полученные на нашей выборочной с какими-либо предполагаемыми результатами, выраженными в виде пропорции.\n\nПример: Мы исследуем оценки жителей одного из городских кварталов относительно того, согласны ли они, чтобы рядом с их домами оборудовали большой парк, с велодорожками и прочими благами цивилизации. В этот же момент времени предполагается благоустройство и других объектов, поэтому городские власти бы выбрать именно этот объект для инвестиций, но только в том случае, если оценки населения будут благоприятны и составят не менее 70%. Проведя опрос среди 500 жителей мы обнаружили, что 370 из них выразили согласие с обустройством парка, тогда как 130 человек по каким-то причинам были против парка. Превышает ли количество жителей допустимый предел?\n\nПомочь в решении нашей задачи может z-критерий для одной пропорции.\nКак и в других случаях мы сформулируем гипотезы:\nНулевая гипотеза:\n\n\\(H_0:p_o=p_e\\)\n\nАльтернативные гипотезы:\n\n\\(H_1:p_o\\neq p_e\\)\n\\(H_1:p_o&gt;p_e\\)\n\\(H_1:p_o&lt;p_e\\)\n\nСтатистика критерия:\n\\[z = \\frac{p_o-p_e}{\\sqrt{p_oq/n}},\\]\nгде:\n\n\\(p_o\\) наблюдаемая доля\n\\(q=1−p_o\\)\n\\(p_e\\) ожидаемая доля\n\\(n\\) объем выборки.\n\n\nres &lt;- prop.test(x = 370, n = 500, p = 0.7, alternative = \"two.sided\")\nres \n\n\n    1-sample proportions test with continuity correction\n\ndata:  370 out of 500, null probability 0.7\nX-squared = 3.6214, df = 1, p-value = 0.05704\nalternative hypothesis: true p is not equal to 0.7\n95 percent confidence interval:\n 0.6987864 0.7774592\nsample estimates:\n   p \n0.74 \n\n\n\n\n\n\n\n\n\n\n\n\n\n10.5.1.2 Сравнение двух долей (пропорций) - z-критерий\nМы можем распространить наши рассуждения на тот случай, когда нам требуется сравнить две пропорции, в каких-то группах.\nВ этом случае критерий несколько преображается:\n\\[z = \\frac{p_1-p_2}{\\sqrt{pq/n_1+pq/n_2}},\\] где:\n\n\\(p_1\\) доля, наблюдаемая в группе 1, имеющей размер \\(n_1\\)\n\\(p_2\\) доля, наблюдаемая в группе 2, имеющей размер \\(n_2\\)\n\\(p\\) и \\(q\\) - общие пропорции по всей выборке\n\\(q=1−p\\)\n\nДопустим, мы анализируем результаты исследования по климату, и нам хотелось бы знать, как относятся к климатическим изменениям мужчины и женщины. Если точнее, считают ли мужчины более опасным проживание вблизи тающих ледников, чем женщины (или наоборот)?\n\ndf$V1&lt;-sjlabelled::as_label(df$V1)\nprop.table(table(df$V19, df$V1), margin = 2)\n\n   \n      Мужской   Женский\n  1 0.4178571 0.6135182\n  2 0.5821429 0.3864818\n\n\nНа первый взгляд кажется, что женщины, действительно, воспринимают риски тающих ледников как более серьезные. Но как это доказать?\n\nlibrary(infer)\ndf&lt;-df %&gt;% \n  mutate(V19=case_when(\n    V19==1 ~ \"Да\",\n    V19==2~\"Нет\"\n  ))\npr_test&lt;-prop_test(df, V19 ~ V1, order = c(\"Мужской\", \"Женский\"))\npr_test\n\n# A tibble: 1 × 6\n  statistic chisq_df     p_value alternative lower_ci upper_ci\n      &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;\n1      28.4        1 0.000000100 two.sided     -0.268   -0.123\n\n\n\n\n10.5.1.3 Наш любимый хи-квадрат\nКритерий \\(\\chi^2\\) Пирсона – один из самых известных тестов для анализа категориальных данных. Основан на сравнении наблюдаемых и ожидаемых частот.\n\\[\\chi^2=\\sum\\frac{(O-E)^2}{E},\\]\nГде: - \\(O\\) - наблюдаемые значения - \\(E\\) - ожидаемые значения (которые были бы в том случае, если бы никакой взаимосвязи между частотами не было)\nГипотезы:\n\n\\(H_0\\) – между зависимой и независимой переменной нет взаимосвязи – наблюдаемые частоты не отклоняются от ожидаемых частот.\n\\(H_1\\)- между зависимой и зависимой переменной есть взаимосвязь, наблюдаемые частоты значимо отклоняются от ожидаемых.\n\nУсловия применения статистического критерия хи-квадрата Пирсона\n\nТип данных: параметры должны быть качественными целочисленными частотами, измеренными в номинальной шкале:\n\nбинарными (пол: мужской/женский) или с большим количеством градаций (регион, группа - обучения, специальность)\nпорядковыми (степень артериальной гипертензии)\n\nКоличество наблюдений более 20\n\nОжидаемая частота, соответствующая нулевой гипотезе должна быть более 5, если ожидаемое явление принимает значение менее 5, то необходимо использовать точный Критерий Фишера.\nДля четырехпольных таблиц (2х2): Если ожидаемое значение принимает значение менее 10 (а именно 5&lt;x&lt;10), необходим расчет поправки Йетса.\n\nСравниваемые группы должны быть примерно одного размера.\nСопоставляемые группы должны быть независимыми (то есть единицы наблюдения не зависят друг от друга). Для парных сравнений (типа «до-после» существует отдельный тест МакНемара (McNemar).\n\n\nЗапрещается: использовать хи-квадрат для анализа непрерывных абсолютных данных, процентов и долей без предварительной перекодировки!\n\n\nchisq.test(df$V1, df$V19)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  df$V1 and df$V19\nX-squared = 28.37, df = 1, p-value = 1.002e-07\n\n\nОдной из наиболее удачных идей визуализации таблиц сопряженности являются мозаичные диаграммы:\n\nlibrary(vcd)\n options(OutDec= \",\") \n#создадим базовую таблицу\nM&lt;- table(df$V19, df$V1)\n#подпишем оси\nnames(dimnames(M)) = c(\"Опасны ли тающие ледники?\",\"Пол\")\n#создадим таблицу с надписями\nlabs &lt;- round(prop.table(M,margin=2)*100, 1)\n#обычная мозаичная диаграмма\nmosaic(M, pop = FALSE, shade = TRUE)\n#добавим значения в ячейках\nlabeling_cells(text = labs, margin = 0)(M)\n\n\n\n\n\n\n\n\nЕще один вариант - ассоциативный (структурный) график:\n\nstruct &lt;- structable(~ V19 + V1, data = df)\nassoc(struct, data = df, shade=T, labeling_args = list(set_varnames = c(V1 = \"Пол\", V19=\"Опасны ли тающие ледники?\")))\n\n\n\n\n\n\n\n\n\n\n\n10.5.2 Сравнение зависимых выборок\n\n10.5.2.1 Тест МакНемара\nТест Макнемара используется для определения наличия статистически значимой разницы в пропорциях для парных номинальных данных, представленных в таблицах 2 × 2.\nНазван в честь Куинна Майкла Макнемара (1900 –1986), американского психолога и статистика, президента Американской психологической ассоциации (1964), разработавшего критерий в 1947 году.\n\nТест МакНемара. Формула и процедура оценки\n\n\n\n\n\n\nРисунок 10.23: Типичная таблица для проведения теста МакНемара\n\n\n\nТест основывается на нулевой гипотезе (\\(H_0\\)) о маргинальных частотах: маргинальные вероятности для каждого исхода равны:\n\\(p_a + p_b = p_a + p_c\\) и \\(p_c + p_d = p_b + p_d\\).\n\n\\(H_0: p_b=p_c\\)\n\\(H_1: p_b\\neq p_c\\)\n\n\\[\\chi^2=\\frac{(b-c)^2}{b+c}\\]\n\n Пример: рассмотрим пример из области общественного здравоохранения. В нашем наболе содержатся данные о страховых компаниях и покрытиях страховыми полисами услуг по оказании инфузионной терапии (часто применяется при раковых заболеваниях). Вопрос: меняется ли количество страховщиков, включающих в полис данную услугу?\n\nСкачать данные\nЗагрузим данные:\n\nload(\"dataset-him-2014-2016-subset2.Rdata\")\n\nСоздадим таблицу по данным 2014 и 2016 гг.:\n\ntable_for_McNemar &lt;-table(data$X.2014, data$X.2016c)\ntable_for_McNemar\n\n     \n      No  Yes\n  No   22 105\n  Yes  54 145\n\n\nДовольно заметно, что количество страховых компаний, покрывающих своими полисами инфузионную терапию за два года выросло довольно существенно: 105 компаний, которые в 2014 году отказывались от этой опции, в 2016 году включили ее в список услуг. Обратная ситуация встречается в два раза реже: только 54 организации, которые в 2014 году оплачивали стоимость инфузионной терапии по страховке, в 2016 году отказались это делать.\nПроведем тест МакНемара:\n\nmcnemar.test(table_for_McNemar)\n\n\n    McNemar's Chi-squared test with continuity correction\n\ndata:  table_for_McNemar\nMcNemar's chi-squared = 15,723, df = 1, p-value = 7,332e-05\n\n\nТест показывает наличие значимых отличий.\n\n\n10.5.2.2 Тест Кохрана\nЕсли количество сравниваемых групп больше двух, то используется расширение теста Макнемара - Q-критерий Кохрана.\nНулевая гипотеза (\\(H_0\\)): доли (пропорции) «успеха» во всех группах равны. Предполагается, что данные организованы в «блоки». «Блоками» могут быть, например, отдельные люди.\n\nФормула теста:\n где:\n\n\\(b\\) = количество блоков,\n\\(k\\) = количество условий,\n\\(X•j\\) = сумма по столбцу для условия j,\n\\(Xi•\\) = сумма по строке для блока i\n\\(N\\) = общая сумма.\n\nПродолжим работу с предыдущим примером.\nПереведем данные из широкого в длинный формат и отберем данные без :\n\ndata&lt;-data %&gt;% \n  select(-ends_with(\"c\")) %&gt;% \n  pivot_longer(cols = contains(\"X.\"), values_to = \"outcome\", names_to = \"year\")\n\nПроведем тест Кохрана:\n\ncochran_qtest(data, outcome ~ year|IssuerId)\n\n# A tibble: 1 × 6\n  .y.         n statistic    df            p method          \n* &lt;chr&gt;   &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt; &lt;chr&gt;           \n1 outcome   326      33.2     2 0.0000000609 Cochran's Q test\n\n\nВыявлены значимые различия между периодами. Но какие? Как и всегда с множеством групп, необходимы парные сравнения.\nСделаем сначала общую таблицу по трем периодам:\n\ndata %&gt;% \n  group_by(year, outcome) %&gt;% \n  summarise(sum=n()) %&gt;% \n  pivot_wider(names_from = outcome, values_from = sum)\n\n# A tibble: 3 × 3\n# Groups:   year [3]\n  year   `No `   Yes\n  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;\n1 X.2014   127   199\n2 X.2015    69   257\n3 X.2016    76   250\n\n\nТеперь проведем парные сравнения, с помощью теста МакНемара:\n\npairwise_mcnemar_test(data, outcome ~ year|IssuerId)\n\n# A tibble: 3 × 6\n  group1 group2            p        p.adj p.adj.signif method      \n* &lt;chr&gt;  &lt;chr&gt;         &lt;dbl&gt;        &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;       \n1 X.2014 X.2015 0.0000000166 0.0000000498 ****         McNemar test\n2 X.2014 X.2016 0.0000733    0.00022      ***          McNemar test\n3 X.2015 X.2016 0.55         1            ns           McNemar test\n\n\nРезультаты сравнительного анализа показывают, что наиболее важный скачок произошел в 2015 году, когда количество организаций увеличилось на 58, в последующий год их количество даже сократилось, но это сокращение не является статистически значимым.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Статистический вывод и тестирование исследовательских гипотез</span>"
    ]
  },
  {
    "objectID": "Statistical-Inference.html#самостоятельная-работа",
    "href": "Statistical-Inference.html#самостоятельная-работа",
    "title": "10  Статистический вывод и тестирование исследовательских гипотез",
    "section": "10.6 Самостоятельная работа",
    "text": "10.6 Самостоятельная работа\n\nПроанализируйте различия в оценках уверенности в нахождении заработка в случае потери актуального места работы (вопрос В21) в зависимости от образовательного уровня респондента (вопрос А6), перекодировав его в две категории - те, кто имеют высшее образование, и кто его не имеет.\nИспользуя созданную переменную важности соблюдения традиций, проведите анализ по уровню образования (по переменной, созданной в упр.1) и по самооценке материального положения (вопрос А10), создав три категории - «низкие доходы», «средние доходы» и «высокие доходы, обеспеченные граждане». Используйте параметрические методы там, где это уместно, и непараметрические там, где возможно применение только таких методов. Обязательно создайте визуализации к вашим результатам.\nПровести анализ оценки лиц, придерживающихся определенной диеты, за три промежутка времени, используя параметрический ANOVA и его непараметрический аналог с парными сравнениями. Сделать соответствующие визуализации.\n\nДанные:\n\nlibrary(datarium )\ndata(\"selfesteem\")\n\n\nПровести анализ таблиц сопряженности по исследованию по климату (две таблицы на ваш выбор) с помощью критерия хи-квадрат. Создать мозаичную диаграмму и ассоциативный график.\nОформите ваши результаты любым удобным для вас способом и приложите в качестве ответа на задание.\n\n\n\n\n\nBrown, Angus. «The strange origins of the Student’s t-test — physoc.org». https://www.physoc.org/magazine-articles/the-strange-origins-of-the-students-t-test/.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Статистический вывод и тестирование исследовательских гипотез</span>"
    ]
  },
  {
    "objectID": "ImportExport.html",
    "href": "ImportExport.html",
    "title": "6  Импорт и экспорт данных",
    "section": "",
    "text": "6.1 Импорт и экспорт файлов Excel\nИмпортировать данные из файла Excel можно несколькими способами.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Импорт и экспорт данных</span>"
    ]
  },
  {
    "objectID": "ImportExport.html#импорт-и-экспорт-файлов-excel",
    "href": "ImportExport.html#импорт-и-экспорт-файлов-excel",
    "title": "6  Импорт и экспорт данных",
    "section": "",
    "text": "Важно!: сохраните все файлы с дополнительными материалами к заданию в ту же директорию, где будете работать (лучше всего создать новый проект под это занятие). Это нужно для того, чтобы лишний раз не прописывать путь к файлу).\nЕще один удобный ход - установить в настройках рабочую директорию в ту же папку, где лежит файл со скриптом:\nSession - Set Working Directory - To Source File Location",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Импорт и экспорт данных</span>"
    ]
  },
  {
    "objectID": "ImportExport.html#способ-1.-скопировать-из-excel-и-сохранить-в-r.",
    "href": "ImportExport.html#способ-1.-скопировать-из-excel-и-сохранить-в-r.",
    "title": "6  Импорт и экспорт данных",
    "section": "6.2 Способ 1. Скопировать из Excel и сохранить в R.",
    "text": "6.2 Способ 1. Скопировать из Excel и сохранить в R.\nДля того, чтобы воспользоваться этим способом, сохраните файл, откройте его, выделите все данные и скопируйте их (Ctrl+C). Затем запустите код ниже:\n\nmy_data &lt;- read.table(file = \"clipboard\", sep = \"\\t\", header=TRUE)\n\nЕсли все сделано правильно, то у Вас в рабочем окружении появится объект my_data, в котором будет 9 наблюдений и 14 переменных (это результаты реализации государственной программы содействия добровольному переселению соотечественников). Чтобы увидеть данные, нажмите на него и он откроется в просмотрщике.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Импорт и экспорт данных</span>"
    ]
  },
  {
    "objectID": "ImportExport.html#способ-2.-импорт-из-файла-excel.",
    "href": "ImportExport.html#способ-2.-импорт-из-файла-excel.",
    "title": "6  Импорт и экспорт данных",
    "section": "6.3 Способ 2. Импорт из файла Excel.",
    "text": "6.3 Способ 2. Импорт из файла Excel.\nПредположим, файл Excel у нас большой, содержит несколько листов, скопировать все и вставить - не очень удачная идея. В этом случае (честно говоря, такие случаи случаются гораздо чаще, чем описанные в способе 1), нам нужно импортировать данные из файла.\nЭто сделать не так сложно, к счастью, у нас есть библиотека readxl, которая как раз и предназначена для таких целей.\nПорядок действий следующий:\n\nЗагружаем библиотеку readxl.\nЗапускаем функцию read_excel(file excel, sheet = \"name of sheet\")\n\nДавайте загрузим тот же файл с соотечественниками, что и в примере выше:\n\nlibrary(readxl)\ndf_excel&lt;-read_excel(\"ProgResCompatriots.xlsx\", sheet=1)\n\nВ рабочем окружении отобразился новый объект, его содержание идентично my_data. Чтобы посмотреть - нужно нажать на голубой кружок с белым треугольником рядом с именем:\n\nПамятка по работе с readxl:",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Импорт и экспорт данных</span>"
    ]
  },
  {
    "objectID": "ImportExport.html#экспорт-результатов-в-excel",
    "href": "ImportExport.html#экспорт-результатов-в-excel",
    "title": "6  Импорт и экспорт данных",
    "section": "6.4 Экспорт результатов в Excel",
    "text": "6.4 Экспорт результатов в Excel\nДавайте попробуем создать новую переменную, куда посчитаем среднее количество перехавших за год в период с 2010 по 2021 гг.\n\nlibrary(dplyr)\ndf_excel$Среднее&lt;-apply(df_excel[,2:13],1, mean)\n\nУ нас появился новый столбец в конце таблицы.\nТеперь эту новую таблицу нужно сохранить в новый файл. Для этого нам понадобится библиотека xlsx, а в ней - функция write.xlxs().\nЗапустите следующий код:\n\ninstall.packages(\"xlsx\") #установим библиотеку\nlibrary(xlsx)\nwrite.xlsx(df_excel, \"df_excel_new.xlsx\")",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Импорт и экспорт данных</span>"
    ]
  },
  {
    "objectID": "ImportExport.html#импорт-и-экспорт-файла-в-формате-csv",
    "href": "ImportExport.html#импорт-и-экспорт-файла-в-формате-csv",
    "title": "6  Импорт и экспорт данных",
    "section": "6.5 Импорт и экспорт файла в формате CSV",
    "text": "6.5 Импорт и экспорт файла в формате CSV\nCSV (от англ. Comma-Separated Values — значения, разделённые запятыми) — текстовый формат, предназначенный для представления табличных данных. Строка таблицы соответствует строке текста, которая содержит одно или несколько полей, разделенных запятыми, то есть, по сути, это данные, представленные в текстовом формате.\nСтатистическая информация часто хранится именно в формате CSV, этому формату уже более 40 лет. Текстовые файлы открываются читаются на любом устройстве и в любой среде без дополнительных инструментов. Из-за своих преимуществ CSV — сверхпопулярный формат обмена данными.\nЗа импорт таких файлов отвечает библиотека readr, а в ней - функция read_csv2 (есть и просто read_csv, но там - разделитель действительно запятая, а в read_csv2 - точка с запятой). В чем проблема? Проблема в том, что в России запятая используется в качестве разделителя десятичных разрядов, и когда мы начинаем сохранять в формате csv возникает конфликт. Чтобы его не было, программа вместо запятой использует разделитель - точку с запятой. Это хорошо видно, если посмотреть, как эти данные выглядят в блокноте.\nЕсть и аналогичные функции базового R - read.csv()и read.csv2. Их отличие от функций read_csv2 и read_csvзаключается в быстроте последних, нюансах в обработке строковых переменных (базовые функции принудительно преобразуют их в факторные переменные, тогда как более современные read_csv2 и read_csv оставляют тип character). Есть и некоторые другие отличие, но их обсуждение выходит за рамки данного пособия.\nПорядок действий:\n\nСкачать файл в формате csv.\nОтрыть файл:\n\n\nlibrary(readr)\ndf_csv&lt;-read_csv2(\"ProgResCompatriots.csv\")\n\nУ нас теперь уже три файла с одинаковыми данными). Что с ними делать? Что-нибудь придумаем по ходу занятия…\nНапример, давайте посчитаем прирост соотечественников в 2021 году по сравнению с 2020 годом.\n\ndf_csv$change2021&lt;-df_csv$`2021`-df_csv$`2020`\n\nСохраним наш файл в этом же формате:\n\nwrite.csv2(df_csv, \"df_csv_new.csv\")",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Импорт и экспорт данных</span>"
    ]
  },
  {
    "objectID": "ImportExport.html#импорт-файла-по-url",
    "href": "ImportExport.html#импорт-файла-по-url",
    "title": "6  Импорт и экспорт данных",
    "section": "6.6 Импорт файла по URL",
    "text": "6.6 Импорт файла по URL\nВ Интернете хранится огромное количество различных данных, но с точки зрения импорта можно рассмотреть несколько случаев:\n\nкогда нам нужно просто загрузить файл из Интернета и сохранить его в директорию\nкогда ссылка является прямой и по ней мы можем загрузить данные определенного формата\nкогда у нас есть ссылка на ресурс, в котором содержатся данные вперемешку с текстом, и нам нужно сохранить только табличные данные\n\n\n6.6.1 Загрузка файла в рабочую папку\nВоспользуемся данными, представленными на портале “Открытые данные России” и скачаем оттуда перечень стран и режимов въезда на их территорию.\nМы будем использовать очень простую фунцию download.file(), в которой мы должны указать два аргумента - ссылку и имя файла, в который мы будем сохранять данные. Заметьте, что формат выгружаемого и сохраняемого файла должны совпадать.\n\ndownload.file(\"https://www.isras.ru/files/File/Bank/010%2080055.pdf\", \"anketa.pdf\", mode=\"wb\")#\n\nПосмотрите в своей рабочей директории, скачался ли файл, попробуйте его открыть.\n\n\n6.6.2 Импорт по прямой ссылке в формате csv\nЗдесь тоже нет ничего сложного. Мы уже только что пробовали открывать файлы в формате csv, которые хранятся у нас на компьютере. То же самое происходит с ссылками из Интернета.\nЗагрузим данные об исследованиии по науке и разработкам, представленное на портале статистического Информационного центра при правительстве Новой Зеландии:\n\nSys.getlocale()\nlibrary(readr)\nRnD&lt;-read_csv(\"https://www.stats.govt.nz/assets/Uploads/Research-and-development-survey/Research-and-development-survey-2022/Download-data/research-and-development-survey-2022.csv\")\n\n\n\n6.6.3 Парсинг таблиц из Интернета\nПожалуй, это самое интересное.\nПредположим, мы читаем статью в Википедии, и нам понравились данные, которые там приводятся. Конечно, мы можем скопировать эти данные с помощью мышки, но зачем? Ведь у нас есть R. К тому же результаты такого копирования часто оставляют желать лучшего.\nНапример, возьмем страницу в Википедии, посвященную международному индексу счастья.\nВпрочем, Вы можете взять любую другую страницу.\nДля того, чтобы выгрузить данные непосредственно со страницы, нам понадобится библиотека rvest.\nЗагрузив библиотеку, создадим объект content, в который мы загрузим данные страницы с помощью функции read_html():\n\nlibrary(rvest)\ncontent &lt;- read_html(\"https://ru.wikipedia.org/wiki/%D0%9C%D0%B5%D0%B6%D0%B4%D1%83%D0%BD%D0%B0%D1%80%D0%BE%D0%B4%D0%BD%D1%8B%D0%B9_%D0%B8%D0%BD%D0%B4%D0%B5%D0%BA%D1%81_%D1%81%D1%87%D0%B0%D1%81%D1%82%D1%8C%D1%8F\")\n\nЕсли рассмотреть этот объект, то можно увидеть, что это список, содержащий содержимое страницы по тэгам - head и body, в которых что-то хранится в формате xml.\nДалее, с помощью функции html_table давайте “вытащим” таблицы и сохраним их отдельно с именем tables:\n\ntables &lt;- content %&gt;% html_table(fill = TRUE)\n\nВидим, что таких таблиц 5.\nДавайте сохраним первую из них:\n\ntable1 &lt;- tables[[3]]\n\nА теперь сохраним ее себе на компьютер с помощью знакомой уже функции write.xlsx():\n\nwrite.xlsx(table1, \"HPI.xlsx\")",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Импорт и экспорт данных</span>"
    ]
  },
  {
    "objectID": "ImportExport.html#импорт-файла-из-spss-файл-.sav",
    "href": "ImportExport.html#импорт-файла-из-spss-файл-.sav",
    "title": "6  Импорт и экспорт данных",
    "section": "6.7 Импорт файла из SPSS (файл .sav)",
    "text": "6.7 Импорт файла из SPSS (файл .sav)\nЭто наш любимый формат, поскольку мы с вами специалисты по социологическим исследованиям, любим SPSS и хотели бы соединить возможности этого пакета с возможностями R.\nЧтобы загрузить в R файл в формате .sav, мы должны:\n\nво-первых, загрузить файл. Это результаты недавнего исследования кафедры об изменениях климата в высокогорных районах Алтая.\nво-вторых, нам нужно загрузить библиотеку haven, с помощью которой мы можем импортировать данных из формата программы SPSS в R.\n\n\nlibrary(haven)\n\nПосле того, как мы загрузили библиотекy, импортируем базу данных с помощью функции read_sav:\n\ndf&lt;-read_sav(\"База_КлимРиск_2023.sav\", user_na = TRUE) # user_na позволяет активировать настройки, насающиеся пропущенных значений, определяемых пользователем, например, когда значение 99 закодировано как \"затрудняюсь ответить\" и в базе данных оно установлено как пропущенное.\n\nИтак, наши данные теперь сохранены в датафрейме с именем df. Мы видим, что в нем есть 202 переменных и 913 наблюдений.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Импорт и экспорт данных</span>"
    ]
  },
  {
    "objectID": "ImportExport.html#самостоятельная-работа",
    "href": "ImportExport.html#самостоятельная-работа",
    "title": "6  Импорт и экспорт данных",
    "section": "6.8 Самостоятельная работа",
    "text": "6.8 Самостоятельная работа\nУпражнение 1. Используя портал Открытых данных России, найти интересующие Вас данные в разных форматах - excel, csv. Загрузить данные по ссылке, внести в них изменения (создать новую переменную, что-то поменять, используя R) и сохранить новый файл в аналогичном формате.\nУпражнение 2. С официального сайта Алтайского края скачать Указ Губернатора о присвоении звания ветерана труда и загрузить его в рабочую папку.\nУпражнение 3. С официального сайта Алтайского края, из раздела, посвященного национальной политике, скачать список национально-культурных организаций и сохранить его в формате excel.\nУпражнение 4. Загрузить базу данных своего магистерского исследования из SPSS в R.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Импорт и экспорт данных</span>"
    ]
  },
  {
    "objectID": "About-R.html",
    "href": "About-R.html",
    "title": "1  Основные сведения об R",
    "section": "",
    "text": "1.1 R?\nR – это язык программирования и свободная программная среда для статистической обработки и визуализации данных.\nНесмотря на наличие огромного количества языков и различных программ для статистической обработки данных, R остается популярным языком и средой обработки и анализа данных для специалистов из разных областей знания.\nПозиции языка R постоянно меняются как вся сфера IT-технологий. Одним из инструментов, позволяющих отслеживать интерес к этому языку, является индекс TIOBE Programming Community Он обновляется ежемесячно и отражает спрос на языки на основе количества поисковых запросов в популярных поисковиках, таких как Google, Bing и другие. Индекс помогает оценивать тренды в программировании, хотя не измеряет напрямую количество кода или инженеров. Так, по данным индекса Tiobe за 2022 год, R занимал 12-е место в мире, в 2024 году его авторитет заметно упал (18-е место), но в декабре 2025 года R снова попал в ТОП-10. Так как статистический анализ и необходимость визуализации больших объемов данных стало очень важным, R вернул свою популярность.\nЗдесь можно увидеть, как менялось значение индекса в течение разных периодов времени.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Основные сведения об R</span>"
    ]
  },
  {
    "objectID": "About-R.html#немного-истории",
    "href": "About-R.html#немного-истории",
    "title": "1  Основные сведения об R",
    "section": "1.3 Немного истории",
    "text": "1.3 Немного истории\nR был создан профессорами Россом Айхэка и Робертом Джентельменом (Ross Ihaka и Robert Gentleman) в 1992 году, сначала как язык программирования для обучения студентов статистике в университете Окленда (Новая Зеландия). Авторы вдохновлялись при создании языком S, используемым в лаборатории Bell, и ради шутки назвали язык R - по первым буквам собственных имен.\nВ июне 1995 года статистик Мартин Махлер убедил Айхэку и Джентельмена опубликовать R как язык со свободным исходным кодом под публичной лицензией GNU. Первая официальная версия была выпущена 29 февраля 2000 года.\nЧуть ранее, в 1997 году Куртом Хорником и Фрицем Лейшем была основана Сеть для архивирования кода R (CRAN, The Comprehensive R Archive Network), цель которой заключалась в хранении исходного кода, выполняемых файлов, документации и библиотек, создаваемых пользователями. На момент декабря 2022 года CRAN имел 103 зеркальных сервера и 18 976 библиотек.\nКоманда разработчиков (R Core Team) также была основана в 1997 году для дальнейшего развития языка. Сейчас в ней состоят ведующие разработчики, статистики, специалисты по компьютерным наукам, всего более 20 человек. В апреле 2003 года для развития проекта была создана некоммерческая организация R Foundation. Цель фонда заключается в предоставлении технической поддержки и коммуникации с создателями R, хранении и управлении технической документацией и интеллектуальной собственностью.\nПодробнее об истории создания R можно узнать из таймлайна:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Основные сведения об R</span>"
    ]
  },
  {
    "objectID": "About-R.html#каковы-преимущества-r",
    "href": "About-R.html#каковы-преимущества-r",
    "title": "1  Основные сведения об R",
    "section": "1.4 Каковы преимущества R?",
    "text": "1.4 Каковы преимущества R?\nИх довольно много:\n\nВозможности для статистической обработки, от простых функций до сложных моделей.\n\nПочти все новое, что появляется в области статистики, можно найти в одной из библиотек R. Например, ANZ банк использует R для моделирования невыплат по ипотечному кредитованию, а The Bank Of America применяет R для формирования финансовой отчетности.\n\nЭто язык программирования с открытым исходным кодом (Open-source)\n\nЧто это значит? Это значит, что, во-первых, все написанное на R открыто для изучения и критики, а, во-вторых, каждый может внести вклад в его развитие и улучшение путем создания новых библиотек и новых функций для решения различных задач.\n\nПоддержка сообщества (Community)\n\nУ R - более 2 миллионов пользователей по всему миру, сообщество пользователей R не только внушительное по размеру, но и очень активное. Каким бы ни был ваш проект - учебным или крупномасштабным, всегда найдется тот, кто поможет разобраться в коде и принять правильное решение. Вы тоже можете найти себе единомышленников и подключаться к другим проектам.\nНекоторые полезные ссылки:\n\nhttps://community.rstudio.com/\nhttps://www.r-bloggers.com/\nhttps://stackoverflow.com/questions/tagged/r\nhttps://rweekly.org/ https://www.reddit.com/r/Rlanguage/\nhttps://r.awesome-programming.com/en/awesome/r-language-02/community\n\nКоммьюнити пользователей R это совсем не Man’s World: весьма активно женское сообщество RLadies, устраивающее митапы по всему миру и продвигающее свой особый, женский, взгляд на использование R для разработки и анализа данных.\n\nОгромная коллекция библиотек и полезных функций, позволяющих расширить возможности базового языка\n\nСамые авторитетные хранятся в CRAN (Comprehensive R Archive Network), их более 10 тысяч, однако, чтобы попасть туда, необходимо, чтобы документация и код соответствовали определенным требованиям, что требует времени. До того момента, когда библиотека загружена на CRAN, она, как правило, хранится в открытом доступе, например, на github, где каждый желающий может принять участие в ее тестировании и доработке.\nПример сетевого анализа библиотек и их взаимозависимостей представлен на рисунке ниже:\n\nПримеры полезных библиотек:\n\nобработка и всевозможные манипуляции с данными (tidytverse)\nработа с большими данными (sparklyr)\nглубокое обучение (keras, TensorFlow)\nмашинное обучение (H2O)\nвизуализация данных (ggplot2)\nсоздание отчетов, итерактивная графика и обучение (Rmarkdown, shiny).\n\n\nСовместимость с другими языками программирования\n\nБольшинство функций и библиотек написаны на самом языке R. Однако, для сложных вычислительных задач, могут использоваться и другие языки, такие как C, C++, FORTRAN. Для манипуляций с объектами, возможно использование других языков - .NET, Java, Python. Иными словами, возможности программирования становятся практически безграничными.\n\nСоздание привлекательных визуализаций\n\nВ современном мире анализ данных невозможен без качественной визуализации, особенно если его результаты планируется использовать в сфере политики и бизнеса. R является одним из лучших инструментов для создания качественной графики, и такие библиотеки как ggplot2, plotly, ggvis помогут создать очень детализированные и эстетически привлекательные визуализации.\nДля пользователей R, которые только начинают изучение языка, на сайте https://r-graph-gallery.com создана галерея визуализаций, разбитых по отдельным тематикам. На основе простых и более сложных примеров можно изучить код и адаптировать его под собственные задачи.\n\nИнтеграция с Hadoop и анализ больших данных\n\nЕсли перед вами стоит задача анализа больших данных, то с такими библиотеками как rmr, rhdfs, rhbase, RHIVE, RHIPE и Rhadoop возможно интегрировать R и Hadoop (проект фонда Apache Software Foundation для разработки и выполнения распределённых вычислений для работы с большими данными).\nВозможности по хранению данных Hadoop и вычислительные достоинства R используют многие в качестве оптимального решения для анализа больших данных. Например, компания Форд использует R и Hadoop для обработки данных обратной связи с потребителями, что позволяет им улучшить дизайн и обосновывать бизнес решения.\n\nСоздание интерактивных веб-приложений\n\nС помощью R и библиотеки shiny можно создавать интерактивные приложения, с помощью которых пользователи (ученики, заказчики, журналисты и пр.) могут познакомиться с вашими данными, провести какие-то виды анализа, сделать визуализацию, возможно изучить какие-то закономерности (часто используются как обучающий инструмент). Примеры можно посмотреть в галерее на сайте, посвященном shiny: (https://shiny.posit.co/r/gallery/).\n\nСовместимость с другими платформами\n\nR может работать с любой конфигурацией оборудования и поддерживает различные операционные системы, независимо от окружения выдает предсказуемые и однозначные результаты.\n\nВозможность запуска кода без компилирования (возможно этот пункт стоило бы поставить на одно из первых мест)\n\nR относится к интерпретируемым языкам, что означает, что ему не требуется компилятор для того, чтобы программа заработала. Иными словами, все команды, которые мы вводим, сразу же выполняются, без дополнительного компилирования (сборки), как это происходит в других языках, например в C, COBOL, Delphi или Fortran. Другими возможностями, которыми обладают интерпретируемые языки, являются кросс-платформенность (способность работать в разных операционных системах и аппаратных средах), простота тестирования и отладки программ.\n\nНапример, Если мы создадим несколько строк кода и запустим их, они выполнятся немедленно и мы увидим результат:\n\nКроме этих преимуществ есть и много других, о которых мы узнаем в процессе изучения курса.\nК слову сказать, данное пособие также создано с помощью R. При его создании использовались возможности open-source системы публикации научной и технической информациии Quarto.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Основные сведения об R</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R в социологических исследованиях (2-е издание)",
    "section": "",
    "text": "Предисловие\nПеред Вами второе издание учебного электронного пособия «R в социологических исследованиях». Изначально оно предназначалось для магистрантов, обучающихся по направлению 39.04.01 «Социология» по программе «Цифровые методы анализа и визуализации данных», и его структура, распределение по темам, содержание и характер самостоятельных работ соответствовали дисциплине «Анализ данных в R». Между тем, пособие будет полезно не только магистрантам данной программы, но и широкому кругу лиц, занимающихся социологическими исследованиями, стремящихся расширить свой профессиональный арсенал методами обработки данных на языке R.\nПервое издание по-прежнему в силе, оно доступно по ссылке. В виду того, что пакеты и сам язык довольно активно обновляются, часть кода там может работать некорректно, и приводить не к тем ожидаемым результатам, что стоит учитывать в процессе использования.\nНеобходимость в обновлении пособия связана как с эволюцией и появлениием новых версий языка R (на момент написания доступна версия 4.5.2), обновлением пакетов и функций. Меняется и авторский подход к преподаванию: на смену максимальному погружению в аналитический контекст, стремлению предоставить исчерпывающую информацию о возможных версиях выполнения одной и той же задачи, существующих пакетах, расширениях и функциях, приходит понимание необходимости более глубокого закрепления материала, формирования у читателей культуры кодинга как необходимого профессионального навыка. С этой целью было создано дополнение к пособию, содержащее интерактивные обучающие материалы по каждой практической теме, предназначенные для отработки навыков использования R для решения рутинных задач, связанных с созданием переменных и более сложных структур данных, выполнением базовых трансформаций, выполнения отдельных процедур анализа.\nИнтерактивные задания хранятся в формате:\n\nРепозитория, находящегося в открытом доступе для тех, кто хочет научиться создавать аналогичные «тьюториалы» с помощью пакета learnr, а может быть покритиковать и поправить автора, предложить изменения (чему она будет весьма рада).\nОдноименного пакета r4ssr, в который, собственно, и «упакованы» самостоятельные работы. Каждая работа развертывается в отдельный сайт с возможностью выполнения кода, получения подсказок, проверки решения. О том как пользоваться пакетом, будет сказано и показано ниже.\n\nЦель курса, равно как и прошлого издания, состоит в том, чтобы познакомить читателей с основами языка, ключевыми командами, функциями и пакетами, которые могут быть полезными в процессе организации, сбора данных и анализа результатов социологических исследований, и особенно - в части представления и распространения результатов.\nХотя R это язык программирования, мы в ходе освоения курса мы практически не будем использовать его в этом качестве, наша основная задача - стать уверенными пользователями R и использовать его для решения профессиональных социологических и учебных задач.\nЧему мы научимся:\n\nРаботать с различными структурами данных, импортировать и экспортировать данные в различных форматах;\nСоздавать пользовательские функции и понимать документацию к пакетам;\nМодифицировать данные для исследовательских задач - создавать новые переменные, разъединять и соединять массивы;\nАнализировать результаты количественных социологических исследований и проверять статистические гипотезы, как простые, так и сложные, применять многомерные методы анализа;\nСоздавать визуализации на основе результатов исследования, создавать интерактивные отчеты и простые приложения, демонстрирующие возможности R в распространении научных публикаций.\n\nВ качестве дополнительного эффекта, своего рода «экстерналией», не связанной напрямую с задачами освоения R, автор считает рассмотрение основ статистического анализа, в том числе вопросов, касающихся выдвижения, подтверждения и опровержения статистических гипотез, корректного использования методов для их проверки.\nЧто по прежнему остается за кадром?\n\nМы не будем учиться создавать библиотеки и писать сложные функции;\nМы не сможем в полной мере охватить все методы анализа;\nВряд ли получится разобрать все инструменты для визуализации;\nЗа пределами нашего внимания останутся и возможности R по созданию сайтов, блогов и книг.\n\nОднако, и того, что запланировано, представляется хватит, для того, чтобы полюбить R и стать частью сообщества исследователей, использующих R в своей повседневной жизни для решения рутинных задач и поиска инновационных решений.\nАвтор выражает благодарность своим коллегам - преподавателям кафедры социальной и молодежной политики Института гуманитарных наук Алтайского государственного университета, участвовавшим в апробации и рецензировании материалов пособия, за ценные замечания и уточнения, моральную поддержку в ходе создания пособия. Особенные слова признательности - всем студентам, магистрантам, аспирантам, для которых это пособие стало первым шагом на пути к R.\nРабота над пособием велась параллельно с обновлением обучающего курса, что стало возможным благодаря стипендиальной программе Фонда Владимира Потанина, предоставившего автору грант по проекту «Редизайн курса “Анализ данных в R” для магистрантов направления “Социология”».",
    "crumbs": [
      "Предисловие"
    ]
  },
  {
    "objectID": "About-R.html#кто-и-как-использует-r",
    "href": "About-R.html#кто-и-как-использует-r",
    "title": "1  Основные сведения об R",
    "section": "1.2 Кто и как использует R?",
    "text": "1.2 Кто и как использует R?\nПрежде всего, это язык, который используют ученые, специалисты, работающие в различных отраслях экономики, менеджеры для анализа реальных данных и разработки научно обоснованных систем принятия решений, поэтому место в общем рейтинге не так высоко. Если посмотреть сферы использования, то на первом месте - академическая среда, на втором - сфера здравоохранения, на третьем - правительственные учреждения ((Article?)).\nR используют банки и маркетинговые агентства, технические компании и информационные корпорации для разных целей - от обработки данных до прогнозирования и представления интерактивной инфографики.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Основные сведения об R</span>"
    ]
  },
  {
    "objectID": "Install-R.html",
    "href": "Install-R.html",
    "title": "2  Установка и начало работы с R",
    "section": "",
    "text": "2.1 Установка R\nПрежде, чем начать работать с R, нам нужно установить его себе на компьютер. Для этого необходимо перейти по ссылке и выбрать версию, подходящую для вашей операционной системы.\nДля установки R рекомендуется использовать официальный сайт CRAN (Comprehensive R Archive Network). Скачать последнюю стабильную версию (на январь 2026 года это версия 4.5.2).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Установка и начало работы с R</span>"
    ]
  },
  {
    "objectID": "Install-R.html#установка-r",
    "href": "Install-R.html#установка-r",
    "title": "2  Установка и начало работы с R",
    "section": "",
    "text": "2.1.1 Windows\nПерейдите на CRAN для Windows. Скачайте файл R-4.5.2-win.exe (около 87 МБ, требует UCRT на Windows 10+). Запустите файл .exe и следуйте инструкциям установщика: соглашайтесь с лицензией, выбирайте путь установки (по умолчанию Program Files) и компоненты. После установки R появится в меню Пуск.\n\n\n2.1.2 macOS\nПерейдите на CRAN для macOS. Выберите пакет по архитектуре: R-4.5.2-arm64.pkg для Apple Silicon (M1/M2+, ~97 МБ) или R-4.5.2-x86_64.pkg для Intel (~100 МБ, macOS 11+). Дважды кликните на .pkg файл, следуйте мастеру установки: соглашайтесь с лицензией, выбирайте “Установить для всех” и вводите пароль администратора. Устанавливаются R Framework и R.app GUI.\n\n\n2.1.3 Linux (Ubuntu/Debian)\nДобавьте репозиторий CRAN:\nsudo apt install r-base r-base-dev -y",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Установка и начало работы с R</span>"
    ]
  },
  {
    "objectID": "Install-R.html#начало-работы-с-r-и-rstudio",
    "href": "Install-R.html#начало-работы-с-r-и-rstudio",
    "title": "2  Установка и начало работы с R",
    "section": "2.3 Начало работы с R и RStudio",
    "text": "2.3 Начало работы с R и RStudio\nПосле первого запуска RStudio вы, скорее всего, увидите вот такую картину:\n\nОсновное окно RStudio будет состоять из трех частей (экранов).\nСлева находится консоль (Console) - здесь можно писать код, и здесь же будут появляться результаты его выполнения, а также различные сообщения, с помощью которых R “общается” с пользователями.\nСправа сверху - рабочее окружение (Environment) - здесь хранятся создаваемые и загружаемые объекты - данные (вектора, датафреймы и пр.), пользовательские функции и некоторые другие объекты.\nОкружение является структурой данных, предназначенной для обеспечения области видимости. Это коллекция каких-то объектов (переменных, функций, массивов данных), которые мы используем. Хэдли Уикхэм предлагает относиться к окружению как к “мешку с именами”, :\n\nЕсть четыре типа окружений:\n\nГлобальное окружение - интерактивное пространство, где мы обычно работаем.\nБазовое окружение, создаваемое базовым пакетом R.\nПустое окружение, не имеющее никаких имен. Является в большей степени абстракцией, хотя и может быть создано с помощью специальной функции.\nТекущее окружение, то есть то, в котором мы работаем в какой-то момент времени.\nОбычно мы не задумываемся над тем, в каком окружении работаем. Это становится важным тогда, когда возникают конфликты функций и ошибки выполняемого кода. Окружение создается автоматически при запуске программы RStudio.\n\nВ этом же окне можно посмотреть Историю (History) выполнения кода, и если вы случайно или специально что-то удалили, часто именно в истории можно найти строки, которые были выполнены, и их можно восстановить. Здесь есть некоторые другие вкладки, они нам понадобятся на более поздних этапах работы с R и RStudio.\nСправа снизу - Окно просмотра. В отдельных вкладках можно посмотреть, какие файлы и папки есть в рабочей директории, какие библиотеки установлены, можно запросить помощь или посмотреть графики (в процессе анализа).\nЭто только в первый раз окна всего три.\nВыберите в меню File - New File - R Script:\n\nОткроется новый файл, и окон станет четыре:\n\nВ этом новом окне можно писать код и комментарии, сохранять его как отдельный файл с расширением .R, который можно запускать повторно, что очень удобно и позволяет значительно сохранить время при рутинной обработке данных. Очень часто в ходе обработки и анализа данных приходится осуществлять повторяющиеся действия, и скрипт поможет ускорить процесс обработки. В этом состоит основное отличие от консоли, где код можно запустить только однажды.\n\n2.3.1 Первые простые действия в R\nВсе естественные языки содержат некоторые правила, описывающие то, как язык должен быть использован, и позволяющие носителям языка понимать друг друга без проблем.\nВ языках программирования, и в R, в частности, тоже есть такие правила, которые нужно соблюдать для того, чтобы компьютер понял, что от него требуется. В обычном языке правила иногда меняются или не соблюдаются, что не мешает людям понимать друг друга. С R, однако такой номер не пройдет. Правила являются фиксированными, и требуется их неукоснительное соблюдение, чтобы не возникало ошибок.\nРассмотрим некоторые, самые базовые правила.\n\n2.3.1.1 Команды R\nКак большинство других языков программирования, R состоит из совокупности команд, формирующих последовательность инструкций, которые выполняет компьютер. Можно условно рассматривать такие команды в качестве глаголов, которые обозначают какие-то действия, которые выполняет R для получения определенного результата.\n\n\n\n\n\n\nСовет\n\n\n\nКод набирается либо в консоли, либо в файле скрипта. В консоли его можно запустить, нажав клавишу Enter, в скрипте - нажав на кнопку Run в меню или с помощью сочетания клавиш Ctrl+Enter.\n\n\nНапример:\n\nprint(\"Привет, мир!\")\n\n[1] \"Привет, мир!\"\n\n\nКоманда print() выполняет вывод на экран сообщения. Код на R обычно содержит множество команд, и обычно, каждая команда распологается на своей строке. Примеры:\n\nprint(\"Сегодня хорошая погода\")\n\n[1] \"Сегодня хорошая погода\"\n\nprint(1+1)\n\n[1] 2\n\nprint(4 &gt; 5)\n\n[1] FALSE\n\n\nПервая команда выводит сообщение, вторая - производит математические действия третья - оценивает истинность или ложность утверждения и выводит результат.\nОбычно каждая команда находится в своей строке, но можно их писать и в одну строку, разделяя точкой с запятой:\n\nx &lt;- 1+1; print(x); print(x^2)\n\n[1] 2\n\n\n[1] 4\n\n\nВ этом примере три команды приведены в одной строке. Первая команда создает переменную x, вторая выводит на экран значения этой переменной, третья - вычисляет квадрат значений переменной x и выводит результат на экран. Точка с запятой используется в качестве знака окончания команды и подсказывает R, где заканчивается одна команда и начинается другая. Если строка содержит только одну команду, точку с запятой можно не ставить, но если поставить, это не будет ошибкой:\n\nprint(\"Здесь нет точки с запятой\")\n\n[1] \"Здесь нет точки с запятой\"\n\nprint(\"А здесь есть точка с запятой\");\n\n[1] \"А здесь есть точка с запятой\"\n\n\n\n\n\n\n\n\nВажное уведомление\n\n\n\nЕсли включить несколько знаков точки с запятой, например, print(“hello”);;, ничего работать не будет!\n\n\n\n\n2.3.1.2 Полезные подсказки (shortcuts)\nВ R создано много удобных вещей, для того, чтобы написание кода было более приятным. Например, если выделить слово и нажать на знак кавычек, они поставятся сразу с двух сторон, то же происходит и со скобками.\nЕще одной удобной подсказкой является клавиша tab, которая работает с функциями и переменными. Начав набирать какую-то команду или переменную, можно нажать клавишу tab и выбрать из списка подходящий вариант, что экономит большое количество времени и позволяет избежать ошибок.\n\n\n2.3.1.3 Пустые строки\nПустые линии R игнорирует, но они позволяют организовать код и сделать его более читаемым:\n\nprint(\"Какое небо голубое!\")\n\n[1] \"Какое небо голубое!\"\n\n# Пустая строка есть, но она ничего не меняет\n\nprint(\"А трава зеленая!\")\n\n[1] \"А трава зеленая!\"\n\n\n\n\n2.3.1.4 Математические операторы\nДовольно часто в процессе обработки данных возникает необходимость вычисления новых переменных, при этом могут потребоваться дополнительные операторы для совершения математических действий. Со сложением и вычитанием все довольно сложно, но что еще?\nВ таблице ниже представлены основные операторы с примерами их использования:\n\n\n\n\n\n\nУведомление\n\n\n\n\n\n\nОператор\nОписание\n\n\n+\nсложение\n\n\n-\nвычитание\n\n\n*\nумножение\n\n\n/\nделение\n\n\n^ или **\nвозведение в степень\n\n\nx %% y\nостаток от деления (x mod y) 5%%2 = 1\n\n\nx %/% y\nцелая часть при делени 5%/%2 =2\n\n\n\n\n\nЕсли мы что-то делаем неправильно, в консоли появится сообщение об ошибке:\n\n-1+a\n\n\n\n2.3.1.5 ?\nВ R имеется очень много сопроводительной документации, доступной по каждой функции и библиотеке, где объясняются особенности работы с ними, приводятся примеры и дается теоретическое обоснование. Чтобы обратиться к этой вспомогательной документации достаточно набрать имя функции, перед которой нужно поставить знак вопроса:\nЛучше всего эту команду попробовать в RStudio. Давайте сделаем это!\n\n\n2.3.1.6 ??\nЕсли вдруг мы забыли как точно называется та или иная функция или нужно посмотреть общие материалы по теме, то можно использовать ??.\n\n??print\n\n\n\n\n\n\n\nУведомление\n\n\n\nХотите узнать больше о синтаксисе R? Попробуйте набрать ?Syntax в консоли R и нажать на Enter.\n\n\n\n\n2.3.1.7 О пользе ошибок\nОшибок и предупреждающих сообщений бояться не надо, напротив, в таких сообщениях практически всегда находится ответ на вопрос, почему такая ошибка возникла, а часто - и возможное решение проблемы.\n\n\n2.3.1.8 Работа с пакетами\nМы уже выяснили, что базовый язык R в настоящее время используется наряду с многочисленными функциями и пакетами, разрабатываемыми коллективами ученых и разработчиками из разных стран мира, включая Россию.\nУстанавливать новые пакеты нам придется практически на каждом занятии, поэтому лучше научиться делать это сразу.\nПакеты хранятся в основном в двух местах:\n\nCRAN\nGithub - веб-сервис для хостинга IT-проектов и совместной разработки.\n\n\n2.3.1.8.1 Как установить пакет с помощью CRAN\nЧтобы скачать и установить нужную библиотеку с помощью CRAN, проще всего воспользоваться меню RStudio. Нужно выбрать пункт меню Tools - Install Packages:\n\nЗатем в окне Packages необходимо ввести имя нужной библиотеки, например, dplyr и нажать на кнопку Install. По умолчанию будет стоять “галочка” - Install Dependencies (установить зависимости) - убирать ее не надо, так как это позволить установить не только саму библиотеку, но и другие библиотеки, от которых она зависит и к которым обращается в ходе выполнения функций. В противном случае, могут возникать ошибки, а оно нам надо?\n\n\n\n2.3.1.8.2 Как установить библиотеку из Github\nНе все библиотеки доступны на CRAN, так как эта процедура достаточно сложная и строгая, предполагает несколько проверок (кода, сопроводительной документации). Достаточно частая практика, когда библиотека еще не подана для регистрации на CRAN, разработчики помещают ее на GitHub, откуда ее можно скачать и использовать по назначению. Это позволяет разработчикам получить обратную связь, устранять возможные ошибки, улучшать код.\nЧтобы установить нужную библиотеку из GitHub, нам понадобится функция install_github(), в которой мы должны указать имя разработчика и название библиотеки. Однако, чтобы выполнить эту функцию, нужен дополнительный пакет devtools. Установить ее можно через CRAN с помощью описанного выше способа или через код install.packages(\"devtools\"):\n\ninstall.packages(\"devtools\") # только один раз для установки пакета\nlibrary (devtools)\ninstall_github(\"DeveloperName/PackageName\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Установка и начало работы с R</span>"
    ]
  },
  {
    "objectID": "Install-R.html#первые-простые-действия-в-r",
    "href": "Install-R.html#первые-простые-действия-в-r",
    "title": "2  Установка и начало работы с R",
    "section": "2.3 Первые простые действия в R",
    "text": "2.3 Первые простые действия в R\nВсе естественные языки содержат некоторые правила, описывающие то, как язык должен быть использован, и позволяющие носителям языка понимать друг друга без проблем.\nВ языках программирования, и в R, в частности, тоже есть такие правила, которые нужно соблюдать для того, чтобы компьютер понял, что от него требуется. В обычном языке правила иногда меняются или не соблюдаются, что не мешает людям понимать друг друга. С R, однако такой номер не пройдет, правила являются фиксированными, и требуется их неукоснительное соблюдение, чтобы не возникало ошибок.\nРассмотрим некоторые, самые базовые правила.\n\n2.3.1 Команды R\nКак большинство других языков программирования, R состоит из совокупности команд, формирующих последовательность инструкций, которые выполняет компьютер. Можно условно рассматривать такие команды в качестве глаголов, которые обозначают какие-то действия, которые выполняет R для получения определенного результата.\nНапример:\n\nprint(\"Привет, мир!\")\n\n[1] \"Привет, мир!\"\n\n\nЭта команда выполняет вывод на экран сообщения. Код на R обычно содержит множество команд, и обычно, каждая команда распологается на своей строке. Примеры:\n\nprint(\"Сегодня хорошая погода\")\n\n[1] \"Сегодня хорошая погода\"\n\nprint(1+1)\n\n[1] 2\n\nprint(4 &gt; 5)\n\n[1] FALSE\n\n\nПервая команда выводит сообщение, вторая - производит математические действия третья - оценивает истинность или ложность утверждения и выводит результат.\nОбычно каждая команда находится в своей строке, но можно их писать и в одну строку, разделяя точкой с запятой:\n\nx &lt;- 1+1; print(x); print(x^2)\n\n[1] 2\n\n\n[1] 4\n\n\nВ этом примере три команды приведены в одной строке. Первая команда создает переменную x, вторая выводит на экран значения этой переменной, третья - вычисляет квадрат значений переменной x и выводит результат на экран. Точка с запятой используется в качестве знака окончания команды и подсказывает R, где заканчивается одна команда и начинается другая. Если строка содержит только одну команду, точку с запятой можно не ставить, но если поставить, это не будет ошибкой:\n\nprint(\"Здесь нет точки с запятой\")\n\n[1] \"Здесь нет точки с запятой\"\n\nprint(\"А здесь есть точка с запятой\");\n\n[1] \"А здесь есть точка с запятой\"\n\n\n\n\n\n\n\n\nВажное уведомление\n\n\n\nЕсли включить несколько знаков точки с запятой, например, print(“hello”);;, ничего работать не будет!\n\n\nИтак, мы сейчас попробовали написать код по-разному - каждая команда в одной строке несколько команд в одной строке. А можно ли написать одну команду в несколько строк? Да, если в это сложная команда, в которой необходимо устанавливать несколько аргументов и такая разбивка часто используется для улучшения читаемости кода. Чуть ниже мы увидим, как это может быть.\nМожно также заключить часть кода в фигурные скобки, и тогда он будет восприниматься R как единое целое:\n\n{\n  print(\"Тут написан код, который воспринимается как единое целое\")\n  print(2^3 - 7)\n  w &lt;- \"hello\"\n  print(w)\n}\n\n[1] \"Тут написан код, который воспринимается как единое целое\"\n[1] 1\n[1] \"hello\"\n\n\nКод в примере выше оформлен с помощью отступов и так выглядит лучше, но это не обязательно. Можно обойтись и без этих отступов, все будет работать. Отступы (идентирование) никак не влияют на результат, а являются частью форматирования кода для улучшения его читабельности.\n\n{\nprint(\"Тут написан код, который воспринимается как единое целое\")\nprint(2^3 - 7)\nw &lt;- \"hello\"\nprint(w)\n}\n\n[1] \"Тут написан код, который воспринимается как единое целое\"\n[1] 1\n[1] \"hello\"\n\n\nЧто это ознчает - сгруппировать код? С практической точки зрения различий мало, но программа воспринимает эти несколько линий вместе и обрабатывает как бы как одну команду. Это в большей степени важно при написании программ на R.\n\n\n2.3.2 Полезные подсказки (shortcuts)\nВ R создано много удобных вещей, для того, чтобы написание кода было более приятным. Например, если выделить слово и нажать на знак кавычек, они поставятся сразу с двух сторон, то же происходит и со скобками.\nЕще одной удобной подсказкой является клавиша tab, которая работает с функциями и переменными. Начав набирать какую-то команду или переменную, можно нажать клавишу tab и выбрать из списка подходящий вариант, что экономит большое количество времени и позволяет избежать ошибок.\n\n\n2.3.3 Пустые строки\nПустые линии R игнорирует, но они позволяют организовать код и сделать его более читаемым:\n\nprint(\"Какое небо голубое!\")\n\n[1] \"Какое небо голубое!\"\n\n# Пустая строка есть, но она ничего не меняет\n\nprint(\"А трава зеленая!\")\n\n[1] \"А трава зеленая!\"\n\n\n\n\n2.3.4 ?\nВ R имеется очень много сопроводительной документации, доступной по каждой функции и библиотеке, где объясняются особенности работы с ними, приводятся примеры и дается теоретическое обоснование. Чтобы обратиться к этой вспомогательной документации достаточно набрать имя функции, перед которой нужно поставить знак вопроса:\nЛучше всего эту команду попробовать в RStudio. Давайте сделаем это!\n\n\n2.3.5 ??\nЕсли вдруг вы забыли как точно называется та или иная функция или хотите посмотреть общие материалы по теме, то можно использовать ??.\n\n??print\n\n\n\n\n\n\n\nУведомление\n\n\n\nХотите узнать больше о синтаксисе R? Попробуйте набрать ?Syntax в консоли R и нажать на Enter.\n\n\n\n\n2.3.6 Самостоятельная работа\nДавайте создадим наш первый скрипт и сохраним его для истории.\n\nВыберите в меню File - New File - R Script (если Вы этого еще не сделали) или нажать Ctrl + Shift + N.\nНапишите код, выполняющий простые арифметические действия. Наберите на клавиатуре любое математическое выражение, например, 1+1. Чтобы выполнить данную команду, нажмите сочетание клавиш Ctrl + Enter или кнопку Run, которая находится в правом углу меню. Команда выполняется автоматически, сразу же.\n\n\n1+1\n\n[1] 2\n\n\nПоэспериментируйте с R как с калькулятором, используя информацию об операторах, используемых для математических вычислений:\n\n\n\n\n\n\nКакие операторы используются для математических действий?\n\n\n\n\n\n\nОператор\nОписание\n\n\n+\nсложение\n\n\n-\nвычитание\n\n\n*\nумножение\n\n\n/\nделение\n\n\n^ или **\nвозведение в степень\n\n\nx %% y\nостаток от деления (x mod y) 5%%2 = 1\n\n\nx %/% y\nцелая часть при делени 5%/%2 =2\n\n\n\n\n\nЕсли мы что-то делаем неправильно, в консоли появится сообщение об ошибке:\n\n-1+a\n\n\n\n\n\n\n\nО пользе ошибок\n\n\n\nОшибок и предупреждающих сообщений бояться не надо, напротив, в таких сообщениях практически всегда находится ответ на вопрос, почему такая ошибка возникла, а часто - и возможное решение проблемы.\n\n\n\nНапишите код, отображающий текстовое сообщение:\n\n\n\"Привет, Алтайский государственный университет!\"\n\n[1] \"Привет, Алтайский государственный университет!\"\n\n\n\nСохраните файл. Файлы скрипта сохраняются с раширением .R, однако есть и другие форматы. Например, объекты, которые создаются в окружении, можно сохранить в формате .Rdata, а в файле .Rhistory может храниться информация о выполненном коде (что вы делали и в какой последовательности).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Установка и начало работы с R</span>"
    ]
  },
  {
    "objectID": "Install-R.html#работа-с-библиотеками",
    "href": "Install-R.html#работа-с-библиотеками",
    "title": "2  Установка и начало работы с R",
    "section": "2.6 Работа с библиотеками",
    "text": "2.6 Работа с библиотеками\nМы уже выяснили, что базовый язык R в настоящее время используется наряду с многочисленными функциями и библиотеками, разрабатываемыми коллективами ученых и разработчиками из разных стран мира, включая Россию.\nУстанавливать новые библиотеки нам придется практически на каждом занятии, поэтому лучше научиться делать это сразу.\nЭти библиотеки хранятся в основном в двух местах:\n\nCRAN\nGithub - нечто вроде социальной сети для программистов, где все друг друга знают, создают совместные проекты и делятся кодом.\n\n\n\n\n\n\n\nКое-что о библиотеке/пакетах\n\n\n\nМы называем “библиотеку” “библиотекой” и подразумеваем под ней набор каких-то полезных утилит, наборов данных и сопутствующей документации, потому что так принято в русскоязычном сегменте Интернета, посвященном программированию.\nОднако, по-английски библиотека называется package, то есть “пакет”, в котором “упакованы” функции, сопровождающие документы и иногда готовые данные, а вот функция, которая этот пакет подключает - `library()` – что собственно и переводится как библиотека, такой вот языковый казус. Об этом стоит помнить и слова эти не путать.\n\n\n\n2.6.1 Как установить библиотеку с помощью CRAN\nЧтобы скачать и установить нужную библиотеку с помощью CRAN, проще всего воспользоваться меню RStudio. Нужно выбрать пункт меню Tools - Install Packages:\n\nЗатем в окне Packages необходимо ввести имя нужной библиотеки, например, dplyr и нажать на кнопку Install. По умолчанию будет стоять “галочка” - Install Dependencies (установить зависимости) - убирать ее не надо, так как это позволить установить не только саму библиотеку, но и другие библиотеки, от которых она зависит и к которым обращается в ходе выполнения функций. В противном случае, могут возникать ошибки, а оно нам надо?\n\n\n\n2.6.2 Как установить библиотеку из Github\nНе все библиотеки доступны на CRAN, так как эта процедура достаточно сложная и строгая, предполагает несколько проверок (кода, сопроводительной документации). Достаточно частая практика, когда библиотека еще не подана для регистрации на CRAN, разработчики помещают ее на GitHub, откуда ее можно скачать и использовать по назначению. Это позволяет разработчикам получить обратную связь, устранять возможные ошибки, улучшать код.\nЧтобы установить нужную библиотеку из GitHub, нам понадобится функция install_github(), в которой мы должны указать имя разработчика и название библиотеки. Однако, чтобы выполнить эту функцию, нужен дополнительный пакет devtools. Установить ее можно через CRAN с помощью описанного выше способа или через код install.packages(\"devtools\"):\n\ninstall.packages(\"devtools\") # только один раз для установки пакета\nlibrary (devtools)\ninstall_github(\"DeveloperName/PackageName\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Установка и начало работы с R</span>"
    ]
  },
  {
    "objectID": "TidyverseTransformations.html",
    "href": "TidyverseTransformations.html",
    "title": "7  Tidyverse и трансформация данных",
    "section": "",
    "text": "7.1 Прямоугольные и непрямоугольные таблицы, а также tidy и messy данные\nДанные могут быть представлены в различных формах, но на самом базовом уровне их можно представить в виде двух типов структур: данные представленные в виде прямоугольных таблиц и непрямоугольных данных.\nПрямоугольные данные имеют форму прямоугольника (действительно!), то есть каждое значение соответствует какой-то строке и столбцу. Большинство датафреймов содержат как раз прямоугольные данные.\nСоответственно непрямоугольные данные не могут так легко распределяться по строкам и столбцам, они, как правило, представляют собой совокупность различных структур данных, в которых между отдельными элементами имеется какое-то сходство. Обычно непрямоугольные данные хранятся в списках.\nTidy по-английски означает “чистый, аккуратный, опрятный”. По отношению к данным выражение - tidy data - “чистые, правильным образом представленные данные” противопоставляется “messy data” - “грязным данным”, представленным в виде, не совсем пригодном или удобном для анализа.\nTidy data - это когда данные имеют так называемый “длинный формат” - каждый столбец представляет собой отдельную переменную, а строка - наблюдение или случай.\nКак из «грязных» данных сделать «чистые»? Как привести набор данных к виду, пригодному для дальнейшего анализа, отобрать нужные переменные, преобразовать их в формат, который нужен для решения аналитической задачи?\nВ рамках данного раздела будут рассмотрены функции, которые используются для трансформации данных - их чистки, фильтрации, вычисления новых переменных и описательных статистик, а также перевода набора данных из «широкого» в «длинный» и обратно.\nВ основном мы будем работать с двумя библиотеками семейства tidyverse - dplyr и tidyr.\n,\nЭти библиотеки является одними из лучших для разнообразных трансформаций данных, потребность в которых возникает постоянно в процессе анализа.\nРассмотрим основные функции на примере набора данных flights из библиотеки - nycflights13, в котором содержится информация о 336 776 полетах, совершенных из аэропорта Нью-Йорка в 2013 году. Данные предоставлены Бюро по статистике США, справку по ним можно найти по запросу ?flights.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Tidyverse и трансформация данных</span>"
    ]
  },
  {
    "objectID": "TidyverseTransformations.html#фильтрация-строк---filter",
    "href": "TidyverseTransformations.html#фильтрация-строк---filter",
    "title": "7  Tidyverse и трансформация данных",
    "section": "7.2 Фильтрация строк - filter()",
    "text": "7.2 Фильтрация строк - filter()\nПрежде, чем начать, установим библиотеки:\n\ninstall.packages(c(\"nycflights13\", \"tidyverse\"))\n\nТеперь загрузим их в наше рабочее пространство:\n\nlibrary(tidyverse)\nlibrary(nycflights13)\n\nВ наборе данных flights содержится следующая информация о переменных:\n\nyear (год), month (месяц), day (день) - дата вылета\ndep_time - время вылета\narr_time - время прилета\nsched_dep_time, sched_arr_time - время вылета и прилета по расписанию.\ndep_delay, arr_delay - задержка вылета и прилета в минутах. Отрицательные значения означают досрочные вылеты / прилеты.\ncarrier - перевозчик\nflight - номер рейса\ntailnum - номер самолета\norigin, dest - место отправления, место назначения\nair_time - время полета в минутах\ndistance - расстояние в милях\nhour, minute - часов, минут - время отлета, разбитое по часам и минутам\ntime_hour - дата и время полета по расписанию в формате даты.\n\nДавайте сначала просто посмотрим на эти данные:\n\nhead(flights)\n\n# A tibble: 6 × 19\n   year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n1  2013     1     1      517            515         2      830            819\n2  2013     1     1      533            529         4      850            830\n3  2013     1     1      542            540         2      923            850\n4  2013     1     1      544            545        -1     1004           1022\n5  2013     1     1      554            600        -6      812            837\n6  2013     1     1      554            558        -4      740            728\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nФункция filter() позволяет отобрать наблюдения, основываясь на значениях. Первый аргумент - имя датафрейма (массива, базы данных), второй и последующие аргументы - выражения, позволяющие отфильтровать данные. Например, мы хотим отобрать все полеты, которые произошли 1 января:\n\nfilter(flights, month == 1, day == 1)\n\n# A tibble: 842 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 832 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nКогда мы запускаем эту строку с кодом, dplyr выполняет фильтрующую операцию и выводит новый датафрейм. Однако, стоит учитывать, что исходные данные не меняются, и если нам необходимо сохранить эти изменения как отдельный объект, нам нужно использовать оператор присваивания, &lt;-, и задать новое имя:\n\njan1 &lt;- filter(flights, month == 1, day == 1)\n\nR либо выводит на экран результаты либо сохраняет их в новую переменную (объект). Если мы хотим одновременно создать новую базу данных, сохранить в нее результаты фильтрации и вывести результат на экран, нужно код заключить в скобки:\n\n(dec25 &lt;- filter(flights, month == 12, day == 25))\n\n# A tibble: 719 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013    12    25      456            500        -4      649            651\n 2  2013    12    25      524            515         9      805            814\n 3  2013    12    25      542            540         2      832            850\n 4  2013    12    25      546            550        -4     1022           1027\n 5  2013    12    25      556            600        -4      730            745\n 6  2013    12    25      557            600        -3      743            752\n 7  2013    12    25      557            600        -3      818            831\n 8  2013    12    25      559            600        -1      855            856\n 9  2013    12    25      559            600        -1      849            855\n10  2013    12    25      600            600         0      850            846\n# ℹ 709 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Tidyverse и трансформация данных</span>"
    ]
  },
  {
    "objectID": "TidyverseTransformations.html#логические-операторы",
    "href": "TidyverseTransformations.html#логические-операторы",
    "title": "7  Tidyverse и трансформация данных",
    "section": "7.3 Логические операторы",
    "text": "7.3 Логические операторы\nОчень часто функция filter() сопровождается логическими операторами (“и”, “или” и др.). В этом случае, результат каждого выражения должен быть истинным, для того, чтобы строки были отобраны в новый набор.\n\n\n\n\n\n\nСовет\n\n\n\nЗапомним основные операторы\n& это “и”\n| это “или”\n! “не равно”\n\n\nГрафически различия между операторами можно представить следующим образом:\n\nНапример, давайте отфильтруем данные так, чтобы у нас отображались два месяца - ноябрь и декабрь.\n\nfilter(flights, month == 11 | month == 12)\n\n# A tibble: 55,403 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013    11     1        5           2359         6      352            345\n 2  2013    11     1       35           2250       105      123           2356\n 3  2013    11     1      455            500        -5      641            651\n 4  2013    11     1      539            545        -6      856            827\n 5  2013    11     1      542            545        -3      831            855\n 6  2013    11     1      549            600       -11      912            923\n 7  2013    11     1      550            600       -10      705            659\n 8  2013    11     1      554            600        -6      659            701\n 9  2013    11     1      554            600        -6      826            827\n10  2013    11     1      554            600        -6      749            751\n# ℹ 55,393 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nДля некоторого упрощения кода полезно будет запомнить “законы де Моргана”:\n!(x & y) это то же самое, что и !x | !y,\n!(x | y) равносильно !x & !y.\nНапример, если нам нужно отобрать полеты, которые задерживались (по прилетам и отлетам) не более чем на два часа, мы могли бы пойти двумя путями:\n\nfilter(flights, !(arr_delay &gt; 120 | dep_delay &gt; 120))\n\n# A tibble: 316,050 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 316,040 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n# Или (результат идентичный)\nfilter(flights, arr_delay &lt;= 120, dep_delay &lt;= 120)\n\n# A tibble: 316,050 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 316,040 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n Самостоятельное задание:\n\nСоздайте набор данных arr_depay_over120, в который отберите рейсы, прилет которых (arr_delay) задерживался на 2 или более часа.\nВыгрузите данные о рейсах в аэропорты Хьюстона - George Bush Intercontinental (IAH) или аэропорт William P Hobby (HOU) (по переменной dest) и сохраните их под именем IAH_OR_HOU.\nОтберите из таблицы все полеты, которые совершались в летние месяцы, и сохраните их под именем summer_flights.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Tidyverse и трансформация данных</span>"
    ]
  },
  {
    "objectID": "TidyverseTransformations.html#упорядочение-строк-с-функцией-arrange",
    "href": "TidyverseTransformations.html#упорядочение-строк-с-функцией-arrange",
    "title": "7  Tidyverse и трансформация данных",
    "section": "7.4 Упорядочение строк с функцией arrange()",
    "text": "7.4 Упорядочение строк с функцией arrange()\nФункция arrange() работает практически идентично с filter(), однако, вместо отбора строк она меняет их порядок. На входе функция берет датафрейм и наименования столбцов, по которым нужно сделать сортировку.Если таких столбцов несколько, каждая дополнительная колонка может быть использована для сортировки значений по уже отсортированным значениям предыдущего столбца.\nНапример, отсортируем полеты по дате (сначала по месяцу, потом по дню):\n\narrange(flights, month, day)\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nЕсли нам нужно отсортировать что-то по убыванию, то функция desc() поможет нам перегруппировать значения в столбце. Например, отсортируем по убыванию значения задержки отлета:\n\narrange(flights, desc(dep_delay))\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     9      641            900      1301     1242           1530\n 2  2013     6    15     1432           1935      1137     1607           2120\n 3  2013     1    10     1121           1635      1126     1239           1810\n 4  2013     9    20     1139           1845      1014     1457           2210\n 5  2013     7    22      845           1600      1005     1044           1815\n 6  2013     4    10     1100           1900       960     1342           2211\n 7  2013     3    17     2321            810       911      135           1020\n 8  2013     6    27      959           1900       899     1236           2226\n 9  2013     7    22     2257            759       898      121           1026\n10  2013    12     5      756           1700       896     1058           2020\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n** ! Пропущенные значения (NA) - всегда располагаются в конце списка, независимо от направления сортировки**. Давайте воспроизведем это свойство на примере, содержащем небольшое количество строк:\n\ndf &lt;- tibble(x = c(5, 2, NA))\narrange(df, x)\n\n# A tibble: 3 × 1\n      x\n  &lt;dbl&gt;\n1     2\n2     5\n3    NA\n\narrange(df, desc(x))\n\n# A tibble: 3 × 1\n      x\n  &lt;dbl&gt;\n1     5\n2     2\n3    NA\n\n\n Самостоятельное задание:\n\nОтсортируйте полеты, чтобы найти те, что имеют наибольшие значения времени задержки прилета (arr_delay). Найдите также полеты, которые вылетели раньше времени по расписанию (вспоминаем функцию tail()).\nНайдите самые короткие (быстрые) полеты по переменной air_time.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Tidyverse и трансформация данных</span>"
    ]
  },
  {
    "objectID": "TidyverseTransformations.html#отбор-переменных-с-функцией-select",
    "href": "TidyverseTransformations.html#отбор-переменных-с-функцией-select",
    "title": "7  Tidyverse и трансформация данных",
    "section": "7.5 Отбор переменных с функцией select()",
    "text": "7.5 Отбор переменных с функцией select()\nДостаточно часто нам для анализа не нужен весь наш набор данных, а только ограниченный набор переменных, с которыми мы работаем в какой-то момент времени. Удобно и полезно отобрать эти переменные и работать только с ними, особенно - если данных очень много, и количество переменных исчисляется сотнями или тысячами. Функция select() позволяет быстро отобрать переменные по их именам:\n\nselect(flights, year, month, day)\n\n# A tibble: 336,776 × 3\n    year month   day\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1  2013     1     1\n 2  2013     1     1\n 3  2013     1     1\n 4  2013     1     1\n 5  2013     1     1\n 6  2013     1     1\n 7  2013     1     1\n 8  2013     1     1\n 9  2013     1     1\n10  2013     1     1\n# ℹ 336,766 more rows\n\n\n\nselect(flights, year:day) # если переменные идут подряд, не обязательно перечислять все, можно воспользоваться оператором :\n\n# A tibble: 336,776 × 3\n    year month   day\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1  2013     1     1\n 2  2013     1     1\n 3  2013     1     1\n 4  2013     1     1\n 5  2013     1     1\n 6  2013     1     1\n 7  2013     1     1\n 8  2013     1     1\n 9  2013     1     1\n10  2013     1     1\n# ℹ 336,766 more rows\n\n\n\nselect(flights, -(year:day))# знак минуса используется для того, чтобы, наоборот, исключить какие-то переменные\n\n# A tibble: 336,776 × 16\n   dep_time sched_dep_time dep_delay arr_time sched_arr_time arr_delay carrier\n      &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt; &lt;chr&gt;  \n 1      517            515         2      830            819        11 UA     \n 2      533            529         4      850            830        20 UA     \n 3      542            540         2      923            850        33 AA     \n 4      544            545        -1     1004           1022       -18 B6     \n 5      554            600        -6      812            837       -25 DL     \n 6      554            558        -4      740            728        12 UA     \n 7      555            600        -5      913            854        19 B6     \n 8      557            600        -3      709            723       -14 EV     \n 9      557            600        -3      838            846        -8 B6     \n10      558            600        -2      753            745         8 AA     \n# ℹ 336,766 more rows\n# ℹ 9 more variables: flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;,\n#   air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nЕсть еще несколько полезных функций, которые идут в паре с select() и делают жизнь проще:\nstarts_with(\"abc\"): имена переменных начинаются на “abc”.\nends_with(\"xyz\"): имена переменных заканчиваются на “xyz”.\ncontains(\"ijk\"): имя переменной содержит “ijk”.\nИногда, в процессе обработки нам требуется переименовать переменную другим способом. Для этой цели подходит функция rename:\n\nrename(flights, tail_num = tailnum)\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tail_num &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n Самостоятельное задание:\n\nИспользовать все возможные способы для того, чтобы отобрать переменные dep_time, dep_delay, arr_time и arr_delay.\nЧто случится, если Вы включите одну и ту же переменную несколько раз в запрос функции select()?",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Tidyverse и трансформация данных</span>"
    ]
  },
  {
    "objectID": "TidyverseTransformations.html#создаем-новую-переменную-с-функцией-mutate",
    "href": "TidyverseTransformations.html#создаем-новую-переменную-с-функцией-mutate",
    "title": "7  Tidyverse и трансформация данных",
    "section": "7.6 Создаем новую переменную с функцией mutate()",
    "text": "7.6 Создаем новую переменную с функцией mutate()\nДа, сейчас явно что-то будет про мутации)))\nПомимо того, что в процессе анализа нам приходится что-то постоянно отбирать (например, только жителей сел или городов, какой-то регион исследования, лиц определенного возраста и пола), часто приходится вычислять новые переменные на основе старых - высчитывать возраст, средний доход, количество детей и т. д.\nВ этом нам может помочь функция mutate().\nПример: создадим новый набор flights_sml, куда отберем даты полета, расстояние и переменные, оканчивающиеся на “delay”. Затем, содадим две новых переменных:\n\nскорость (speed)\nколичество минут, которые самолет смог догнать при задержке рейса (как разность между задержкой вылета и прилета) (gain).\n\n\nflights_sml &lt;- select(flights,\n  year:day,\n  ends_with(\"delay\"),\n  distance,\n  air_time\n)\nmutate(flights_sml,\n  gain = dep_delay - arr_delay,\n  speed = distance / (air_time / 60)\n)\n\n# A tibble: 336,776 × 9\n    year month   day dep_delay arr_delay distance air_time  gain speed\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  2013     1     1         2        11     1400      227    -9  370.\n 2  2013     1     1         4        20     1416      227   -16  374.\n 3  2013     1     1         2        33     1089      160   -31  408.\n 4  2013     1     1        -1       -18     1576      183    17  517.\n 5  2013     1     1        -6       -25      762      116    19  394.\n 6  2013     1     1        -4        12      719      150   -16  288.\n 7  2013     1     1        -5        19     1065      158   -24  404.\n 8  2013     1     1        -3       -14      229       53    11  259.\n 9  2013     1     1        -3        -8      944      140     5  405.\n10  2013     1     1        -2         8      733      138   -10  319.\n# ℹ 336,766 more rows\n\n\nОтметим, что мы можем «не отходя от кассы» использовать новые переменные в последующих расчетах для создания других переменных. Например, давайте создадим переменную gain (см. пример выше), но и посчитаем время полета в часах и среднее количество минут, которое нагонял самолет каждый час:\n\nmutate(flights_sml,\n  gain = dep_delay - arr_delay,\n  hours = air_time / 60,\n  gain_per_hour = gain / hours\n)\n\n# A tibble: 336,776 × 10\n    year month   day dep_delay arr_delay distance air_time  gain hours\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  2013     1     1         2        11     1400      227    -9 3.78 \n 2  2013     1     1         4        20     1416      227   -16 3.78 \n 3  2013     1     1         2        33     1089      160   -31 2.67 \n 4  2013     1     1        -1       -18     1576      183    17 3.05 \n 5  2013     1     1        -6       -25      762      116    19 1.93 \n 6  2013     1     1        -4        12      719      150   -16 2.5  \n 7  2013     1     1        -5        19     1065      158   -24 2.63 \n 8  2013     1     1        -3       -14      229       53    11 0.883\n 9  2013     1     1        -3        -8      944      140     5 2.33 \n10  2013     1     1        -2         8      733      138   -10 2.3  \n# ℹ 336,766 more rows\n# ℹ 1 more variable: gain_per_hour &lt;dbl&gt;\n\n\n Самостоятельное задание:\n\nСравните air_time` и `arr_time - dep_time. Что мы можем увидеть? Какие выводы можно сделать?\nСравните dep_time, sched_dep_time и dep_delay. Как эти переменные связаны друг с другом?\nНа основе уже знакомого нам набора iris создайте новую переменную - sepal_ratio, в которой посчитайте отношение длины чашелистника (Sepal.Length) к его ширине (Sepal.Width) и переменную petal_ratio, в которой посчитайте отношение длины лепестка (Petal.Length) к его ширине (Petal.Width). Сохраните данные в новом наборе iris2.\n\n\n\nЕсли трудно, код можно посмотреть здесь\niris2&lt;- mutate(iris, sepal_ratio=Sepal.Length/Sepal.Width, petal_ratio=Petal.Length/Petal.Width)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Tidyverse и трансформация данных</span>"
    ]
  },
  {
    "objectID": "TidyverseTransformations.html#изменения-в-нескольких-переменных-mutate-across",
    "href": "TidyverseTransformations.html#изменения-в-нескольких-переменных-mutate-across",
    "title": "7  Tidyverse и трансформация данных",
    "section": "7.7 Изменения в нескольких переменных: mutate + across",
    "text": "7.7 Изменения в нескольких переменных: mutate + across\nФункция mutate() не обязательно означает, что мы будем вычислять новые переменные, вполне возможно мы будем как-то видоизменять те переменные, которые у нас уже есть в наборе данных. Например, мы хотим округлить значение только что полученных нами переменных sepal_ratio petal_ratio (см. самостоятельная работа выше, упражнение 3) до одного знака после запятой.\n\n\nWarning: `as.tibble()` was deprecated in tibble 2.0.0.\nℹ Please use `as_tibble()` instead.\nℹ The signature and semantics have changed, see `?as_tibble`.\n\n\n\nmutate(iris2, across(c(sepal_ratio, petal_ratio), round, 1))# функция round используется для округления, цифра 1 означает, что мы будем округлять до одного знака после запятой \n\n# A tibble: 150 × 7\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species sepal_ratio\n          &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;         &lt;dbl&gt;\n 1          5.1         3.5          1.4         0.2 setosa          1.5\n 2          4.9         3            1.4         0.2 setosa          1.6\n 3          4.7         3.2          1.3         0.2 setosa          1.5\n 4          4.6         3.1          1.5         0.2 setosa          1.5\n 5          5           3.6          1.4         0.2 setosa          1.4\n 6          5.4         3.9          1.7         0.4 setosa          1.4\n 7          4.6         3.4          1.4         0.3 setosa          1.4\n 8          5           3.4          1.5         0.2 setosa          1.5\n 9          4.4         2.9          1.4         0.2 setosa          1.5\n10          4.9         3.1          1.5         0.1 setosa          1.6\n# ℹ 140 more rows\n# ℹ 1 more variable: petal_ratio &lt;dbl&gt;\n\n\nМы могли бы написать наш код чуть по-другому:\n\niris2 %&gt;%\n  mutate(across(c(sepal_ratio, petal_ratio), round, 1))\n\n# A tibble: 150 × 7\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species sepal_ratio\n          &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;         &lt;dbl&gt;\n 1          5.1         3.5          1.4         0.2 setosa          1.5\n 2          4.9         3            1.4         0.2 setosa          1.6\n 3          4.7         3.2          1.3         0.2 setosa          1.5\n 4          4.6         3.1          1.5         0.2 setosa          1.5\n 5          5           3.6          1.4         0.2 setosa          1.4\n 6          5.4         3.9          1.7         0.4 setosa          1.4\n 7          4.6         3.4          1.4         0.3 setosa          1.4\n 8          5           3.4          1.5         0.2 setosa          1.5\n 9          4.4         2.9          1.4         0.2 setosa          1.5\n10          4.9         3.1          1.5         0.1 setosa          1.6\n# ℹ 140 more rows\n# ℹ 1 more variable: petal_ratio &lt;dbl&gt;\n\n\n\nСтоп! Что за новый знак %&gt;%?\nЭто оператор, который позволяет сократить код и написать его по типу “матрешки”. По-английски это называется “pipe-operator”, здесь явная аналогия с телескопическими трубами, которые как бы вкладываются друг с другом. Так и наши различные функции dplyr следуют друг за другом уже без лиших аргументов и дублирования информации о наборе данных, лишних скобок и знаков $, с помощью которых мы вводим имя переменной из набора. Код становится более читаемым и быстрым.\n\nГрафически это можно представить вот так:\n В dplyr оператор %&gt;% попал из библиотеки magrittr, разработчики которой (Stefan Milton Bache, Hadley Wickham, Lionel Henry) преследовали две основных цели - сократить процесс написания кода и улучить его поддержку.\nНазвана библиотека в честь бельгийского художника-сюрреалиста Рене Магритта (1898-1967), который был известен своими остроумными и загадочными работами.\n Причем тут трубы, спросите вы? А вот причем. У Магритта есть одна знаменитая картина – «Вероломство образов», которую автор создал в 1929 году, увлекшись исследованием связи в искусстве между словами и образами. На картине изображена курительная трубка, выглядит она очень правдоподобно и нарисована тщательно. Вдоль нижней кромки полотна идет надпись, сделанная крупным каллиграфическим почерком: «Ceci n’est pas une pipe» - «Это не трубка». Картина породила множество интерпретаций, а сам автор по поводу нее сказал следующее: «Разве я написал неправду? Да, на картине нарисована трубка, но разве вы можете раскурить ее? Нет, потому что это всего лишь репрезентация. И если бы я написал “Это - трубка”, то я бы солгал!».\n В общем, разработчики отдали дань творчеству автора, и в языке R появился новый оператор.\n\n\n\n\n\n\nСовет\n\n\n\nЧтобы быстро набрать пайп-оператор нужно одновременно нажать три клавиши:\nCtrl+Shift+M\n\n\nНачиная с версии R 4.1.0. в базовом R появился свой, «нативный» оператор, почти аналогичный %&gt;%. Выглядит он как |&lt; и “ведет себя” почти всегда так же. Есть некоторые нюансы, однако обсуждать мы их пока не будем. В качестве дополнительного матерала для тех, кто хочет действительно разобраться, отсылаю к статье: https://www.tidyverse.org/blog/2023/04/base-vs-magrittr-pipe/\nВ настройках RStudio можно указать, какой именно операторв вы хотите использовать, но только один:",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Tidyverse и трансформация данных</span>"
    ]
  },
  {
    "objectID": "TidyverseTransformations.html#вычисление-основных-статистик-с-функцией-summarise-и-group_by",
    "href": "TidyverseTransformations.html#вычисление-основных-статистик-с-функцией-summarise-и-group_by",
    "title": "7  Tidyverse и трансформация данных",
    "section": "7.8 Вычисление основных статистик с функцией summarise() и group_by()",
    "text": "7.8 Вычисление основных статистик с функцией summarise() и group_by()\nЕще одна функция, которую мы изучим в рамках данного урока, – summarise(). Как следует из названия, ее предназначение - рассчитывать какие-то новые значения на основе разнообразных функций - арифметических, статистических и др. Например, посчитаем среднее количество минут задержки вылета по набору полетов:\n\nsummarise(flights, dep_delay_mean = mean(dep_delay, na.rm = TRUE))#na.rm = TRUE - данный аргумент удаляет из анализа все пропущенные значения (na - not avaliable, rm - remove), в противном случае вычисление среднего не возможно.\n\n# A tibble: 1 × 1\n  dep_delay_mean\n           &lt;dbl&gt;\n1           12.6\n\n\nФункция summarise() часто идет в паре с другой функцией - group_by(), позволяющей провести анализ в отдельных подгруппах. Соответственно, когда мы применяем функции dplyr к сгруппированным данным, они автоматически рассчитываются для каждой подгруппы по отдельности. Например, мы можем применить код из примера выше, но предварительно сгруппировать данные по месяцам. Получится следующее:\n\nflights %&gt;%\ngroup_by(month) %&gt;%\nsummarise(dep_delay_delay = mean(dep_delay, na.rm = TRUE))\n\n# A tibble: 12 × 2\n   month dep_delay_delay\n   &lt;int&gt;           &lt;dbl&gt;\n 1     1           10.0 \n 2     2           10.8 \n 3     3           13.2 \n 4     4           13.9 \n 5     5           13.0 \n 6     6           20.8 \n 7     7           21.7 \n 8     8           12.6 \n 9     9            6.72\n10    10            6.24\n11    11            5.44\n12    12           16.6 \n\n\nИз таблицы видно, что рейсы в среднем задерживались чаще в летние месяцы (июне и июле) и декабре.\nПриведем еще один пример совместного использования функций: проведем группировку по месту назначения и посчитаем общее количество полетов, среднее расстояние и среднюю задержку вылета в минутах, а также отберем 5 направлений с максимальными средними значениями по задержке вылетов и 5 направлений с минимальным задержками:\n\ndelays &lt;- flights %&gt;%\n  group_by(dest) %&gt;%\n  summarise(\n    Всего = n(),\n    Сред_расст = mean(distance, na.rm = TRUE),\n    Ср_задержка_вылета = mean(arr_delay, na.rm = TRUE)\n  ) %&gt;% \n  arrange(desc(Ср_задержка_вылета)) %&gt;% \n  slice(c(1:5, n()-5:n()))#slice отбирает строки, в скобках указываются номера, 1:5 - первые пять, а n()-5:n() - последние пять: n()-5 - номер строки минус 5 указывает на начало интервала,n() - общее количество строк - на конец. Мы могли бы указать это как 101:105, но допустим, мы не знаем, сколько строк, такой подход позволяет избежать ручного ввода номеров строк.\ndelays\n\n# A tibble: 105 × 4\n   dest  Всего Сред_расст Ср_задержка_вылета\n   &lt;chr&gt; &lt;int&gt;      &lt;dbl&gt;              &lt;dbl&gt;\n 1 CAE     116       604.            41.8   \n 2 TUL     315      1215             33.7   \n 3 OKC     346      1325             30.6   \n 4 JAC      25      1876.            28.1   \n 5 TYS     631       639.            24.1   \n 6 ANC       8      3370             -2.5   \n 7 HNL     707      4973.            -1.37  \n 8 SEA    3923      2413.            -1.10  \n 9 MVY     221       173             -0.286 \n10 LGB     668      2465             -0.0620\n# ℹ 95 more rows\n\n\nХотя в таблице :::callout-note Другие полезные функции: - median() - медиана - sd() - стандартное отклонение - min() - минимум - max() - максимум - first() - первое значение - nth() - значение определенного порядка (2, 3 и т.д.) - last() - последнее - n() - количество, размер группы - n_distinct() - количество уникальных значений :::\nНапример:\n\nflights %&gt;%\nna.omit() %&gt;% #na.omit() - опускает пропущенные значения\ngroup_by(month) %&gt;%\n  summarise(\n    first_dep = first(dep_time),\n    last_dep = last(dep_time)\n  )\n\n# A tibble: 12 × 3\n   month first_dep last_dep\n   &lt;int&gt;     &lt;int&gt;    &lt;int&gt;\n 1     1       517     2354\n 2     2       456     2359\n 3     3         4     2358\n 4     4       454     2351\n 5     5         9     2355\n 6     6         2     2359\n 7     7         1     2352\n 8     8        12     2359\n 9     9         9     2349\n10    10       447     2357\n11    11         5     2354\n12    12        13     2356\n\n\nЕще пример: сгруппируем полеты по месту назначения и посчитаем уникальных перевозчиков, отсортировав нашу таблицу по месту назначения.\n\nflights %&gt;%\nna.omit() %&gt;%\ngroup_by(dest) %&gt;%\n  summarise(carriers = n_distinct(carrier)) %&gt;%\n  arrange(desc(carriers))\n\n# A tibble: 104 × 2\n   dest  carriers\n   &lt;chr&gt;    &lt;int&gt;\n 1 ATL          7\n 2 BOS          7\n 3 CLT          7\n 4 ORD          7\n 5 TPA          7\n 6 AUS          6\n 7 DCA          6\n 8 DTW          6\n 9 IAD          6\n10 MSP          6\n# ℹ 94 more rows",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Tidyverse и трансформация данных</span>"
    ]
  },
  {
    "objectID": "TidyverseTransformations.html#сочетание-нескольких-трансформаций-mutate-filter-summarise",
    "href": "TidyverseTransformations.html#сочетание-нескольких-трансформаций-mutate-filter-summarise",
    "title": "7  Tidyverse и трансформация данных",
    "section": "7.9 Сочетание нескольких трансформаций (mutate, filter, summarise)",
    "text": "7.9 Сочетание нескольких трансформаций (mutate, filter, summarise)\nГруппировка обычно сопутствует функции summarise(),но ее также можно сочетать с mutate() и filter():\nНапример, мы хотим оставить в наболе данные только по наиболее популярным направлениям, принявшим не менее 5000 полетов за год:\n\nflights %&gt;%\n  group_by(dest) %&gt;%\n  filter(n()&gt;17000)\n\n# A tibble: 34,498 × 19\n# Groups:   dest [2]\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      554            600        -6      812            837\n 2  2013     1     1      554            558        -4      740            728\n 3  2013     1     1      558            600        -2      753            745\n 4  2013     1     1      600            600         0      837            825\n 5  2013     1     1      606            610        -4      837            845\n 6  2013     1     1      608            600         8      807            735\n 7  2013     1     1      615            615         0      833            842\n 8  2013     1     1      629            630        -1      824            810\n 9  2013     1     1      656            700        -4      854            850\n10  2013     1     1      658            700        -2      944            939\n# ℹ 34,488 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nИли, например, нам понадобилось посчитать по каждому направлению среднее количество полетов и сохранить его в переменной mean:\n\nflights %&gt;%\n  group_by(dest) %&gt;%\n  mutate(mean=mean(n()), .keep = \"used\") #.keep = \"used\" - оставляет в наборе только используемые переменные, в противном случае останется весь набор\n\n# A tibble: 336,776 × 2\n# Groups:   dest [105]\n   dest   mean\n   &lt;chr&gt; &lt;dbl&gt;\n 1 IAH    7198\n 2 IAH    7198\n 3 MIA   11728\n 4 BQN     896\n 5 ATL   17215\n 6 ORD   17283\n 7 FLL   12055\n 8 IAD    5700\n 9 MCO   14082\n10 ORD   17283\n# ℹ 336,766 more rows",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Tidyverse и трансформация данных</span>"
    ]
  },
  {
    "objectID": "TidyverseTransformations.html#создание-новых-переменных-из-старых-по-условиям-if_else-и-case_when",
    "href": "TidyverseTransformations.html#создание-новых-переменных-из-старых-по-условиям-if_else-и-case_when",
    "title": "7  Tidyverse и трансформация данных",
    "section": "7.10 Создание новых переменных из старых по условиям: if_else() и case_when()",
    "text": "7.10 Создание новых переменных из старых по условиям: if_else() и case_when()\nТакого рода перекодировки в исследовательской практике встречаются очень часто, когда мы хотим объединить некоторые значения в группы или заменить на другие значения. В решении этой задачи нам помогут две полезные функции: if_else()и case_when().\n\n7.10.1 if_else()\nЭта функция позволяет нам перекодировать значения, совпадающие или несовпадающие с некоторым условием, и одновременно обрабатывать отсутствующие значения. Для того, чтобы корректно применить эту функцию, нам нужно знать, какое значение в наших данных будет условно «истинным, корректным», какое «ложным, неправильным», и какое значение будут принимать пропущенные значения.\nОбщая формула для этой функции выглядит так:\nif_else(condition, true, false, missing = NULL)\nВ аргументах функции задается следующее: condition - условие, которое мы проверяем true - значение, которое присваиваем, если условие выполняется false- значение, которое присваиваем, если условие не выполняется.\nПриведем простой пример: Создадим вектор x, содержащий значения от -10 до +10, а затем все положительные значения заменим на 555:\n\nx &lt;- c(-10:10)\nif_else(x &gt; 0, 555, x)\n\n [1] -10  -9  -8  -7  -6  -5  -4  -3  -2  -1   0 555 555 555 555 555 555 555 555\n[20] 555 555\n\n\nКонструкция if_else() очень часто используется вместе с mutate()для того, чтобы создать новые переменные.\nДавайте создадим по набору iris новую переменную - sepal_group, которую перекодируем следующим образом: если значение Sepal.Length равно превышает среднее значение по всем наблюдениям - 1, если меньше - 0.\n\niris3&lt;-iris %&gt;%\n  mutate(sepal_group = if_else(Sepal.Length &gt;= mean(Sepal.Length), 1, 0))\n\n Самостоятельное задание:\n1.Создайте в набореiris3 переменную petal_group, таким же способом, как мы только что создавали sepal_group. 2. Перекодируйте переменную petal_group с помощью if_else() так, чтобы 1 были трансформированы в “Больше среднего”, а 0 - “Меньше среднего”.\n\n\n7.10.2 case_when()\nКогда условий несколько, можно использовать вложенные конструкции if_else(), но это не очень удобно. Вместо это лучше использовать специальную функцию case_when(), которая как раз предназначена для таких случаев.\nДопустим, нам нужно перекодировать полеты из набора flights в зависимости от расстояния (dist)и создать переменную dist_group, в которой бы все полеты были бы распределены по четырем категориям: “менее 500 миль”, “от 500 до 1000 миль”, “от 1000 до 1500 миль” и “свыше 1500 миль”. Получается, что нам нужно сверить множество условий, и, значит, это работа для case_when ():\n\nflights %&gt;%\n  mutate(dist_group=  case_when(\n  distance &lt; 500 ~ \"менее 500 миль\",\n  between(distance, 501, 1000) ~ \"от 500 до 1000 миль\",\n  between(distance, 1001, 1500) ~ \"от 1000 до 1500 миль\",\n  distance &gt; 1500 ~ \"свыше 1500 миль\"), .keep=\"used\")\n\n# A tibble: 336,776 × 2\n   distance dist_group          \n      &lt;dbl&gt; &lt;chr&gt;               \n 1     1400 от 1000 до 1500 миль\n 2     1416 от 1000 до 1500 миль\n 3     1089 от 1000 до 1500 миль\n 4     1576 свыше 1500 миль     \n 5      762 от 500 до 1000 миль \n 6      719 от 500 до 1000 миль \n 7     1065 от 1000 до 1500 миль\n 8      229 менее 500 миль      \n 9      944 от 500 до 1000 миль \n10      733 от 500 до 1000 миль \n# ℹ 336,766 more rows\n\n\n Самостоятельное задание:\n\nПо набору iris создайте переменную sepal4cats, в которой бы содержались следующие значение о длине чашелистника: “Меньше 5,0”, “от 5,1 до 5.8”, “от 5.9 до 6.4” и “свыше 6.5”.\nПосчитайте количество ирисов в каждой группе.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Tidyverse и трансформация данных</span>"
    ]
  },
  {
    "objectID": "TidyverseTransformations.html#между-широким-и-длинным-форматом",
    "href": "TidyverseTransformations.html#между-широким-и-длинным-форматом",
    "title": "7  Tidyverse и трансформация данных",
    "section": "7.11 Между «широким» и «длинным» форматом",
    "text": "7.11 Между «широким» и «длинным» форматом\nПоследнее, чем мы займемся в рамках данной темы, будет преобразование из «широкого» в «длинный» формат и обратно.\n\n7.11.1 Случай 1. Из «широкого» в «длинное»\nИногда случается так, что данные представлены в виде двумерной таблицы, уже подготовленной для описания, но такие данные не очень подходят для дальнейшего анализа.\nНапример, давайте посмотрим на таблицу взаимосвязей между уровнем дохода и религиозной принадлежностью:\n\nrelig_income\n\n# A tibble: 18 × 11\n   religion `&lt;$10k` `$10-20k` `$20-30k` `$30-40k` `$40-50k` `$50-75k` `$75-100k`\n   &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n 1 Agnostic      27        34        60        81        76       137        122\n 2 Atheist       12        27        37        52        35        70         73\n 3 Buddhist      27        21        30        34        33        58         62\n 4 Catholic     418       617       732       670       638      1116        949\n 5 Don’t k…      15        14        15        11        10        35         21\n 6 Evangel…     575       869      1064       982       881      1486        949\n 7 Hindu          1         9         7         9        11        34         47\n 8 Histori…     228       244       236       238       197       223        131\n 9 Jehovah…      20        27        24        24        21        30         15\n10 Jewish        19        19        25        25        30        95         69\n11 Mainlin…     289       495       619       655       651      1107        939\n12 Mormon        29        40        48        51        56       112         85\n13 Muslim         6         7         9        10         9        23         16\n14 Orthodox      13        17        23        32        32        47         38\n15 Other C…       9         7        11        13        13        14         18\n16 Other F…      20        33        40        46        49        63         46\n17 Other W…       5         2         3         4         2         7          3\n18 Unaffil…     217       299       374       365       341       528        407\n# ℹ 3 more variables: `$100-150k` &lt;dbl&gt;, `&gt;150k` &lt;dbl&gt;,\n#   `Don't know/refused` &lt;dbl&gt;\n\n\nФункция pivot_longer() приведет данные в более удобоваримый вид:\nКомментарии: - cols- на основе каких переменных мы “переворачиваем” данные - names_to - как будет называться новая переменная (или переменные, если их несколько) - values_to- как будет называться переменная, содержащая частоту\n\nrelig_income %&gt;% \n  pivot_longer(\n    cols = !religion, \n    names_to = \"income\", \n    values_to = \"count\"\n  )\n\n# A tibble: 180 × 3\n   religion income             count\n   &lt;chr&gt;    &lt;chr&gt;              &lt;dbl&gt;\n 1 Agnostic &lt;$10k                 27\n 2 Agnostic $10-20k               34\n 3 Agnostic $20-30k               60\n 4 Agnostic $30-40k               81\n 5 Agnostic $40-50k               76\n 6 Agnostic $50-75k              137\n 7 Agnostic $75-100k             122\n 8 Agnostic $100-150k            109\n 9 Agnostic &gt;150k                 84\n10 Agnostic Don't know/refused    96\n# ℹ 170 more rows\n\n\nДавайте посмотрим на еще один вариант:\n\nbillboard\n\n# A tibble: 317 × 79\n   artist     track date.entered   wk1   wk2   wk3   wk4   wk5   wk6   wk7   wk8\n   &lt;chr&gt;      &lt;chr&gt; &lt;date&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 2 Pac      Baby… 2000-02-26      87    82    72    77    87    94    99    NA\n 2 2Ge+her    The … 2000-09-02      91    87    92    NA    NA    NA    NA    NA\n 3 3 Doors D… Kryp… 2000-04-08      81    70    68    67    66    57    54    53\n 4 3 Doors D… Loser 2000-10-21      76    76    72    69    67    65    55    59\n 5 504 Boyz   Wobb… 2000-04-15      57    34    25    17    17    31    36    49\n 6 98^0       Give… 2000-08-19      51    39    34    26    26    19     2     2\n 7 A*Teens    Danc… 2000-07-08      97    97    96    95   100    NA    NA    NA\n 8 Aaliyah    I Do… 2000-01-29      84    62    51    41    38    35    35    38\n 9 Aaliyah    Try … 2000-03-18      59    53    38    28    21    18    16    14\n10 Adams, Yo… Open… 2000-08-26      76    76    74    69    68    67    61    58\n# ℹ 307 more rows\n# ℹ 68 more variables: wk9 &lt;dbl&gt;, wk10 &lt;dbl&gt;, wk11 &lt;dbl&gt;, wk12 &lt;dbl&gt;,\n#   wk13 &lt;dbl&gt;, wk14 &lt;dbl&gt;, wk15 &lt;dbl&gt;, wk16 &lt;dbl&gt;, wk17 &lt;dbl&gt;, wk18 &lt;dbl&gt;,\n#   wk19 &lt;dbl&gt;, wk20 &lt;dbl&gt;, wk21 &lt;dbl&gt;, wk22 &lt;dbl&gt;, wk23 &lt;dbl&gt;, wk24 &lt;dbl&gt;,\n#   wk25 &lt;dbl&gt;, wk26 &lt;dbl&gt;, wk27 &lt;dbl&gt;, wk28 &lt;dbl&gt;, wk29 &lt;dbl&gt;, wk30 &lt;dbl&gt;,\n#   wk31 &lt;dbl&gt;, wk32 &lt;dbl&gt;, wk33 &lt;dbl&gt;, wk34 &lt;dbl&gt;, wk35 &lt;dbl&gt;, wk36 &lt;dbl&gt;,\n#   wk37 &lt;dbl&gt;, wk38 &lt;dbl&gt;, wk39 &lt;dbl&gt;, wk40 &lt;dbl&gt;, wk41 &lt;dbl&gt;, wk42 &lt;dbl&gt;, …\n\n\nЭто данные о песнях их ротации по радиостанциям и о занимаемой позиции:\n\nbillboard %&gt;% \n  pivot_longer(\n    cols = starts_with(\"wk\"), \n    names_to = \"week\", \n    values_to = \"rank\",\n    values_drop_na = TRUE\n  )\n\n# A tibble: 5,307 × 5\n   artist  track                   date.entered week   rank\n   &lt;chr&gt;   &lt;chr&gt;                   &lt;date&gt;       &lt;chr&gt; &lt;dbl&gt;\n 1 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk1      87\n 2 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk2      82\n 3 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk3      72\n 4 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk4      77\n 5 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk5      87\n 6 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk6      94\n 7 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk7      99\n 8 2Ge+her The Hardest Part Of ... 2000-09-02   wk1      91\n 9 2Ge+her The Hardest Part Of ... 2000-09-02   wk2      87\n10 2Ge+her The Hardest Part Of ... 2000-09-02   wk3      92\n# ℹ 5,297 more rows\n\n\nНам возможно было бы интересно, сколько недель какой-либо трек продержался в чарте, но для этого нужно превратить неделю из строковой переменной в числовую. Это можно сделать следующим образом:\n\nbillboard %&gt;% \n  pivot_longer(\n    cols = starts_with(\"wk\"), \n    names_to = \"week\", \n    names_transform = readr::parse_number,\n    values_to = \"rank\",\n    values_drop_na = TRUE,\n  )\n\n# A tibble: 5,307 × 5\n   artist  track                   date.entered  week  rank\n   &lt;chr&gt;   &lt;chr&gt;                   &lt;date&gt;       &lt;dbl&gt; &lt;dbl&gt;\n 1 2 Pac   Baby Don't Cry (Keep... 2000-02-26       1    87\n 2 2 Pac   Baby Don't Cry (Keep... 2000-02-26       2    82\n 3 2 Pac   Baby Don't Cry (Keep... 2000-02-26       3    72\n 4 2 Pac   Baby Don't Cry (Keep... 2000-02-26       4    77\n 5 2 Pac   Baby Don't Cry (Keep... 2000-02-26       5    87\n 6 2 Pac   Baby Don't Cry (Keep... 2000-02-26       6    94\n 7 2 Pac   Baby Don't Cry (Keep... 2000-02-26       7    99\n 8 2Ge+her The Hardest Part Of ... 2000-09-02       1    91\n 9 2Ge+her The Hardest Part Of ... 2000-09-02       2    87\n10 2Ge+her The Hardest Part Of ... 2000-09-02       3    92\n# ℹ 5,297 more rows\n\n\n\n\n7.11.2 Случай 2: из «длинного» в «широкий» формат\nФункция pivot_wider() является противоположной pivot_longer(): она расширяет набор данных за счет создания новых столбцов и сокращения количества строк. Например, это может быть полезно для написания отчетов или создания презентаций.\nЧтобы посмотреть на эту функцию в действии, давайте создадим “длинные” данные из набора о пингвинах:\n\nlibrary(palmerpenguins)\n\n\nПрисоединяю пакет: 'palmerpenguins'\n\n\nСледующие объекты скрыты от 'package:datasets':\n\n    penguins, penguins_raw\n\npenguins_long &lt;- penguins %&gt;%  \n  mutate(sample = row_number()) %&gt;%  \n  pivot_longer(contains(\"_\"),\n               names_to = c(\"part\", \"measure\" , \"unit\"),\n               names_sep = \"_\",\n               values_drop_na = TRUE)\npenguins_long\n\n# A tibble: 1,368 × 9\n   species island    sex     year sample part    measure unit   value\n   &lt;fct&gt;   &lt;fct&gt;     &lt;fct&gt;  &lt;int&gt;  &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;  &lt;dbl&gt;\n 1 Adelie  Torgersen male    2007      1 bill    length  mm      39.1\n 2 Adelie  Torgersen male    2007      1 bill    depth   mm      18.7\n 3 Adelie  Torgersen male    2007      1 flipper length  mm     181  \n 4 Adelie  Torgersen male    2007      1 body    mass    g     3750  \n 5 Adelie  Torgersen female  2007      2 bill    length  mm      39.5\n 6 Adelie  Torgersen female  2007      2 bill    depth   mm      17.4\n 7 Adelie  Torgersen female  2007      2 flipper length  mm     186  \n 8 Adelie  Torgersen female  2007      2 body    mass    g     3800  \n 9 Adelie  Torgersen female  2007      3 bill    length  mm      40.3\n10 Adelie  Torgersen female  2007      3 bill    depth   mm      18  \n# ℹ 1,358 more rows\n\n\nА теперь превратим их в широкий формат:\n\npenguins_long |&gt; \n  pivot_wider(names_from = c(\"part\", \"measure\", \"unit\"),\n              names_sep = \"_\",\n              values_from = value)\n\n# A tibble: 342 × 9\n   species island    sex     year sample bill_length_mm bill_depth_mm\n   &lt;fct&gt;   &lt;fct&gt;     &lt;fct&gt;  &lt;int&gt;  &lt;int&gt;          &lt;dbl&gt;         &lt;dbl&gt;\n 1 Adelie  Torgersen male    2007      1           39.1          18.7\n 2 Adelie  Torgersen female  2007      2           39.5          17.4\n 3 Adelie  Torgersen female  2007      3           40.3          18  \n 4 Adelie  Torgersen female  2007      5           36.7          19.3\n 5 Adelie  Torgersen male    2007      6           39.3          20.6\n 6 Adelie  Torgersen female  2007      7           38.9          17.8\n 7 Adelie  Torgersen male    2007      8           39.2          19.6\n 8 Adelie  Torgersen &lt;NA&gt;    2007      9           34.1          18.1\n 9 Adelie  Torgersen &lt;NA&gt;    2007     10           42            20.2\n10 Adelie  Torgersen &lt;NA&gt;    2007     11           37.8          17.1\n# ℹ 332 more rows\n# ℹ 2 more variables: flipper_length_mm &lt;dbl&gt;, body_mass_g &lt;dbl&gt;",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Tidyverse и трансформация данных</span>"
    ]
  },
  {
    "objectID": "TidyverseTransformations.html#самостоятельная-работа",
    "href": "TidyverseTransformations.html#самостоятельная-работа",
    "title": "7  Tidyverse и трансформация данных",
    "section": "7.12 Самостоятельная работа",
    "text": "7.12 Самостоятельная работа\n\nДля каждого направления (dest) по набору flights посчитайте общее пройденное расстояние, используя функцию sum().\nПерекодируйте переменную month таким образом, чтобы вместо цифр были названия месяцев.\nНа основе переменной air_time, показывающей время в полете в минутах, создайте переменную air_time_hours, в которой время переведено в часы.\nНабор данных о пингвинах penguins из библиотеки palmerpenguinsсгруппируйте по видам и полу пингвинов (species, sex) и посчитайте средний размер клюва в мм (bill_length_mm). Затем трансформируйте результаты так, чтобы получилась вот такая таблица (там будут пропущенные значения по полу, их нужно будет убрать):\n\n\n\n\n\n\nspecies\nfemale\nmale\n\n\n\n\nAdelie\n37.3\n40.4\n\n\nChinstrap\n46.6\n51.1\n\n\nGentoo\n45.6\n49.5\n\n\n\n\n\n\n\n\nВсе результаты оформить в виде документа RMarkdown и опубликовать на RPubs. Результаты прикрепить в виде ссылки.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Tidyverse и трансформация данных</span>"
    ]
  },
  {
    "objectID": "Install-R.html#установка-ide",
    "href": "Install-R.html#установка-ide",
    "title": "2  Установка и начало работы с R",
    "section": "2.2 Установка IDE",
    "text": "2.2 Установка IDE\nВместе с R устанавливается небольшая консоль, в которой можно набирать команды на R, но работать в ней не очень удобно, поэтому большинство пользователей предпочитает работать со специальным интерфейсом, или интегрированной средой разработки – Integrated Development Environement, IDE. Наиболее популярной IDE является RStudio (с другими средами, в которых можно работать в R, такими как Visual Studio, Emacs, Eclipse и другими, Вы можете познакомиться самостоятельно).\nЭто бесплатная программа, скачать которую можно по ссылке.\nhttps://posit.co/products/open-source/rstudio/\nШаги по установке не очень трудны и не потребуют каких-то особых навыков, но на всякий случай, можно обратиться к однму из обучающих видео:\n\n\n\n\n\n\nУведомление\n\n\n\nВ комьютерных классах устанавливать ничего не нужно, эти инструкции пригодятся для домашнего использования.\n\n\n\n\n\n\n\n\nRStudio online\n\n\n\nКроме “стационарного” использования RStudio, компания Posit предоставляет возможность использования облачных технологий для работы с RStudio онлайн. Бесплатный план позволяет создать до 25 проектов и предоставляет 25 часов в месяц вычислительного времени. Этого будет достаточно для наших учебных проектов. Для получения доступа к Posit Cloud необходимо зарегистрироваться на сайте https://posit.cloud/ или войти с помощью социальных сетей и создать новый проект.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Установка и начало работы с R</span>"
    ]
  },
  {
    "objectID": "Install-R.html#какие-операторы-используются-для-математических-действий",
    "href": "Install-R.html#какие-операторы-используются-для-математических-действий",
    "title": "2  Установка и начало работы с R",
    "section": "2.4 Какие операторы используются для математических действий?",
    "text": "2.4 Какие операторы используются для математических действий?\nДовольно часто в процессе обработки данных возникает необходимость вычисления новых переменных, при этом могут потребоваться дополнительные операторы для совершения математических действий. Со сложением и вычитанием все довольно сложно, но что еще?\nВ таблице ниже представлены основные операторы с примерами их использования:\n\n\n\n\n\n\nУведомление\n\n\n\n\n\n\nОператор\nОписание\n\n\n+\nсложение\n\n\n-\nвычитание\n\n\n*\nумножение\n\n\n/\nделение\n\n\n^ или **\nвозведение в степень\n\n\nx %% y\nостаток от деления (x mod y) 5%%2 = 1\n\n\nx %/% y\nцелая часть при делени 5%/%2 =2\n\n\n\n\n\nЕсли мы что-то делаем неправильно, в консоли появится сообщение об ошибке:\n\n-1+a",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Установка и начало работы с R</span>"
    ]
  },
  {
    "objectID": "Install-R.html#о-пользе-ошибок",
    "href": "Install-R.html#о-пользе-ошибок",
    "title": "2  Установка и начало работы с R",
    "section": "2.5 О пользе ошибок",
    "text": "2.5 О пользе ошибок\nОшибок и предупреждающих сообщений бояться не надо, напротив, в таких сообщениях практически всегда находится ответ на вопрос, почему такая ошибка возникла, а часто - и возможное решение проблемы. :::",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Установка и начало работы с R</span>"
    ]
  },
  {
    "objectID": "Install-R.html#самостоятельная-работа",
    "href": "Install-R.html#самостоятельная-работа",
    "title": "2  Установка и начало работы с R",
    "section": "2.4 Самостоятельная работа",
    "text": "2.4 Самостоятельная работа\nВсе самостоятельные работы «зашиты» в отдельный пакет - r4ssr, созданный специально для курса «R в социологических исследованиях».\nПоскольку его код расположен на GitHub, воспользуемся следующим кодом:\n\ndevtools::install_github(\"domelia/r4ssr\", dependencies=TRUE)\n\nВместе с пакетом r4ssr установятся два других пакета - learnr и tutorial.helpers, которые нужны для корректного отображения заданий.\nДля получения первого опыта нужно будет выполнить работу Assignment0.\nДля запуска интерактивной среды запустить следующий код:\n\nlearnr::run_tutorial(\"Assignment0\", package=\"r4ssr\")\n\nОткроется браузер, необходимо представиться, выполнить задания и сохранить ответы в формате html.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Установка и начало работы с R</span>"
    ]
  },
  {
    "objectID": "Data-types-and-structures.html",
    "href": "Data-types-and-structures.html",
    "title": "3  Типы и структуры данных",
    "section": "",
    "text": "3.1 Типы данных в R\nТип данных (встречается также термин «вид данных») — фундаментальное понятие теории программирования.\nТип данных определяет множество значений, набор операций, которые можно применять к таким значениям и способ реализации хранения значений и выполнения операций. Любые данные, которыми оперируют программы, относятся к определённым типам.\nR может хранить и обрабатывать различные виды информации и типы данных:\n2 &gt; 3\n\n[1] FALSE\nМы можем присваивать логические значения в качестве значений переменных, используя полный или краткий вариант:\na &lt;- TRUE\nb &lt;- FALSE\n# Или\na &lt;- T\nb &lt;- F\nЛогический тип данных может интерпретироваться как числовой. При такой интерпретации значение TRUE эквивалентно числу 1, а значение FALSE — числу 0.\nНапример:\nTRUE + TRUE  # TRUE считается как 1\n\n[1] 2\nFALSE * 7 # FALSE принимается за 0\n\n[1] 0\nИли, вот еще интересный пример:\n(2 &lt; 3) + (1 == 2)\n\n[1] 1\nПочему получился именно такой результат?\n2 &lt; 3 - TRUE, то есть 1\n1 == 2 - FALSE, то есть 0\nTRUE + FALSE = 1 + 0 = 1\nНапример, проверим, принадлежит ли число -5,6 к типу numeric. Сделать это можно с помощью функции is.numeric().\nОбщая функция, с помощью которой проверяетс тип данных – class().\nis.numeric(-5.6)\n\n[1] TRUE\n\na &lt;- -11\nb &lt;- 13.37\nc &lt;- 1/137\nclass(a)\n\n[1] \"numeric\"\n\nclass(b)\n\n[1] \"numeric\"\n\nclass(c)\n\n[1] \"numeric\"\nС числовыми переменными мы можем совершать различные математические операции, например, сложение, вычитание, возведение в степень и другие.\nОсновные функции, которые могут встречаться в социальных исследованиях, приведены в таблице Вот таблица с базовыми математическими функциями R, отобранными для применения в анализе данных социальных исследований (описательная статистика, индексы, нормализация переменных).\na &lt;- -11\nb &lt;- 13.37\nc &lt;- 1/137\na + b * c\n\n[1] -10.90241\n\na^2 + sqrt(b) - c/5\n\n[1] 124.655\nНапример, проверим, к какому принадлежит переменная, содержащая значение 28:\nx &lt;- 28\nclass(x)\n\n[1] \"numeric\"\n\n#Проверим, является ли x целым числом\nis.integer(x)\n\n[1] FALSE\n#Создадим переменную y\nИногда, для того, чтобы имплицитно привести число к целочисленному типу в конце добавляется буква L:\ny&lt;-25L #при вводе, для того, чтобы R понял, что число именно целое, добавляется буква L\n#Проверим, является ли y целым числом\nis.integer(y)\n\n[1] TRUE\n\n#Проверим, является ли y числовой переменной\nis.numeric(y)\n\n[1] TRUE\nz &lt;- 1:2 + 1i*(8:9)\nclass(z)\n\n[1] \"complex\"\ncolor&lt;-\"Красный\"\nclass(color)\n\n[1] \"character\"\n\nis.character(color)\n\n[1] TRUE\nПри вводе строковых данных возможны разные варианты написания, как с помощью двойных кавычек, так и одинарных. Главное, не использовать те и другие вместе, иначе будет ошибка.\nПример корректного употребления кавычек:\nd &lt;- \"Hello\"         # С помощью двойных кавычек \ne &lt;- 'how are you?'  # С помощью одинарных кавычек\nd\n\n[1] \"Hello\"\n\ne\n\n[1] \"how are you?\"\nОднако, нельзя использовать и те, и другие сразу, будет ошибка:\nf &lt;- \"Так работать не будет'\nИногда вместо текста нет ничего, но это все равно будет строковая переменная (пустая строка):\nh &lt;- \"\"              # Это пустая строка!\nС числовыми переменными мы можем совершать различные операции. Как насчет строковых переменных? К ним тоже можно применять разные функции, и их довольно много. Вот некоторые:\nПосчитаем количество символов в строке:\ng&lt;-\"Как упоительны в России вечера!\"\nnchar(g)\n\n[1] 31\nЕсли нам нужен какой-то определенный фрагмент текста, мы можем его «вытащить» оттуда:\ng&lt;-\"Как упоительны в России вечера!\"\nsubstr(g, 4, 25) \n\n[1] \" упоительны в России в\"\nДовольно часто при анализе текстовых данных их нужно разделить на отдельные кусочки. В этом может помочь функция strplit(). В качестве аргументов этой функции указываются: переменная, содержащая текст, и символ, используемый для разделения, например, знак «пробела»:\ng&lt;-\"Как упоительны в России вечера!\"\nstrsplit(g, \" \")   \n\n[[1]]\n[1] \"Как\"        \"упоительны\" \"в\"          \"России\"     \"вечера!\"\nx &lt;- c(\"single\",\"married\",\"married\",\"single\")\nclass(x)\n\n[1] \"character\"\nx &lt;- factor(levels = c(\"Согласен\", \"И да, и нет\", \"Не согласен\"))\nclass(x)\n\n[1] \"factor\"\nФункция str() дает больше информации о переменной, не только какого она типа, но сколько элементов содержит, какие в ней значения, уровни и т.д.\nstr(x)\n\n Factor w/ 3 levels \"Согласен\",\"И да, и нет\",..:\nas.factor() - данная функция превращает в факторный тип переменные другого типа, даже если они числовые или строковые.\ncolors&lt;-c(\"red\", \"yellow\", \"green\")\ncolors&lt;-as.factor(colors)\nstr(colors)\n\n Factor w/ 3 levels \"green\",\"red\",..: 2 3 1\nИтак, мы познакомились с различными типами данных в R. С ними мы будем сталкиваться постоянно, поэтому нужно уметь не только корректно определять тип переменной, но и понимать, как происходит конверсия типов, каковы требования различных методов. Например, в сравнительном анализе часто используется некоторая группирующая переменная (пол, уровень образования, национальность, профессиональный статус). Эта переменная может быть закодирована числами, но для того, чтобы корректно применять методы статистического анализа, предназначенные для групповых сравнений (например, t-критерий), группирующая переменная должна быть факторой. Перевод в факторную переменную требуется и для создания визуализаций, если требуется группировка по уровням переменной. Эти и другие случаи мы обязательно рассмотрим в следующих разделах.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Типы и структуры данных</span>"
    ]
  },
  {
    "objectID": "Data-types-and-structures.html#типы-данных-в-r",
    "href": "Data-types-and-structures.html#типы-данных-в-r",
    "title": "3  Типы и структуры данных",
    "section": "",
    "text": "Логический (logical) – TRUE, FALSE – ИСТИНА, ЛОЖЬ.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nЧисловой (numeric) тип включает целые и дробные, положительные и отрицательные числа.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nФункция\nОписание\nПример применения в социологических исследованиях\n\n\n\n\nabs(x)\nАбсолютное значение\nРасчет отклонений от медианы в анализе дисперсии мнений\n\n\nmin(...), max(...)\nМинимум и максимум значений\nОпределение диапазона шкал (возраст, доход) в описательной статистике\n\n\nround(x, digits)\nОкругление\nФорматирование коэффициентов корреляции или p-value в отчетах\n\n\nsqrt(x)\nКвадратный корень\nНормализация в индексах уязвимости или стандартизация отклонений\n\n\nlog(x, base)\nЛогарифм\nТрансформация скошенных распределений (доходы, миграционные потоки)\n\n\nexp(x)\nЭкспонента\nОбратная трансформация логарифмированных данных в статистических моделях\n\n\n\n\nПримеры:\n\n\n\nЦелый, целочисленный (integer) тип включает только целые числа, положительные и отрицательные.\n\n\n\n\n\n\n\nКомплексные (complex) числа и соответствующий тип – это уже абстрактная математика (числа состоящие из действительной (реальной) и мнимой части), например, \\(3+2i\\). Нам они вряд ли пригодятся, но на всякий случай запомним.\n\n\n\nТекстовые, строковые (character) данные, как уже понятно из названия, содержат “буквенные” значения, например: «Привет, мир!», «а», «4В»\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nФакторные (factor) переменные – могут быть строковыми и числовыми. Они использования для распределения значений по категориям или группам, которые, в случае порядкового измерения, могут быть разбиты на уровне. Этот тип объединяет номинальные и порядковые переменные, часто также называемые категориальными. Примеры: пол, семейный статус, цвет волос и т.д.\n\n\n\nfactor () - функция, которая создает факторные переменные. Аргумент levels используется для обозначения уровней (порядка, если он нужен).\n\n\n\n\n\n\n\n\n3.1.1 Вопросы для самопроверки\n1. К какому типу переменных относится число -3,5?\n\n numeric character complex integer\n\n2. Какой результат получится, если попытаться превратить в число строковую переменную?\n\n NA TRUE NAs introduced by coercion\n\n3. Какая функция используется для проверки принадлежности переменной к факторному типу?\n\n is.factor() as.factor() factor()\n\n4. Могут ли логические переменные быть представлены в числовом виде?\n\n Да Нет Затрудняюсь ответить",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Типы и структуры данных</span>"
    ]
  },
  {
    "objectID": "Data-types-and-structures.html#структуры-данных",
    "href": "Data-types-and-structures.html#структуры-данных",
    "title": "3  Типы и структуры данных",
    "section": "3.2 Структуры данных",
    "text": "3.2 Структуры данных\n\n\n\n\n\n\nСтруктура данных — это способ организации информации для более эффективного использования. В программировании структурой обычно называют набор данных, связанных определённым образом.\n\n\n\nОсновные структуры данных в R это векторы, матрицы, массивы, списки и таблицы данных.\nРассмотрим их подробнее.\n\n\n3.2.1 Векторы\n\nВектор – одномерный массив проиндексированных (пронумерованных) элементов, набор однотипных элементов (либо числа, либо буквы) без их сочетания. Векторы могут быть такими же, как и типы данных - числовыми, строковыми, логическими и пр.\n\nВот пример того, как можно создать числовой вектор, содержащий числа от 1 до 10:\n\nx&lt;-1:10\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nА вот так - вектор с текстовыми элементами:\n\ny&lt;-c(\"Красный\", \"Синий\", \"Желтый\")\ny\n\n[1] \"Красный\" \"Синий\"   \"Желтый\" \n\n\nЗаметьте, чтобы создать вектор с наименованиями цветов, мы использовали функцию c(), в которой c является производной отcombine. В этом есть смысл, поскольку мы соединяем, как бы “комбинируем”, несколько числовых или текстовых объектов в один ряд.\n\n\n\n\n\n\nПро имена\n\n\n\nКак назвать вектор (да и другие структуры данных) правильно? Есть несколько простых правил. Имя может содержать:\n\nбуквы\nчисла\nточку или символ «подчеркивания” (_)\n\nи обязательно должно начинаться с буквы или точки, но за точкой нельзя поставить цифру.\nТак, например, имя “.2b” не является валидным, а “.b2” - вполне годится для именования.\nНельзя также использовать в именах зарезервированные слова,такие как if, else, repeat, while, function, for, in, next, break, TRUE, FALSE, NULL, Inf, NaN, NA, NA_integer_, NA_real_, NA_complex_, NA_character_ некоторых других, которые являются базовыми конструкциями языка и используются в качестве аргументов при вызове функций.\n\n\nНапример, если мы попробуем создать такой вектор, то программа выдаст ошибку:\n\nbreak&lt;-1:5\n\nError in `break &lt;- 1:5`:\n! неправильная (NULL) левая сторона присваивания\n\n\nДлину числового вектора можно определить с помощью функции length():\n\nx&lt;-1:3\nlength(x)\n\n[1] 3\n\n\nФункция class() помогает определить, какой тип данных хранится в векторе. Напомним, что вектор содержит данные одного типа.\n\nx&lt;-c(1:3)\nclass(x)\n\n[1] \"integer\"\n\n\nЕще примеры создания логических и текстовых векторов:\n\ny &lt;- c(TRUE, TRUE, FALSE, TRUE)\nz &lt;- c(\"to\", \"be\", \"or\", \"not\", \"to\", \"be\") \nclass(y)\n\n[1] \"logical\"\n\nlength(y)\n\n[1] 4\n\nclass(z)\n\n[1] \"character\"\n\nlength(z)\n\n[1] 6\n\n\n\n\n\n\n\n\nЕсли мы попытаемся соединить два вектора - числовой и строковый - то цифры “превратятся” в буквы, поскольку R автоматически переведет все элементы к наиболее подходящему общему типу данных. Поскольку слова в цифры превратить нельзя, то таким общим типом будет строковый.\n\n\n\n\nx&lt;-c(1:10)\ny&lt;-c(\"Красный\", \"Синий\", \"Желтый\")\nz&lt;-c(x,y)\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\ny\n\n[1] \"Красный\" \"Синий\"   \"Желтый\" \n\nz\n\n [1] \"1\"       \"2\"       \"3\"       \"4\"       \"5\"       \"6\"       \"7\"      \n [8] \"8\"       \"9\"       \"10\"      \"Красный\" \"Синий\"   \"Желтый\" \n\n\nТо, что произошло в коде выше, называется конверсией типов. Такая конверсия случается тогда, когда элементы вектора разные. Запомниим, что вектор всегда хранит данные только одного типа!\nА что будет, если мы соединим вместе логические и числовые?\n\nx&lt;-c(TRUE, 5, FALSE, 6)\nx\n\n[1] 1 5 0 6\n\n\nМы можем также создавать «пустые» вектора, обозначая только тип данных и количество элементов.\nНапример:\n\nempty &lt;- numeric(10)   # Создаем пустой числовой вектор с 10 элементами\nprint(empty)\n\n [1] 0 0 0 0 0 0 0 0 0 0\n\n\nЗаметьте, что даже если мы не просили R внести какие-то значения, в числовом векторе всем элементам автоматически были присвоены нули - это значение по умолчанию.\nВот таким образом можно создать пустые векторы других типов:\n\nempty_int &lt;- integer(45)   # числовой вектор с 45 элементами\nempty_cha &lt;- character(2)  # строковый вектор с 2 элементами\nempty_log &lt;- logical(1000)    # логический вектор с 1000 элементами\n\n\n3.2.1.1 Адресация и изменение элементов вектора\nПосле того, как вектор создан, как мы можем посмотреть или изменить его элементы. Это сделать достаточно просто:\nПоменять элемент в векторе:\n\na &lt;- c(1, 2, 3)  # Создадим числовой вектор с 3 элементами\na[2] &lt;- 4        # Изменим значение второго элемента на 4\na                # Посмотрим, что получилось\n\n[1] 1 4 3\n\n\nКак вы уже догадались, чтобы обратиться к какому-то элементу вектора, нужно набрать его порядковый номер в квадратных скобках [ порядковыйномер ]:\n\na &lt;- c(1, 2, 3)  # Создадим числовой вектор с 3 элементами\na[3]             # Попросим R вывести третий элемент\n\n[1] 3\n\n\nКак вы думаете, что будет если запустить следующий код? Каким будет результат?\n\nvec &lt;- c(4, 5, 6)\nvec[3] == 6\n\nДогадались? Попробуйте теперь запустить этот код и проверить, правы вы или нет:\n\n\n3.2.1.2 Операции над векторами\nС векторами можно производить большое количество действий, которые позволяют те типы данных, которые в них содержатся.\nПроще всего совершать арифметические действия:\n\na &lt;- 3   # Создаем единичный числовой объект\na + 4  # Прибавляем к нему константу\n\n[1] 7\n\n\nМы можем прибавить число не только к другому числу, но и ко всему вектору:\n\na &lt;- c(1, 2, 3)   # Создаем числовой вектор\na + 4   # Прибавляем к нему константу\n\n[1] 5 6 7\n\n\nТакой тип “поведения” программы называется поэлементным. То есть операция производится над каждым элементом по отдельности.\nЕще примеры:\n\na &lt;- c(1, 2, 3)\na - 3 \n\n[1] -2 -1  0\n\na * 1.5\n\n[1] 1.5 3.0 4.5\n\na ^ 2\n\n[1] 1 4 9\n\na == 2\n\n[1] FALSE  TRUE FALSE\n\n\nВ последнем случае мы сравнивали каждый элемент со значением два и в результате получили новый вектор, состоящий из логических значений.\nВ R имеется много различных функций для трансформации, мы с ними будем активно работать на следующих занятиях. Но самая простая функциюя - sum(), которая просто складывает все элементы вектора вместе:\n\na &lt;- c(1, 2, 3)\nsum(a)\n\n[1] 6\n\n\nНекоторые операции можно производить над векторами, если у них одинаковая длина:\n\na &lt;- c(1, 2, 3)\nb &lt;- c(1, 0, 1)\na + b\n\n[1] 2 2 4\n\nb * a\n\n[1] 1 0 3\n\na ^ b\n\n[1] 1 1 3\n\n\nМы можем даже сравнить два вектора и сохранить результат в отдельном логическом векторе\n\na &lt;- c(1, 2, 3)\nb &lt;- c(1, 0, 1)\nz &lt;- a &gt; b   # сравниваем a и b, поэлементно, результат сохраняем в векторе z\nz\n\n[1] FALSE  TRUE  TRUE\n\nz == TRUE\n\n[1] FALSE  TRUE  TRUE\n\nz == FALSE\n\n[1]  TRUE FALSE FALSE\n\n!z #выдает противоположное значение\n\n[1]  TRUE FALSE FALSE\n\n\nА что можно делать со строковыми векторами?\nДавайте создадим пару векторов и попробуем с ними совершить какие-нибудь действия, например, сравним их попарно:\n\na &lt;- c(\"собака\", \"молоко\", \"квартира\", \"чай\", \"морозы\", \"душа\")\nb &lt;- c(\"собака\", \"мыло\", \"сахар\", \"чай\", \"малина\", \"душа\")\na==b\n\n[1]  TRUE FALSE FALSE  TRUE FALSE  TRUE\n\n\nЧтобы найти какой-то текстовый элемент и его место в векторе, можно использовать функцию grep():\n\na &lt;- c(\"собака\", \"молоко\", \"квартира\", \"чай\", \"морозы\", \"квартира\", \"душа\")\ngrep(\"квартира\", a)\n\n[1] 3 6\n\n\nРезультат показывает, что слово “квартира” встречается два раза, это третий и шестой элементы. Что будет, если поищем то, чего нет?\n\na &lt;- c(\"собака\", \"молоко\", \"квартира\", \"чай\", \"морозы\", \"квартира\", \"душа\")\ngrep(\"редиска\", a)\n\ninteger(0)\n\n\nЕсли векторы разного типа, то иногда с ними можно что-то сделать, иногда нет, зависит от типа данных:\n\na &lt;- c(1, 2, 3)\nb &lt;- c(\"one\", \"two\", \"three\")\nc &lt;- c(TRUE, TRUE, FALSE)\na+b\n\nНичего не получилось, потому что нельзя к словам прибавлять числа. А вот если сложить вектора a и c, все получится:\n\na &lt;- c(1, 2, 3)\nb &lt;- c(\"one\", \"two\", \"three\")\nc &lt;- c(TRUE, TRUE, FALSE)\na+c\n\n[1] 2 3 3\n\n\n\n\n3.2.1.3 Некторые полезные функции для работы с векторами\n\nСоздать вектор случайных чисел в определенном диапазоне:\n\n\nx &lt;- runif(10, min = -5, max = 5)\nx\n\n [1]  4.4437208 -4.1474411  2.3955110 -1.8172539 -3.9805016  2.9035523\n [7]  0.7691572  3.2197267  2.8939580  0.3703292\n\n\n\nСоздать вектор с повторяющимися значениями с помощью функции rep() – повторить (что, сколько раз):\n\n\nb&lt;- rep(c(1,2,3),4)\nb\n\n [1] 1 2 3 1 2 3 1 2 3 1 2 3\n\n\n\nСоздать вектор из повторяющихся значений определенной длины с помощью функции rep() – повторить (что, до каких пор, какой длины):\n\n\nc&lt;- rep(c(4,5,6), length.out=10)\nc\n\n [1] 4 5 6 4 5 6 4 5 6 4\n\n\n\nСоздать вектор через последовательность определенных значений с помощью функции seq() – задать последовательность (от, до, с каким шагом)\n\n\nseq &lt;- seq(from=2,to=15,by=0.5)\nseq\n\n [1]  2.0  2.5  3.0  3.5  4.0  4.5  5.0  5.5  6.0  6.5  7.0  7.5  8.0  8.5  9.0\n[16]  9.5 10.0 10.5 11.0 11.5 12.0 12.5 13.0 13.5 14.0 14.5 15.0\n\n\n\nПроверить, является ли какой-то объект вектором с помощью функции is.vector() :\n\n\nseq &lt;- seq(from=2,to=15,by=0.5)\nis.vector(seq)\n\n[1] TRUE\n\n\n\nПроверить, удовлетворяет ли хотя бы один элемент вектора какому-либо условию с помощью функции any():\n\n\nvec &lt;- as.integer(c(34,23,53,42,16,42,64,32,76))\nany(vec,vec&gt;50)\n\n[1] TRUE\n\n\n\nПроверить, удовлетворяют ли все элементы вектора какому-либо условию с помощью функции all():\n\n\nvec &lt;- as.integer(c(34,23,53,42,16,42,64,32,76))\nall(vec,vec&gt;10)\n\n[1] TRUE\n\n\n\nПрименить какую-либо функцию преобразования ко всем элементам вектора с помощью функции sapply():\n\n\nvec &lt;- as.integer(c(34,23,53,42,16,42,64,32,76))\nvec3&lt;-sapply(vec, sqrt)\nvec3\n\n[1] 5.830952 4.795832 7.280110 6.480741 4.000000 6.480741 8.000000 5.656854\n[9] 8.717798\n\n\nВ принципе, можно это сделать и проще:\n\nvec &lt;- as.integer(c(34,23,53,42,16,42,64,32,76))\nsqrt(vec)\n\n[1] 5.830952 4.795832 7.280110 6.480741 4.000000 6.480741 8.000000 5.656854\n[9] 8.717798\n\n\nНо иногда, когда у нас не вектор, а более сложный объект, функция sapply() и ее родные сестры apply() и lapply()часто очень выручают. Подробнее эти функции будут рассмотрены на следующих занятиях.\n\n\n\n3.2.2 Матрицы\n\nМатрица – двумерная совокупность числовых, логических или текстовых величин. Чтобы создать матрицу, нужно воспользоваться функцией matrix(). Можно думать о матрице как о решетке каких-то числовых величин.\n\nПростой пример:\n\ndata &lt;- c(1, 2, 3, 4, 5, 6, 7, 8, 9)\nA &lt;- matrix(data, ncol=3, nrow=3)\nA\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n\nМы создали матрицу с тремя строками и тремя колонками на основе данных, изначально хранящихся в виде числового вектора.\n\n\n\n\n\n\nR заполняет матрицу сверху вниз, колонку за колонкой, двигаясь справа налево.\n\n\n\nЕсли мы хотим это изменить, нужно поменять параметр byrow:\n\ndata &lt;- c(1, 2, 3, 4, 5, 6, 7, 8, 9)\nA &lt;- matrix(data, ncol=3, nrow=3, byrow = T)\nA\n\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    4    5    6\n[3,]    7    8    9\n\n\n\n3.2.2.1 Полезные функции и операции над матрицами\n\nКак обратиться к одному из элементов матрицы? В случае с вектором мы задавали один элемент в квадратных скобках, а что теперь? Матрица - двумерный объект, поэтому мы должны теперь в квадратных скобках задать две координаты - номер строки (первое число) и номер колонки (второе число) - через запятую:\n\n\ndata &lt;- c(1, 2, 3, 4, 5, 6, 7, 8, 9)\nA &lt;- matrix(data, ncol=3, nrow=3, byrow = T)\nA\n\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    4    5    6\n[3,]    7    8    9\n\nA[1,1] # Первый элемент первой строки\n\n[1] 1\n\nA[2,3] # Третий элемент второй строки\n\n[1] 6\n\nA[1,]  # Первая строка полностью\n\n[1] 1 2 3\n\nA[,3]  # Третий столбец полностью\n\n[1] 3 6 9\n\n\nЧтобы получить диагональные элементы, есть специальная функция - diag()\n\ndata &lt;- c(1, 2, 3, 4, 5, 6, 7, 8, 9)\nA &lt;- matrix(data, ncol=3, nrow=3, byrow = T)\ndiag(A)\n\n[1] 1 5 9\n\n\nЧтобы узнать размерность матрицы (количество строк и столбцов) - можно использовать функцию dim():\n\ndata &lt;- c(1, 2, 3, 4, 5, 6, 7, 8, 9)\nA &lt;- matrix(data, ncol=3, nrow=3, byrow = T)\ndim(A)\n\n[1] 3 3\n\n\nВ результате применения функции мы получаем числовой вектор, первое значение в котором обозначает количество строк, второе - столбцов.\nС матрицами, как и с векторами, можно совершать разные математические операции - сложение, умножение, вычитание и т.д.:\n\nНапример, прибавить единицу к каждому элементу матрицы:\n\n\ndata &lt;- c(1, 2, 3, 4, 5, 6, 7, 8, 9)\nA &lt;- matrix(data, ncol=3, nrow=3, byrow = T)\nA\n\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    4    5    6\n[3,]    7    8    9\n\nA + 1   \n\n     [,1] [,2] [,3]\n[1,]    2    3    4\n[2,]    5    6    7\n[3,]    8    9   10\n\n\n\nУмножить каждый элемент на 2:\n\n\ndata &lt;- c(1, 2, 3, 4, 5, 6, 7, 8, 9)\nA &lt;- matrix(data, ncol=3, nrow=3, byrow = T)\nA\n\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    4    5    6\n[3,]    7    8    9\n\nA * 2\n\n     [,1] [,2] [,3]\n[1,]    2    4    6\n[2,]    8   10   12\n[3,]   14   16   18\n\n\n\nВозвести в квадрат:\n\n\ndata &lt;- c(1, 2, 3, 4, 5, 6, 7, 8, 9)\nA &lt;- matrix(data, ncol=3, nrow=3, byrow = T)\nA\n\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    4    5    6\n[3,]    7    8    9\n\nA^2\n\n     [,1] [,2] [,3]\n[1,]    1    4    9\n[2,]   16   25   36\n[3,]   49   64   81\n\n\n\nДовольно часто возникает необходимость транспонирования, когда строки становятся столбцами и наоборот:\n\n\ndata &lt;- c(1, 2, 3, 4, 5, 6, 7, 8, 9)\nA &lt;- matrix(data, ncol=3, nrow=3, byrow = T)\nA\n\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    4    5    6\n[3,]    7    8    9\n\nt(A)\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n\n\nНайдти след матрицы\n\n\n\n\n\n\n\n*След матрицы** - это сумма диагональных элементов.\n\n\n\n\ndata &lt;- c(1, 2, 3, 4, 5, 6, 7, 8, 9)\nA &lt;- matrix(data, ncol=3, nrow=3, byrow = T)\nsum(diag(A))\n\n[1] 15\n\n\n\nСложить две матрицы\n\n\ndata &lt;- c(1, 2, 3, 4, 5, 6, 7, 8, 9)\nA &lt;- matrix(data, ncol=3, nrow=3, byrow = T)\nB &lt;- matrix(1, 3, 3)\nA + B\n\n     [,1] [,2] [,3]\n[1,]    2    3    4\n[2,]    5    6    7\n[3,]    8    9   10\n\n\nЗаметьте, как мы это сделали: мы создали матрицу В одной строкой (матрица 3*3, состоящая из одних единиц).\n\nПеремножить элементы двух матриц:\n\n\ndata &lt;- c(1, 2, 3, 4, 5, 6, 7, 8, 9)\nA &lt;- matrix(data, ncol=3, nrow=3, byrow = T)\nB &lt;- matrix(2, 3, 3)\nA * B\n\n     [,1] [,2] [,3]\n[1,]    2    4    6\n[2,]    8   10   12\n[3,]   14   16   18\n\n\n\nОсуществить матричное умножение:\n\n\ndata &lt;- c(1, 2, 3, 4, 5, 6, 7, 8, 9)\nA &lt;- matrix(data, ncol=3, nrow=3, byrow = T)\nB &lt;- matrix(2, 3, 3)\nA %*%  B\n\n     [,1] [,2] [,3]\n[1,]   12   12   12\n[2,]   30   30   30\n[3,]   48   48   48\n\n\nВ чем разница между двумя умножениями? В первом случае элементы поэлементно умножаются друг на друга, во втором - происходит умножение матриц по законам линейной алгебры.\n\n\n\n3.2.3 Массивы\n\nМассив (array) – объект в R, в котором хранится данные, содержащие более двух измерений. Например, если мы создадим массив с параметрами (2, 3, 4), у нас получится 4 прямоугольные матрицы размером 2 на 3. В массивах хранятся данные только одного типа.\n\nДля того, чтобы создать массив, есть специальная функция array(). Она принимает на входе вектора и использует значения параметра dim, чтобы создать массив.\nПример:\n\nvector1 &lt;- c(5,9,3)\nvector2 &lt;- c(10,11,12,13,14,15)\narr1 &lt;- array(c(vector1,vector2),dim = c(3,3,2))\narr1\n\n, , 1\n\n     [,1] [,2] [,3]\n[1,]    5   10   13\n[2,]    9   11   14\n[3,]    3   12   15\n\n, , 2\n\n     [,1] [,2] [,3]\n[1,]    5   10   13\n[2,]    9   11   14\n[3,]    3   12   15\n\n\nМы можем присвоить имена строкам, колонкам и матрицам массива с помощью параметра dimnames:\n\nvector1 &lt;- c(5,9,3)\nvector2 &lt;- c(10,11,12,13,14,15)\ncolumn.names &lt;- c(\"COL1\",\"COL2\",\"COL3\")\nrow.names &lt;- c(\"ROW1\",\"ROW2\",\"ROW3\")\nmatrix.names &lt;- c(\"Matrix1\",\"Matrix2\")\nresult &lt;- array(c(vector1,vector2),dim = c(3,3,2),dimnames = list(row.names,column.names,\n   matrix.names))\nresult\n\n, , Matrix1\n\n     COL1 COL2 COL3\nROW1    5   10   13\nROW2    9   11   14\nROW3    3   12   15\n\n, , Matrix2\n\n     COL1 COL2 COL3\nROW1    5   10   13\nROW2    9   11   14\nROW3    3   12   15\n\n\n\n3.2.3.1 Операции с элементами массива\nТак как массив - это по сути совокупность нескольких матриц, то и операции мы с массивами мы можем производить практически такие же.\n\nОбращение к элементам\n\nОбращение к элементам массива практически такое же, как и в случае с матрицами, чуть сложнее, поскольку добавляются новые измерения, но общая логика остается та же - мы в квадратных скобках указываем координаты элемента, которые нам нужны:\n\nvector1 &lt;- c(5,9,3)\nvector2 &lt;- c(10,11,12,13,14,15)\ncolumn.names &lt;- c(\"COL1\",\"COL2\",\"COL3\")\nrow.names &lt;- c(\"ROW1\",\"ROW2\",\"ROW3\")\nmatrix.names &lt;- c(\"Matrix1\",\"Matrix2\")\nresult &lt;- array(c(vector1,vector2),dim = c(3,3,2),dimnames = list(row.names,column.names,\n   matrix.names))\nresult[3,,2] # Третья строка второй матрицы\n\nCOL1 COL2 COL3 \n   3   12   15 \n\nresult[1,3,1] # Третий элемент первой строки первой матрицы\n\n[1] 13\n\nresult[,,2] #Вся вторая матрица\n\n     COL1 COL2 COL3\nROW1    5   10   13\nROW2    9   11   14\nROW3    3   12   15\n\n\n\nСоздание матрицы на основе массива:\n\n\nvector1 &lt;- c(5,9,3)\nvector2 &lt;- c(10,11,12,13,14,15)\ncolumn.names &lt;- c(\"COL1\",\"COL2\",\"COL3\")\nrow.names &lt;- c(\"ROW1\",\"ROW2\",\"ROW3\")\nmatrix.names &lt;- c(\"Matrix1\",\"Matrix2\")\narray1 &lt;- array(c(vector1,vector2),dim = c(3,3,2),dimnames = list(row.names,column.names,\n   matrix.names))\nmatrix1 &lt;- array1[,,1]\nmatrix2 &lt;- array1[,,2]\nmatrix1\n\n     COL1 COL2 COL3\nROW1    5   10   13\nROW2    9   11   14\nROW3    3   12   15\n\nmatrix2\n\n     COL1 COL2 COL3\nROW1    5   10   13\nROW2    9   11   14\nROW3    3   12   15\n\n\n\nАрифметические действия с элементами:\n\n\nvector1 &lt;- c(5,9,3)\nvector2 &lt;- c(10,11,12,13,14,15)\ncolumn.names &lt;- c(\"COL1\",\"COL2\",\"COL3\")\nrow.names &lt;- c(\"ROW1\",\"ROW2\",\"ROW3\")\nmatrix.names &lt;- c(\"Matrix1\",\"Matrix2\")\narray1 &lt;- array(c(vector1,vector2),dim = c(3,3,2),dimnames = list(row.names,column.names,\n   matrix.names))\narray1[1,2,1]+array1[3,2,2]\n\n[1] 22\n\n\n\n\n\n3.2.4 Списки (lists)\nСписок(list) - сложный объект, в котором могут храниться данные разных типов и структуры, включая вектора, матрицы и т.д.\nПример:\n\nA &lt;- list(\"Red\", \"Green\", c(21,32,11), matrix(c(1:9), nrow=3, ncol=3), TRUE, 51.23, 119.1)\nA\n\n[[1]]\n[1] \"Red\"\n\n[[2]]\n[1] \"Green\"\n\n[[3]]\n[1] 21 32 11\n\n[[4]]\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n[[5]]\n[1] TRUE\n\n[[6]]\n[1] 51.23\n\n[[7]]\n[1] 119.1\n\n\nСписки могут даже содержать внутри другие списки!\n\nA &lt;- list(\"Red\", \"Green\", c(21,32,11), matrix(c(1:9), nrow=3, ncol=3), TRUE, 51.23, 119.1)\nA[[8]]&lt;-list(\"One\", 256, TRUE)\nA\n\n[[1]]\n[1] \"Red\"\n\n[[2]]\n[1] \"Green\"\n\n[[3]]\n[1] 21 32 11\n\n[[4]]\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n[[5]]\n[1] TRUE\n\n[[6]]\n[1] 51.23\n\n[[7]]\n[1] 119.1\n\n[[8]]\n[[8]][[1]]\n[1] \"One\"\n\n[[8]][[2]]\n[1] 256\n\n[[8]][[3]]\n[1] TRUE\n\n\nКомпоненты списка также могут иметь имена:\n\nA &lt;- list(\"Red\", \"Green\", c(21,32,11), matrix(c(1:9), nrow=3, ncol=3), TRUE, 51.23, 119.1)\nA[[\"color\"]] &lt;- \"yellow\"\nA\n\n[[1]]\n[1] \"Red\"\n\n[[2]]\n[1] \"Green\"\n\n[[3]]\n[1] 21 32 11\n\n[[4]]\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n[[5]]\n[1] TRUE\n\n[[6]]\n[1] 51.23\n\n[[7]]\n[1] 119.1\n\n$color\n[1] \"yellow\"\n\n\nТаким образом, обратиться к компоненту списка можно по имени или по порядковому номеру элемента:\n\nA &lt;- list(\"Red\", \"Green\", c(21,32,11), matrix(c(1:9), nrow=3, ncol=3), TRUE, 51.23, 119.1)\nA[[\"color\"]] &lt;- \"yellow\"\nA[[\"color\"]]\n\n[1] \"yellow\"\n\nA$color #Альтернативный вариант обращения по имени\n\n[1] \"yellow\"\n\nA[[8]]\n\n[1] \"yellow\"\n\n\n\n\n3.2.5 Таблицы данных (data frame)\nДля аналитиков данных это самый важный объект, с которым приходится работать чаще всего. Большинство из тех данных, с которыми нам приходится иметь дело, хранятся именно в формате датафрейма.\nТаблица данных (data frame) - может включать данные разного типа, но только по столбцам. Иными словами, в таблице может быть несколько типов данных, но в каждом столбце может быть только один тип.\nПример:\n\ndf &lt;- data.frame(\n  age        = c(25, 34, 42, 29),\n  gender     = c(\"мужчина\", \"женщина\", \"мужчина\", \"женщина\"),\n  education  = c(\"высшее\", \"среднее\", \"высшее\", \"среднее специальное\"),\n  prof_status = c(\"работающий\", \"занятый на производстве\", \"пенсионер\", \"работающий\")\n)\ndf\n\n  age  gender           education             prof_status\n1  25 мужчина              высшее              работающий\n2  34 женщина             среднее занятый на производстве\n3  42 мужчина              высшее               пенсионер\n4  29 женщина среднее специальное              работающий\n\n\nОбращаться к элементам таблицы данных можно точно так же, как и к элементам матрицы:\n\ndf[1,4]\n\n[1] \"работающий\"\n\n\nЧтобы добавить новую переменную, нужно использовать знак $:\n\ndf$region &lt;- c(\"Алтайский край\", \"Кемеровская область\", \"Новосибирская область\", \"Красноярский край\")\ndf\n\n  age  gender           education             prof_status                region\n1  25 мужчина              высшее              работающий        Алтайский край\n2  34 женщина             среднее занятый на производстве   Кемеровская область\n3  42 мужчина              высшее               пенсионер Новосибирская область\n4  29 женщина среднее специальное              работающий     Красноярский край\n\n\nС помощью этого же знака, можно вывести все значения той или иной переменной:\n\ndf$education\n\n[1] \"высшее\"              \"среднее\"             \"высшее\"             \n[4] \"среднее специальное\"\n\n\nЧтобы узнать размерность таблицы, можно воспользоваться уже известной нам функции dim():\n\ndim(df)\n\n[1] 4 5\n\n\nЕсли у нас есть список векторов, мы можем их легко трансформировать в датафрейм:\n\npeople &lt;- list(name=c(\"Анна Власова\", \"Петр Гордеев\", \"Андрей Павлов\"), \n               grade=c(99.4, 87.6, 22.1), \n               sex=c(\"женский\", \"мужской\", \"мужской\"))\nas.data.frame(people)\n\n           name grade     sex\n1  Анна Власова  99.4 женский\n2  Петр Гордеев  87.6 мужской\n3 Андрей Павлов  22.1 мужской\n\n\nВ R содержатся много загруженных наборов данных, таких например, как swiss`, в котором содержатся данные рождаемости, образовании, доли католического населения, младенческой смертности в кантонах Швейцарии:\n\nhead(swiss)\n\n             Fertility Agriculture Examination Education Catholic\nCourtelary        80.2        17.0          15        12     9.96\nDelemont          83.1        45.1           6         9    84.84\nFranches-Mnt      92.5        39.7           5         5    93.40\nMoutier           85.8        36.5          12         7    33.77\nNeuveville        76.9        43.5          17        15     5.16\nPorrentruy        76.1        35.3           9         7    90.57\n             Infant.Mortality\nCourtelary               22.2\nDelemont                 22.2\nFranches-Mnt             20.2\nMoutier                  20.3\nNeuveville               20.6\nPorrentruy               26.6\n\n\n\nswiss\n\nСамые простые операции с таблицами данных заключаются в выводе данных, наименований строк и столбцов.\nКак вывести первые несколько наблюдений?\n\nhead(swiss)\n\n             Fertility Agriculture Examination Education Catholic\nCourtelary        80.2        17.0          15        12     9.96\nDelemont          83.1        45.1           6         9    84.84\nFranches-Mnt      92.5        39.7           5         5    93.40\nMoutier           85.8        36.5          12         7    33.77\nNeuveville        76.9        43.5          17        15     5.16\nPorrentruy        76.1        35.3           9         7    90.57\n             Infant.Mortality\nCourtelary               22.2\nDelemont                 22.2\nFranches-Mnt             20.2\nMoutier                  20.3\nNeuveville               20.6\nPorrentruy               26.6\n\n\nКак вывести последние несколько наблюдений?\n\ntail(swiss)\n\n             Fertility Agriculture Examination Education Catholic\nNeuchatel         64.4        17.6          35        32    16.92\nVal de Ruz        77.6        37.6          15         7     4.97\nValdeTravers      67.6        18.7          25         7     8.65\nV. De Geneve      35.0         1.2          37        53    42.34\nRive Droite       44.7        46.6          16        29    50.43\nRive Gauche       42.8        27.7          22        29    58.33\n             Infant.Mortality\nNeuchatel                23.0\nVal de Ruz               20.0\nValdeTravers             19.5\nV. De Geneve             18.0\nRive Droite              18.2\nRive Gauche              19.3\n\n\nКакие переменные содержатся в наборе mtcars?\n\nnames(mtcars)\n\n [1] \"mpg\"  \"cyl\"  \"disp\" \"hp\"   \"drat\" \"wt\"   \"qsec\" \"vs\"   \"am\"   \"gear\"\n[11] \"carb\"\n\n\nНу, и напоследок, представим сводку о данных, содержащихся в этом наборе:\n\nsummary(swiss)\n\n   Fertility      Agriculture     Examination      Education    \n Min.   :35.00   Min.   : 1.20   Min.   : 3.00   Min.   : 1.00  \n 1st Qu.:64.70   1st Qu.:35.90   1st Qu.:12.00   1st Qu.: 6.00  \n Median :70.40   Median :54.10   Median :16.00   Median : 8.00  \n Mean   :70.14   Mean   :50.66   Mean   :16.49   Mean   :10.98  \n 3rd Qu.:78.45   3rd Qu.:67.65   3rd Qu.:22.00   3rd Qu.:12.00  \n Max.   :92.50   Max.   :89.70   Max.   :37.00   Max.   :53.00  \n    Catholic       Infant.Mortality\n Min.   :  2.150   Min.   :10.80   \n 1st Qu.:  5.195   1st Qu.:18.15   \n Median : 15.140   Median :20.00   \n Mean   : 41.144   Mean   :19.94   \n 3rd Qu.: 93.125   3rd Qu.:21.70   \n Max.   :100.000   Max.   :26.60",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Типы и структуры данных</span>"
    ]
  },
  {
    "objectID": "Data-types-and-structures.html#таблицы-данных-data-frame",
    "href": "Data-types-and-structures.html#таблицы-данных-data-frame",
    "title": "3  Типы и структуры данных",
    "section": "3.3 Таблицы данных (data frame)",
    "text": "3.3 Таблицы данных (data frame)\nДля аналитиков данных это самый важный объект, с которым приходится работать чаще всего. Большинство из тех данных, с которыми нам приходится иметь дело, хранятся именно в формате датафрейма.\nТаблица данных (data frame) - может включать данные разного типа, но только по столбцам. Иными словами, в таблице может быть несколько типов данных, но в каждом столбце может быть только один тип.\n\ndf&lt;- data.frame(id = c(1:4), city = c(\"Москва\",\"Лондон\",\"Париж\",\"Нью-Йорк\"), population = c(16555000, 10840000,10960000,21045000))\ndf\n\n  id     city population\n1  1   Москва   16555000\n2  2   Лондон   10840000\n3  3    Париж   10960000\n4  4 Нью-Йорк   21045000\n\n\nОбращаться к элементам таблицы данных можно точно так же, как и к элементам матрицы:\n\ndf&lt;- data.frame(id = c(1:4), city = c(\"Москва\",\"Лондон\",\"Париж\",\"Нью-Йорк\"), population = c(16555000, 10840000,10960000,21045000))\ndf[1,2]\n\n[1] \"Москва\"\n\n\nЧтобы добавить новую переменную, нужно использовать знак $:\n\ndf&lt;- data.frame(id = c(1:4), city = c(\"Москва\",\"Лондон\",\"Париж\",\"Нью-Йорк\"), population = c(16555000, 10840000,10960000,21045000))\ndf$country&lt;-c(\"Россия\", \"Великобритания\", \"Франция\", \"США\")\ndf$pop2&lt;-df$population/1000\ndf\n\n  id     city population        country  pop2\n1  1   Москва   16555000         Россия 16555\n2  2   Лондон   10840000 Великобритания 10840\n3  3    Париж   10960000        Франция 10960\n4  4 Нью-Йорк   21045000            США 21045\n\n\nС помощью этого же знака, можно вывести все значения той или иной переменной:\nЧтобы добавить новую переменную, нужно использовать знак $:\n\ndf&lt;- data.frame(id = c(1:4), city = c(\"Москва\",\"Лондон\",\"Париж\",\"Нью-Йорк\"), population = c(16555000, 10840000,10960000,21045000))\ndf$country&lt;-c(\"Россия\", \"Великобритания\", \"Франция\", \"США\")\ndf$pop2&lt;-df$population/1000\ndf$city\n\n[1] \"Москва\"   \"Лондон\"   \"Париж\"    \"Нью-Йорк\"\n\n\nЧтобы узнать размерность таблицы, можно воспользоваться уже известной нам функции dim():\n\ndf&lt;- data.frame(id = c(1:4), city = c(\"Москва\",\"Лондон\",\"Париж\",\"Нью-Йорк\"), population = c(16555000, 10840000,10960000,21045000))\ndf$country&lt;-c(\"Россия\", \"Великобритания\", \"Франция\", \"США\")\ndf$pop2&lt;-df$population/1000\ndim(df)\n\n[1] 4 5\n\n\nЕсли у нас есть список векторов, мы можем их легко трансформировать в датафрейм:\n\npeople &lt;- list(name=c(\"Alice\", \"Bob\", \"Charlie\"), \n               grade=c(99.4, 87.6, 22.1), \n               sex=c(\"F\", \"M\", \"M\"))\nas.data.frame(people)\n\n     name grade sex\n1   Alice  99.4   F\n2     Bob  87.6   M\n3 Charlie  22.1   M\n\n\nВ R содержатся много загруженных наборов данных, таких например, как mtcars, в котором содержатся данные по 32 маркам автомобилей:\n\nmtcars\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n\n\nСамые простые операции с таблицами данных заключаются в выводе данных, наименований строк и столбцов.\nКак вывести первые несколько наблюдений?\n\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\nКак вывести последние несколько наблюдений?\n\ntail(mtcars)\n\n                mpg cyl  disp  hp drat    wt qsec vs am gear carb\nPorsche 914-2  26.0   4 120.3  91 4.43 2.140 16.7  0  1    5    2\nLotus Europa   30.4   4  95.1 113 3.77 1.513 16.9  1  1    5    2\nFord Pantera L 15.8   8 351.0 264 4.22 3.170 14.5  0  1    5    4\nFerrari Dino   19.7   6 145.0 175 3.62 2.770 15.5  0  1    5    6\nMaserati Bora  15.0   8 301.0 335 3.54 3.570 14.6  0  1    5    8\nVolvo 142E     21.4   4 121.0 109 4.11 2.780 18.6  1  1    4    2\n\n\nКакие переменные содержатся в наборе mtcars?\n\nnames(mtcars)\n\n [1] \"mpg\"  \"cyl\"  \"disp\" \"hp\"   \"drat\" \"wt\"   \"qsec\" \"vs\"   \"am\"   \"gear\"\n[11] \"carb\"\n\n\nНу, и напоследок, представим сводку о данных, содержащихся в этом наборе:\n\nsummary(mtcars)\n\n      mpg             cyl             disp             hp       \n Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n 1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \n Median :19.20   Median :6.000   Median :196.3   Median :123.0  \n Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n 3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \n Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n      drat             wt             qsec             vs        \n Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n 1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \n Median :3.695   Median :3.325   Median :17.71   Median :0.0000  \n Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n 3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \n Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n       am              gear            carb      \n Min.   :0.0000   Min.   :3.000   Min.   :1.000  \n 1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000  \n Median :0.0000   Median :4.000   Median :2.000  \n Mean   :0.4062   Mean   :3.688   Mean   :2.812  \n 3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000  \n Max.   :1.0000   Max.   :5.000   Max.   :8.000",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Типы и структуры данных</span>"
    ]
  },
  {
    "objectID": "Data-types-and-structures.html#самостоятельная-работа-1",
    "href": "Data-types-and-structures.html#самостоятельная-работа-1",
    "title": "3  Типы и структуры данных",
    "section": "3.4 Самостоятельная работа №1",
    "text": "3.4 Самостоятельная работа №1\n\nНапоминаем, что для выполнения интерактивных самостоятельных работ, необходимо установить пакет r4ssr.\n\nЕсли Вы еще не выполняли Самостоятельную работу 0, то предварительно установите пакет r4ssr:\n\ndevtools::install_github(\"domelia/r4ssr\")\n\nДля выполнения интерактивной самостоятельной работы №1 скопируйте и запустите следующий код:\n\nlearnr::run_tutorial(\"Assignment1\", package=\"r4ssr\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Типы и структуры данных</span>"
    ]
  },
  {
    "objectID": "Data-types-and-structures.html#самостоятельная-работа",
    "href": "Data-types-and-structures.html#самостоятельная-работа",
    "title": "3  Типы и структуры данных",
    "section": "3.3 Самостоятельная работа",
    "text": "3.3 Самостоятельная работа\n\nНапоминаем, что для выполнения интерактивных самостоятельных работ, необходимо установить пакет r4ssr.\n\n\ndevtools::install_github(\"domelia/r4ssr\", dependencies=TRUE)\n\nДля выполнения интерактивной самостоятельной работы по данному разделу скопируйте и запустите следующий код:\n\nlearnr::run_tutorial(\"Assignment1\", package=\"r4ssr\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Типы и структуры данных</span>"
    ]
  }
]