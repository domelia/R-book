{"title":"Регрессионный анализ в R","markdown":{"yaml":{"title":"Регрессионный анализ в R"},"headingText":"Основы линейной регрессии","containsRefs":false,"markdown":"\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE, message = F, warning = F)\n```\n\n\nНа предыдущих занятиях мы изучили основные типы и структуры данных, овладели основами создания простых функций на R, научились импортировать и экспортировать данные в разных форматах. Мы уже умеем проводить различные трансформации данных, визуализировать данные разного типа и проводить одномерный и двумерный анализ на категориальных данных. Мы даже справились со сложной задачей анализа данных с помощью метода главных компонент и анализа соответствий.\n\nМы продолжаем изучать многомерные методы, и следующий на очереди - регрессионный анализ и его отдельные разновидности.\n\nВ обобщенном виде уравнение регрессионного анализа может быть представлено как:\n\n$$Y=X\\beta+\\epsilon,$$\n\nгде $Y$ - вектор значений зависимой переменной, $X$ - матрица значений независимых переменных (предикторов), $\\beta$ - вектор коэффициентов регрессии, используемых для подгонки к известным значениям зависимой переменной, $\\epsilon$ - ошибки модели (остатки, разница между реальными и предсказанными значениями).\n\nПреимуществами и причинами популярности регрессионного анализа являются следующие:\n\n-   регрессионные модели могут включать множество предикторов одновременно, что позволяет оценивать вклад каждого при условии контроля над остальными параметрами;\n-   существует большое количество разновидностей регрессионного анализа, могут использоваться различные типы предикторов и зависимых переменных;\n-   довольно легко интерпретировать результаты;\n-   достаточно просты в применении и не слишком сложны с математической точки зрения.\n\nЧаще всего используются следующие виды регрессионного анализа:\n\n-   линейная регрессия (зависимая переменная числовая)\n-   логистическая регрессия (зависимая переменная бинарная)\n-   порядковая регрессия (зависимая переменная - упорядоченная факторная)\n-   мультиномиальная регрессия (зависимая переменная категориальная)\n\n### Загрузка данных\n\nВ качестве практического примера мы будем использовать учебный набор данных о взаимосвязи между объемами продаж и затратами на рекламу. Это небольшой набор, который поможет понять основные идеи метода и его различные реализации.\n\nКак всегда, мы начинаем с загрузки данных.\n\n{{< iconify arcticons 1dm size=42px >}}[Скачать данные](https://github.com/domelia/rcourse/blob/main/Advertising.csv)\n\n```{r}\nlibrary(readr)\nAdvertising = read_csv(\"Advertising.csv\")\n```\n\nПосле загрузки данных в R, первым делом нужно посмотреть сами данные, их структуру. Поскольку мы использовали функцию `read_csv()`, данные были загружены в формате tibble (tbl_df). Это практически датафрейм, вернее его усовершенствованная версия, в которой данные обрабатываются быстрее, и не происходит некоторых неприятных трансформаций (например, не меняются типы и имена данных).\n\nМы видим, что у нас всего 200 наблюдений и 4 переменных типа `double` (числовой формат, в котором происходит более точное округление десятичных знаков до 16 знаков после запятой - с 64-битной, то есть двойной точностью).\n\nВ нашем наборе переменная **Sales** (Продажи) будет являться зависимой переменной, и мы будем пытаться выявить взаимосвязи между продажами и тремя другими - независимыми переменными: TV, Radio, и Newspaper, обозначающими, соответственно, затраты на рекламу на телевидении, радио и в газетах.\n\n### Предварительная визуализация данных\n\nПосле рассмотрения структуры данных, следующий шаг - это визуализация. Поскольку у нас только количественные (не категориальные) переменные, лучший способ их представить - это диаграммы рассеяния, которые мы можем сделать для каждого индивидуального предиктора.\n\nНапример, для рекламы на телевидении\n\n```{r}\nplot(Sales ~ TV, data = Advertising, col = \"dodgerblue\", pch = 20, cex = 1.5,\n     main = \"Продажи vs Реклама на телевидении\")\n```\n\n**Самостоятельная работа**: сделайте аналогичные графики для других переменных.\n\nЧтобы сделать все графики сразу, можно воспользоваться функцией `pairs()`.\n\n```{r}\npairs(Advertising)\n```\n\nЧасто нам интересно посмотреть взаимосвязи только между зависимой переменной и предиктором, а функция `pairs()` выдает много лишнего.\n\nФункция `featurePlot()` из библиотеки `caret` [(Classification And REgression Training)](https://topepo.github.io/caret/), подходит для этой цели гораздо лучше.\n\n```{r}\nlibrary(caret)\nfeaturePlot(x = Advertising[ , c(\"TV\", \"Radio\", \"Newspaper\")], y = Advertising$Sales)\n```\n\nМы видим, что есть явный рост продаж по мере увеличения рекламы на радио и телевидении, тогда как связь с рекламой в газетах не так очевидна.\n\n### Простая линейная регрессия и функция lm()\n\nДавайте построим простую линейную модель для продаж, в которой в качестве предиктора будут выступать затраты на телевизионную рекламу.\n\n```{r}\nmod_1 = lm(Sales ~ TV, data = Advertising)\n```\n\n#### Общие результаты и тестирование гипотез\n\nФункция `summary()` позволяет вывести на экран информацию о модели, полученную с помощью функции `lm()`, которая может быть полезной для тестирования гипотез, касающихся предикторов и оценки значимости регрессионных коэффициентов.\n\n```{r}\nsummary(mod_1)\n```\n\nДавайте разбираться!\n\nВывод начинается с повторения регрессионного уравнения под заголовком `Call`: `lm(formula = Sales ~ TV, data = Advertising)`.\n\nЗатем в модели приводится распределение остатков. Остатки должны иметь нормальное распределение с абсолютными значениями минимума и максимума, также как и квартилями очень близкими друг к другу, что предполагает их примерно одинаковое расстояние от центра распределения.\n\nВ нашем случае это правило выполняется.\n\nСледующая часть вывода содержит таблицу с коэффициентами.\n\nЧтобы понять, что они означают и каким образом получаются, приведем формулу регрессии, но для случая с одной переменной:\n\n$$y_i=\\alpha+\\beta x_i+\\varepsilon_i,$$\n\nгде $y$ - зависимая переменная, $i$ - единица анализа, $\\alpha$ - интерцепт (константа), $\\beta$ - коэффициент регрессии, $x$ - независимая переменная и $\\varepsilon$ - ошибка.\n\nКогда мы работаем с выборочными данными, формула изменяется, так как вместо истинных значений у нас будут оценки:\n\n$$\\hat{y}=\\hat{\\alpha}+\\hat{\\beta}x$$\n\nОценка для интерцепта ($\\alpha$) - значение $y$ когда $x = 0$. В геометрическом смысле это точка пересечения регрессионной прямой с осью $OY$. Иногда интерцепт может иметь смысл и подлежит интерпретации, но часто он может принимать несуществующие значения, выходящие за рамки возможных значений переменных, не описывается и не интерпретируется (допустим, мы пытаемся выявить зависимость веса от роста, получается, что интерцепт нам покажет, чем равен вес, когда рост равен нулю, что не имеет смысла).\n\nВ нашем примере интерцепт - это среднее значение продаж при нулевых затратах на рекламу.\n\nДалее следуют коэффициенты для предикторов. Для каждого предиктора коэффициент обозначает ожидаемое изменение в зависимой переменной при изменении предиктора на одну единицу.\n\nГеометрический смысл beta-beкоэффициента: это угол наклона регрессионной прямой:\n\n![](https://assets.coursehero.com/study-guides/lumen/images/pima-concepts-statistics/linear-regression-3-of-4/m3_examining_relationships_topic_3_2_fit_line_algebra11.gif)\n\nЧтобы найти коэффициенты $\\alpha$ и $\\beta$ нам нужно понять, как они вычисляются. Начнем с коэффициентов $\\beta$:\n\n$$\\hat{\\beta}=\\frac{cov(x,y)}{var(x)}$$\n\nЧтобы узнать коэффициент $\\beta$ для рекламы на телевидении, нам необходимо найти ковариацию между рекламой и продажами и дисперсию затрат на рекламу:\n\n```{r}\ncov_x_y<-cov(Advertising$TV, Advertising$Sales)\nvar_x<-var(Advertising$TV)\nbeta_x<-cov_x_y/var_x\nbeta_x\n```\n\nКоэффициент показывает, на сколько увеличатся продажи, при увеличении затрат на рекламу на единицу.\n\nПосчитав коэффициент $\\beta$, мы можем перейти к $\\alpha$:\n\n$$\\hat{\\alpha}=\\bar{y} - \\hat{\\beta}\\bar{x}$$\n\nДля того, чтобы вычислить его вручную, нам необходимо знать средние значения $\\bar{y}$ и $\\bar{x}$.\n\n```{r}\ny_bar<-mean(Advertising$Sales)\nx_bar<-mean(Advertising$TV)\nintercept<-y_bar-beta_x*x_bar\nintercept\n```\n\nВсе сходится!\n\nТеперь мы можем посчитать предсказанные значения по продажам на основе нашей модели:\n\n```{r}\nyhat <- intercept + beta_x * Advertising$TV\nhead(yhat) \n```\n\nКогда мы проводим регрессионный анализ, один из главных вопросов, которые мы себе задаем - можно ли на основе знаний о переменной $x$ понять, как ведет себя $y$. Говоря формальным языком, мы исследуем эту взаимосвязь, оценивая остаточные значения, ассоциированные с коэффициентами $\\alpha$ и $\\beta$, на основе тестирования гипотез о том, отличаются ли данные коэффициенты от нуля.\n\nНапример:\n\n-   $H_0: \\beta=0$\n-   $H_1: \\beta \\neq 0$\n\nИными словами, если коэффициент $\\beta$ равен нулю, то переменная $x$ никак не объясняет $y$, так как $0 \\times x=0$.\n\nПоскольку мы основываемся на допущении о том, что остатки имеют нормальное распределение, это нам позволяет рассчитать t-статистику для коэффициентов и проверить их статистическую значимость.\n\nХотя R все делает автоматически, давайте разберемся, как это происходит.\n\nПосчитаем разницу между реальными и предсказанными значениями по продажам:\n\n```{r}\nres <- Advertising$Sales - yhat\nhead(res)\n```\n\nДалее мы должны посчитать разброс наблюдений вокруг регрессионной прямой, которую мы только что воспроизвели, а также стандартную ошибку остатков, которая используется для оценки ошибок коэффициентов и их статистической значимости.\n\nЧтобы найти стандартную ошибку остатков, нам требуется:\n\n-   найти сумму квадратов отклонений остатков ($RSS$)\n-   найти количество степеней свободы ($df=N-2$)\n\n```{r}\nres.sqr <- res^2\nRSS <- sum(res.sqr, na.rm=T)\ndf <- length(Advertising$Sales) - 2\ndf\nRSE <- sqrt(RSS / df)\nRSE\n```\n\nСобственно говоря, это мы и видим в выводе: `Residual standard error: 3.259 on 198 degrees of freedom`.\n\nЗная стандартную ошибку остатков, мы можем вычислить стандартные ошибки для наших коэффициентов. Для этого, мы должны сначала вычислить сумму квадратов отклонений по независимой переменной (затраты на рекламу ТВ):\n\n```{r}\nTSSx <- sum((Advertising$TV - x_bar)^2)\nTSSx\n```\n\nЧтобы найти стандартную ошибку коэффициента $\\beta$, нужно стандартную ошибку остатков разделить на квадратный корень из суммы квадратов отклонений по переменной $x$:\n\n$$SE_{\\beta}=\\frac{RSE}{\\sqrt{TSS_x}}$$\n\n```{r}\nSEB <- RSE / sqrt(TSSx)\nSEB\n```\n\nДля интерцепта алгоритм несколько отличается:\n\n$$SE_{\\alpha}=RSE*\\sqrt{\\frac{1}{N}+\\frac{\\bar{x^2}}{TSS_x}}$$\n\n```{r}\nSEA <- RSE * sqrt((1 / 200)+(x_bar^2 / TSSx))\nSEA\n```\n\nЗная стандартные ошибки, мы можем теперь посчитать соответствующие t-статистики, чтобы оценить, отличаются ли наши коэффициенты от нуля. Для этого нужно значения коэффициентов разделить на их стандартные ошибки.\n\nДля коэффициента по переменной телевизионной рекламы:\n\n```{r}\nt.B <- beta_x / SEB\nt.B\n```\n\nДля интерцепта:\n\n```{r}\nt.A <- intercept / SEA\nt.A\n```\n\nЭти значения измеряются в стандартных отклонениях и показывают, насколько далеко наши коэффициенты находятся от нуля. Значения 17,6 и 15,4 очень большие, следовательно, наши коэффициенты статистически значимы, что и подтверждают соответствующие p-значения из вывода: `15.36   <2e-16 ***` и `17.67   <2e-16 ***`.\n\n#### Показатели качества модели\n\nКакие еще важные показатели мы должны принять во внимание, когда мы анализируем результаты регрессионного анализа?\n\nОбратимся к оставшейся части вывода.\n\nОсновной мерой, показывающей, насколько хорошо регрессионная модель объясняет данные, является коэффициент детерминации - $R^2$. Для того, чтобы найти $R^2$, нужны следующие промежуточные вычисления о некоторых компонентах дисперсии зависимой переменной:\n\n-   сумме квадратов остатков ($RSS$)\n-   общей сумме квадратов отклонений от среднего ($TSS$)\n-   сумме квадратов отклонений, объясненной моделью ($ESS$)\n\n```{r}\nTSS <- sum((Advertising$Sales - y_bar)^2)\nTSS\nESS <- TSS - RSS\nESS\nr.sqr <- ESS / TSS\nr.sqr\n```\n\nТаким образом, 61,2% дисперсии зависимой переменной объясняется регрессионной моделью.\n\nНесмотря на то, что показатель $R^2$ является довольно информативным, у него есть один существенный недостаток: он имеет свойство неоправданно возрастать, при включении дополнительных переменных в анализ, даже если они не оказывают существенного влияния на зависимую переменную. Иными словами, чем более комплексной будет модель, тем выше будет $R^2$, что не очень хорошо.\n\nПоэтому вместо обычного $R^2$ в качестве более точной оценки качества модели используется скорректированный показатель - `adjusted`$R^2$. Проблему множественных предикторов этот показатель решает, путем внесения «наказаний» (пенальти) за включение в модель дополнительных переменных. Чтобы найти скорректированный $R^2$ используется формула:\n\n$$1-\\frac{(1-R^{2})(n-1)}{n-k-1},$$\n\nгде $k$ - количество предикторов в модели, не считая интерцепта (A).\n\n```{r}\nr.sqr.adj<-1-(((1 - r.sqr) * (200 - 1)) / (200 - 1 - 1))\nr.sqr.adj\n```\n\nПоскольку у в модели один предиктор, значение уменьшилось незначительно.\n\nУ нас остался нерассмотренным только один показатель из вывода - F-статистика. F-критерий является «глобальным» тестом, показывающим, насколько лучше наша модель базовой модели - такой, в которую включен только один интерцепт.\n\nЕще одна интерпретация: этот тест показывает, что в нашей модели есть хотя бы один значимый предиктор.\n\nВ нашем выводе `F-statistic: 312.1 on 1 and 198 DF,  p-value: < 2.2e-16`, что указывает на то, что модель с предиктором существенно лучше базовой модели объясняет зависимую переменную.\n\n#### Сравнение нескольких моделей\n\nДопустим, мы хотим создать несколько двумерных моделей и сравнить их. Это возможно с помощью функции `mtable()` из пакета `memisc` (Management of Survey Data and Presentation of Analysis Results - управление данными исследований и презентация результатов анализа). Для демонстрации создадим три модели, иллюстрирующие взаимосвязь между продажами и каждым типом рекламы.\n\nПервая модель у нас уже есть, создадим две других:\n\n```{r}\nmod_2 = lm(Sales ~ Radio, data = Advertising)\nmod_3 = lm(Sales ~ Newspaper, data = Advertising)\n```\n\nБлагодаря функции 'mtable()' мы можем создать таблицу, в которой сведем всю важную информацию по всем трем моделям:\n\n```{r}\nlibrary(memisc)\nmtable<-mtable(mod_1, mod_2, mod_3)\nmtable\n```\n\nВидим, что хотя во всех моделях интерцепты и коэффициенты предикторов являются значимыми, показатель $R^2$ максимально высок в модели, где в качестве объясняющей переменной используется показатель затрат на рекламу на телевидении.\n\n### Множественная регрессия\n\nРассмотрим случай, когда количество предикторов больше одного, то есть наша модель является моделью уже не простой, а множественной регрессии.\n\nФормула для нескольких предикторов приобретает вид:\n\n$$\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1}x_1 + \\hat{\\beta_2}x_2 ...+... \\hat{\\beta_n}x_n$$\n\nСинтаксис в R аналогичен тому, что мы использовали для двумерной регрессии. Создадим модель, в которую включим сразу все независимые переменные.\n\n```{r}\nmod_4 = lm(Sales ~ TV + Radio + Newspaper, data = Advertising)\n#mod_4 = lm(Sales ~ ., data = Advertising) можно использовать и такой синтаксис\n```\n\nКроме функции `summary()` красивую таблицу с результатами можно создать с помощью функции `tab_model()` из библиотеки `sjPlot`\n\n```{r}\nsjPlot::tab_model(mod_4)\n```\n\nРезультаты показывают, что значимыми являются только коэффициенты для радио- и телерекламы, тогда как реклама в газетах не является значимым фактором, определяющим продажи.\n\nСкорректированный коэффициент детерминации (`Adjusted R-squared`), показывает, что эта модель гораздо лучше, чем любая модель с одним предиктором, и объясняет 89,6% дисперсии.\n\nДополнительно, в целях сравнения, давайте создадим более простую модель, без газет.\n\n```{r}\nmod_5 = lm(Sales ~ TV + Radio, data = Advertising)\nsjPlot::tab_model(mod_5)\n```\n\nКак видим, коэффициент детерминации не изменился (что неудивительно, ведь у удаленной переменной коэффициент регрессии равнялся нулю).\n\n#### Сравнение моделей с помощью дисперсионного анализа\n\nЧтобы сравнить, какая модель работает лучше, можно применить функцию `anova()`, запускающую дисперсионный анализ. В нашем случае, мы будем сравнивать модель со всеми предикторами **mod_1** с сокращенной моделью **mod_0**. Наша задача будет заключаться в том, чтобы понять, какую роль играет переменная газетной рекламы в аддитивной модели.\n\n```{r}\nanova(mod_4, mod_5)\n```\n\nМы видим, что разница между моделями в одну степень свободы (1 параметр - как раз наша переменная о рекламе в газетах).\n\n::: {.alert .alert-info role=\"alert\"}\n**Число степеней свободы (df)** − важный показатель регрессионного анализа, используемый в формулах метрик, показывающих качество модели:\n\n-   Res.Df - число степеней свободы, рассчитываемых для остатков (разности между предсказанными и реальными значениями).\n-   Res.Df - количество наблюдений - количество оцениваемых параметров.\n-   Model 1: Sales \\~ TV + Radio\n-   df= 197= 200-3 (2 предиктора + константа)\n-   Model 2: Sales \\~ TV + Radio + Newspaper\n-   df= 196= 200-4 (2 предиктора + константа)\n:::\n\nРезультаты дисперсионного анализа показывают, что качество модели не поменялось, и значит мы можем использовать более лаконичную (сокращенную) модель.\n\n#### Предсказание значений зависимой переменной для новых данных\n\nОбычно у регрессионного анализа две основные задачи - **объяснение** взаимосвязи между переменными и **предсказание** новых (неизвестных) значений зависимой переменной на основе модели. Для осуществления прогноза чаще всего используется функция `predict()`, обладающая большой гибкостью (может применяться с различными методами моделирования и типами данных).\n\nЕсли эту функцию использовать к модели, созданной на основе функции `lm()`, то она будет рассчитывать предсказанные значения для каждого наблюдения.\n\nДавайте посмотрим первые десять.\n\n```{r}\nhead(predict(mod_5), n = 10)\n```\n\nОтметим, что эффект функции `predict()`будет зависеть от того, какие данные даются на входе. Наша модель относится к классу`lm`, поэтому `predict()` запускает функцию`predict.lm()` Если нам нужно что-то другое, можно посмотреть подробности с помощью `?predict.lm()`.\n\nМы также можем сгенерировать новые данные, и попробовать посчитать зависимую переменную на них.\n\nДавайте создадим новый набор с идентичными именами переменных.\n\n```{r}\nnew_obs = data.frame(TV = 150, Radio = 40, Newspaper = 1)\n```\n\nТеперь мы можем использовать `predict()`, чтобы посчитать оценки и доверительные интервалы для новых данных.\n\nЕсли указать только модель и источник данный, `R` выдаст точечную оценку, то есть \"предсказанное значение\" $\\hat{y}$.\n\n```{r}\npredict(mod_5, newdata = new_obs)\n```\n\nЕсли указать дополнительно аргумент`interval` со значением `\"confidence\"`, `R` покажет также 95% доверительные интервалы для среднего значения по данному наблюдению.\n\n```{r}\npredict(mod_1, newdata = new_obs, interval = \"confidence\")\n```\n\nКроме того, мы можем изменить уровень и выбрать не доверительные интервалы, а предсказательные интервалы (доверительные интервалы прогноза). В чем отличие?\n\nПредсказательные интервалы показывают, в каком диапазоне значений будет находиться будущее наблюдение, тогда как доверительные интервалы показывают вероятный диапазон, в котором будет находится какой-либо статистический параметр, например, среднее в генеральной совокупности.\n\nПоскольку предсказательные интервалы рассчитываются в ситуации большей неопределенности, то они обычно шире, чем доверительные интервалы.\n\n```{r}\npredict(mod_1, newdata = new_obs, interval = \"prediction\", level = 0.95)\n```\n\n#### Диагностика модели и оценка влияния наблюдений на результаты\n\nВ `R` доступны несколько функций, позволяющих оценить, насколько полученная модель хорошо воспроизводит исходные данные, и как различные наблюдения вносят вклад в предсказательные способности этой модели:\n\n-   `resid()` выдает остаток (разность между предсказанным и реальным значением)\n-   `hatvalues()` показывает `leverage` - отклонение в значениях по независимым переменным по каждому наблюдению. Данный показатель важен для понимания, как экстремальные значения по независимым переменным могут повлиять на результаты анализа.\n\n::: {callout-info}\nЧто такое `hat` - значения? `hat` - *по-английски* «шляпа», а также диакритический знак «циркумфлекс» ($\\hat{ }$), с помощью которого обозначаются значения зависимой переменной, предсказанные с помощью регрессионной модели.\n\nЭти предсказанные значения обозначаются как $\\hat{y}$ и рассчитываются по формуле:\n\n$$\\hat{y}=Xb$$\n\nДля коэффициентов линейной регрессии используется следующая формула:\n\n$$b = (X^{'}X)^{-1}X^{'}y$$\n\nСледовательно, мы можем переписать уравнение для предсказанных значений как:\n\n$$\\hat{y}=X(X^{'}X)^{-1}X^{'}y$$\n\nТаким образом, предсказанные значения могут быть получены путем умножения $n \\times 1$ вектора $y$, содержащего наблюдаемые значения на $n \\times n$ матрицы $H$:\n\n$$H=X(X^{'}X)^{-1}X^{'}$$\n\nИли, более лаконично:\n\n$$\\hat{y}=Hy$$\n\nМатрица $H$ часто называется `hat-matrix` - «матрица в шляпе», а ее диагональные значения как раз и являются значениями левериджа.\n:::\n\n-   `rstudent()` стьюдентизированные остатки по каждому наблюдению (остаток в регрессионной модели деленный на ее скорректированную стандартную ошибку)\n-   `cooks.distance()` рассчитывает важность каждого наблюдения\n\n```{r}\nddf <- data.frame(residuals=residuals(mod_5), rstandard=rstandard(mod_5), rstudent=rstudent(mod_5), leverage=hatvalues(mod_5), cookd=cooks.distance(mod_5))\n```\n\nКак мы можем это использовать?\n\nНапример, мы можем отобрать наблюдения, чьи стандартизированные остатки отклоняются более, чем на 2 стандартных отклонения в обе стороны:\n\n```{r}\nlibrary(dplyr)\nfilter(ddf, abs(rstandard) > 2 | abs(rstudent) > 2)\n```\n\n> {{< iconify arcticons brain-it-on size=42px >}} **Задание**: проанализируйте в таблице исходных данных наблюдения с указанными номерами. Какие выводы можно сделать?\n\nВторой важный момент: анализ показателей `leverage` и `Cook's distance`.\n\nЗамечательная вещь по поводу левериджа заключается в том, что его значения помогают выявить экстремальные значения $x$, которые могут влиять на результаты регрессионного анализа. Каким образом? Мы должны понять, какое значение левериджа нужно признать большим, то есть соответствующим значениям $x$, расположенным максимально далеко от средних значений по всем другим наблюдениям. Общим является правило, согласно которому, любое наблюдение, чье значение левериджа в три раза превышает среднее значение, является нетипичным / странным / достойным внимания:\n\n$$\\bar{h}=\\frac{\\sum_{i=1}^{n}h_{ii}}{n}=\\frac{p}{n}$$\n\nИными словами, если:\n\n$$h_{ii} >3\\left( \\dfrac{p}{n}\\right),$$\n\nто мы должны обратить внимание на это наблюдение. Сумма всех значений левериджа равняется количеству параметров модели: `r sum(ddf$leverage)` - два предиктора + интерцепт (константа).\n\n```{r}\nhat_max = 3*3/200\nfilter(ddf, leverage>hat_max)\n```\n\nЧто мы видим? Мы видим, что по модели 5 у нас нет таких наблюдений, чей леверидж превышал бы максимально возможное значение.\n\nМы также можем отсортировать наблюдения по расстоянию Кука, чтобы понять, какие наблюдения являются наиболее влиятельными:\n\n$$D_i=\\frac{(y_i-\\hat{y}_i)^2}{(k+1) \\times MSE}\\left[ \\frac{h_{ii}}{(1-h_{ii})^2}\\right],$$\n\nгде $MSE$ - среднеквадратическая ошибка регрессии, а $h_{ii}$ - значения левериджа.\n\n```{r}\narrange(ddf, desc(cookd))[1:6,]\n```\n\nРекомендуется исключать из анализа наблюдения, расстояние Кука для которых превышает 1. В нашем анализе таких нет, но вот наблюдения 131 и 6 являются все-таки подозрительными, как имеющие наиболее расстояние Кука и самые большие остатки.\n\nАналогичную информацию можно получить с помощью специальных графиков:\n\n```{r}\npar(mfrow = c(2, 2))\nplot(mod_5)\n```\n\nЧто показывают графики?\n\n**1. Residuals vs Fitted** (Остатки vs предсказанные значения)\n\nЭтот график показывает, есть ли в остатках регресии какие-либо нелинейные паттерны. Такое может случиться, если между предикторными переменными и зависимой переменной имеются нелинейные взаимосвязи, соответственно если эта нелинейность возникает на графике, значит модель плохо воспроизводит эти отношения. Если мы видим, что остатки равномерно распределены вокруг линии предсказанных значений без каких-либо серьезных колебаний, это хороший знак, значит у нас в модели таких нелинейных взаимосвязей нет. На нашем графике есть еле заметный «прогиб», но четким паттерном его назвать вряд ли возможно.\n\n**2. Normal Q-Q residuals**\n\nДанный график показывает, что остатки нормально распределены (то есть маленьких остатков много и их среднее значение приближается к нулю, в больших остатков мало). У нас с нормальностью остатков практически все в порядке, если не считать постоянно выбивающееся наблюдение 131.\n\n**3. Scale-Location**\n\nДанный график позволяет протестировать допущение о гомогенности дисперсии остатков (гомоскедастичности). Если мы видим, что остатки распределены вдоль линии равномерно, и их форма не напоминает «фен», то все хорошо.\n\n**4. Residuals vs Leverage**\n\nНу и, наконец, последний график визуализирует самые влиятельные наблюдения - одновременно через леверидж и расстояние Кука. Сомнительные наблюдения на всех графиках обозначены цифрами.\n\nБиблиотека `olsrr` (Tools for Building OLS Regression Models) также содержит несколько полезных функций, которые могут помочь в выявлении таких наблюдений.\n\n```{r}\nlibrary(olsrr)\nols_plot_cooksd_bar(mod_1)\n```\n\n```{r}\nols_plot_cooksd_chart(mod_1)\n```\n\n```{r}\nols_plot_dfbetas(mod_1)\n```\n\nЧто дальше? Мы выяснили, что некоторые наблюдения являются нетипичными, что может приводить к искаженным вычислениям. Но мы можем попробовать удалить переменные, которые вызвали наибольшее количество вопросов, и сравнить результаты.\n\n```{r}\nAdvertising2<-Advertising[-c(6,131),]\nmod_6 = lm(Sales ~ TV + Radio, data = Advertising2)\nmtable<-mtable(mod_5, mod_6)\nmtable\n```\n\nПосле удаления экстремальных наблюдений, качество модели улучшилось (скорректированный $R^2=91.5%$), хотя общие выводы аналогичны.\n\n#### Мультиколлинеарность\n\nЕще один сложный термин))) Что такое мультиколлинеарность? Мультиколлинеарность случается тогда, когда один предиктор может предсказывать другой. Иными словами, мы хотели бы, чтобы предикторы хорошо предсказывали поведение зависимой переменной, но не друг друга, и если такое случается, то это и называется мультиколлинеарностью. Хотя слишком высокая мультиколлинеарность является редкостью, проверка на нее является одной из стандартных процедур регрессионного анализа. Отметим, что проблема мультиколлинеарности является важной, когда мы исследуем важность предикторов, пытаемся на основе интерпретации коэффициентов регрессии обнаружить значимые закономерности (например, доказать, что повышение уровня образования может привести к значительному увеличению доходов или что по мере развития ассоциаций между гражданами увеличивается уровень институционального доверия). Если же первостепенной задачей моделирования является предсказание (как бывает во многих задачах машинного обучения), то проблема мультиколлинеарности не является релевантной, и ее можно проигнорировать.\n\nКак мы можем проверить, если в нашей модели чрезмерная мультиколлинеарность?\n\nСамый простой способ - посмотреть на коэффициенты корреляции между предикторами:\n\n```{r}\nAdvertising %>%\n  dplyr::select(TV, Radio, Newspaper) %>%\n  cor()\n```\n\nНаши независимые переменные связаны друг с другом довольно слабо. Специальной мерой, позволяющей проверить мультиколлинеарность, является $VIF$- variance inflation factor, показывающая увеличение в дисперсии коэффициентов после включения дополнительной переменной:\n\n```{r}\ncar::vif(mod_6)\n```\n\nVIF \\< 3 обозначает слабую корреляцию между переменными (идеальные условия). Чаще всего в литературе приводится пороговое значение $VIF=5$, и только переменные $VIF<5$ должны быть включены в модель.\n\nУ нас в модели с мультиколлинеарностью все в порядке.\n\n### Линейная регрессия с категориальными предикторами\n\nНапомним, что **категориальные переменные** (также известные, как качественные, или факторные переменные) - это такие переменные, которые позволяют разделить наблюдения на группы. Их особенностями является ограниченное количество значений (уровней). Типичными являются примеры с полом (два уровня - мужчины и женщины) или национальностью, социальным статусом или уровнем образования (например, лица с общим средним, средним профессиональным и высшим образованием).\n\nОбычно регрессионный анализ проводится с количественными переменными, и когда исследователь желает включить в модель категориальную переменную, необходимы некоторые шаги, чтобы сделать результаты более интерпретируемыми.\n\nВ частности, категориальные переменные перекодируются в набор так называемых «dummy» (фиктивных) переменных, в результате создается **матрица контрастов**. Современные программы, в том числе и R, «умеют» это делать автоматически.\n\n> {{< iconify arcticons anz size=42px >}} **Пример**: воспользуемся набором данных `Salaries` из пакета `car`, в котором содержатся данные о зарплате ассистентов, ассоциированных профессоров и профессоров в одном из американских колледжей (данные - за 2008-2009 учебный год). Данные были собраны администрацией для того, чтобы отслеживать различия между зарплатой, получаемой преподавателями мужчинами и женщинами,\n\nЗагрузим данные:\n\n```{r}\nlibrary(car)\ndata(\"Salaries\")\nhead(Salaries, 3)\n```\n\n#### Категориальные переменные с двумя уровнями\n\nВспомним, что в регрессионном уравнении для того, чтобы предсказать переменную $y$ на основе независимой переменной $x$, нужно суммировать все основные компоненты:\n\n$$y = b_0 + b_1*x$$\n\nПри этом:\n\n-   $b_0$ и $b_1$ являются регрессионными коэффициентами, представляющими константу (интерцепт) и угол наклона регрессионной прямой (`slope`).\n\nДопустим, мы хотим проанализировать различия в заработной плате у мужчин и женщин.\n\nНа основе переменной пола, мы можем создать новую фиктивную переменную, которая будет принимать значения:\n\n-   1 если преподаватель мужчина\n-   0 если преподаватель женщина\n\nи использовать эту переменную в регрессионном уравнении. При этом интерпретация коэффициентов и самого уравнения будет следующей:\n\n-   $b_0$ средняя зарплата у женщин,\n-   $b_0 + b_1$ средняя зарплата у мужчин,\n-   $b_1$ различия в среднем между зарплатой мужчин и женщин.\n\nСоздадим модель:\n\n```{r}\nmod_7  <- lm(salary ~ sex, data = Salaries)\nsummary(mod_7)$coef\n```\n\nИсходя из выведенной информации, средняя зарплата у преподавателей женщин - 101002 долларов (за 9 месяцев), тогда как у мужчин `101002 + 14088 = 115090`. Полученное p-значение для фиктивной переменной `sexMale` очень значимое, что указывает на то, что имеются статистические обоснования наличия различий в зарплате по полу.\n\nФункция `contrasts()`позволяет посмотреть код, который использовался для создания фиктивных переменных:\n\n```{r}\ncontrasts(Salaries$sex)\n```\n\nПри такой кодировке женщины являются референтной группой, с которой сравниваются мужчины, и в целом, любая подобная кодировка является условной, ее результаты будут влиять только на интерпретацию коэффициентов регрессии.\n\nЕсли нас такая кодировка не устраивает, мы можем использовать функцию `relevel()` для смены уровней:\n\n```{r}\nSalaries <- Salaries %>%\n  mutate(sex = relevel(sex, ref = \"Male\"))\n```\n\nПосле перекодировки результаты регрессионного анализа будут следующими:\n\n```{r}\nmod_8 <- lm(salary ~ sex, data = Salaries)\nsummary(mod_7)$coef\n```\n\nПоскольку мы теперь сравниваем зарплату женщин с зарплатой мужчин, коэффициент переменной `sexFemale` негативный, что означает более низкий уровень зарплат у женщин, по сравнению с мужчинами.\n\nКоэффициент $b_0$ равено 115090 (средняя зарплата у мужчин), тогда как коэффициент $b_1$ - -14088, показывает, на сколько, в среднем, ниже зарплата у женщин. Соответственно, 115090 - 14088 = 101002 - средняя зарплата женщин.\n\n#### Категориальная переменная с более чем двумя уровнями\n\nЧто делать, если в качественной переменной, которую мы хотим использовать, более двух уровней? Наиболее типичным является подход, когда такая категориальная переменная трансформируется в `n-1` бинарных переменных, каждая из которых имеет по два уровня. И эти `n-1` новых переменных содержат ту же информацию, что исходная переменная. В результате такой кодировки создается таблица контрастов.\n\nНапример, в нашем наборе есть переменная `rank`, которая имеет три уровня: `AsstProf`, `AssocProf` и `Prof`. Мы можем создать две фиктивных переменных - `AssocProf` и `Prof`:\n\n-   если `rank = AssocProf`, тогда в новом столбце `AssocProf` преподавателями, являющими ассоциированными профессорами, будет присвоено значение 1, а профессорам - 0.\n-   если `rank = Prof`, тогда в новом столбце `Prof` все профессора получат значение 1, а ассоциированные профессора - 0.\\\n-   что же с ассистентами? В обоих новых столбцах они получат значение 0.\n\nТакого рода кодировка в R осуществляется автоматически. С помощью функции `model.matrix()` мы можем посмотреть, как такая матрица контрастов может выглядеть:\n\n```{r}\nres <- model.matrix(~rank, data = Salaries)\nhead(res[, -1])\n```\n\nВ практике регрессионного анализа есть различные способы кодирования категориальных переменных (создания контрастов). По умолчанию в R первый уровень используется в качестве референтного, а остальные интерпретируются уже по отношению к этому уровню.\n\n::: {callout-tip}\nПример, который мы только что рассмотрели, показывает, что дисперсионный анализ - ANOVA (analyse of variance) является специальным случаем линейной модели, в которой предикторами являются категориальные переменные. И поскольку R это тоже «понимает», мы можем извлечь из модели результаты дисперсионного анализа (предпочтительнее использовать функцию `Anova()` из пакета `car` (car означает Companion to Applied Regression - компаньон для прикладных задач регрессионного анализа).\n:::\n\nСоздадим модель, в которой мы будем предсказывать зарплату от всех других переменных в наборе (знак плюс означает, что мы будем рассматривать только главные эффекты, без интеракций):\n\n```{r}\nmod_9<- lm(salary ~ yrs.service + rank + discipline + sex,\n             data = Salaries)\nAnova(mod_9)\n```\n\nПосле того, как мы приняли во внимание другие переменные (стаж - yrs.service, должность - rank, область знаний - discipline), стало понятно, что фактор пола уже не имеет значения и не вносит вклада в вариабельность заработной платы. Значимыми становятся должность и область знания.\n\nЧтобы вывести более подробные результаты анализа, лучше воспользоваться функцией `summary()`:\n\n```{r}\nsummary(mod_9)\n```\n\nРезультаты показывают, что зарплата ассоциированного профессора в среднем на 14560.40 долларов выше, чем у ассистента, при прочих равных условиях, а у профессора - выше на 49159.64 долларов. Интересно, что зарплата значительно варьирует от специализации: на прикладных кафедрах (applied departments) наблюдается в среднем на 13473.38 долее высокая зарплата, по сравнению с теоретическими дисциплинами (theoretical departments).\n\n#### Интеракции\n\nИнтеракции происходят тогда, когда эффект одного из предикторов зависит от другой переменной в модели.\n\nЧтобы продемонстрировать эффект интеракции, рассмотрим взаимосвязь между должностью и областью знаний в примере про зарплату преподавателей:\n\n$$\n\\begin{split}\ny_i &=\\beta_0 + \\beta_1*(rank) + \\beta_2*(discipline) + \\beta_3*(rank*discipline) +\\\\ & \\beta_4*(yrs.service) + \\beta_5*(sex) + \\varepsilon_i\n\\end{split}\n$$\n\n```{r}\nmod_9 <- lm(salary ~ yrs.service + sex + rank * discipline, data = Salaries)\nsummary(mod_9)\n```\n\n::: {callout-tip}\nОтметим, что хотя в формуле мы указали только интеракцию, в выводе содержатся также сведения и об индивидуальных эффектах. R включает эту информацию автоматически.\n:::\n\nИнтерпретируя результаты отметим, что эффект от взаимосвязи не значим, и единственным значимым предиктором в модели остается должность: значительная прибавка в зарплате отмечается только у профессоров, тогда как дисциплинарная принадлежность значима только на уровне статистической тенденции ($p=0,086$).\n\n#### Отбор переменных для модели\n\nПрежде чем перейти к моделированию, аналитик проводит тщательную работу по отбору переменных. Обычно, этому предшествует теоретический анализ, который позволит определить, какие показатели, важные для целевой переменной, необходимо включить в исследование, а затем - в модель.\n\nОднако, когда эксперимент уже проведен, наступает время проверки статистических гипотез. Очевидно, что не всегда все включаемые в модель параметры, в конце концов оказываются значимыми.\n\nКакие алгоритмы мы можем использовать для определения финальной, самой лучшей модели из возможных?\n\nОтбор переменных (variable selection) - это процесс выбора наиболее значимых переменных для включения в регрессионную модель. Методы отбора помогают улучшить производительность модели и избежать чрезмерной подгонки.\n\nВ рамках данного занятия мы рассмотрим следующие методы отбора:\n\n-   анализ всех возможных моделей / лучшей модели, определяемой на основе оценке качества модели\n-   пошаговые алгоритмы\n\nДля работы мы будем использовать пакет `olsrr`:\n\n```{r eval=FALSE}\ninstall.packages(\"olsrr\")\nlibrary(olsrr)\n```\n\n###### Анализ всех возможных моделей\n\nПрежде чем мы рассмотрим методы пошагового отбора, давайте вкратце рассмотрим регрессию по всем/лучшим подмножествам. Поскольку они оценивают все возможные комбинации переменных, эти методы требуют больших вычислительных затрат и могут вывести систему из строя, если использовать их с большим набором переменных.\n\nМетод `All subset regression` (все возможные варианты) представляет результаты по всем возможным комбинациям предикторов. Если у нас есть $k$ потенциальных независимых переменных, не считая константы, то количество отдельных моделей, которые потребуется проанализировать, составит - $2^k$. Например, если у нас 10 предикторов, то количество моделей - $2^10$ - `r 2^10`, а если переменных 20 - то количество комбинаций превышает миллион.\n\n```{r}\nmodel <- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)\nols_step_all_possible(model)\n```\n\n###### Подборка лучших моделей (Best Subset Regression)\n\nДанный метод позволяет отобрать модели, которые являются лучшими по обобщенным критериям модели, например, имеет наибольший $R^2$ или меньшие $MSE$ (средняя квадратичная ошибка) или $AIC$ (информационный критерий Акаике).\n\n```{r}\nmodel <- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)\nols_step_best_subset(model)\n```\n\n###### Пошаговый отбор (Stepwise Selection)\n\n**Пошаговая регрессия** - это метод подбора регрессионных моделей, который предполагает итерационный отбор независимых переменных для использования в модели. Он может быть реализован с помощью прямого отбора, обратного исключения или комбинации обоих методов.\n\nМетод **прямого отбора** начинается с модели без предикторов и постепенно добавляет каждую новую переменную, проверяя ее статистическую значимость, а метод **обратного исключения**, напротив,ю начинается с полной модели и затем по очереди удаляет наименее статистически значимые переменные.\n\n> {{< iconify arcticons anz size=42px >}} **Пример**: Для иллюстрации возможностей пошагового отбора воспользуемся данными из области недвижимости (HousingData).Набор включает данные по 506 объектам недвижимости, оцененных по 14 показателям.\n\nПеременные:\n\n-   CRIM : уровень преступности\n-   ZN : proportion of residential land zoned for lots over 25,000 sq.ft.\n-   INDUS : доля промышленных предприятий среди нежилых объектов\n-   CHAS : дамми переменная, показывает расположение относительно главной реки\n-   NOX : концентрация нитрита озота\n-   RM : среднее количество комнат\n-   AGE : доля зданий, построенных до 1940\n-   DIS : взвешенное расстояние до пяти значимых бостонских деловых центров\n-   RAD : индекс доступности хайвея\n-   TAX : налоги\n-   PTRATIO : соотношение между учителями и преподавателями (обеспеченность учителями)\n-   B : доля чернокожено населения\n-   LSTAT : доля населения с низкими доходами\n-   MEDV : медианная стоимость в 1000 долларов\n\n{{< iconify arcticons 1dm size=42px >}}[Скачать данные](https://www.kaggle.com/datasets/altavish/boston-housing-dataset/data)\n\nЗагрузим данные и создадим общую модель:\n\n```{r}\nHousingData<-read.csv(\"HousingData.csv\")\nmodel <- lm(MEDV ~ ., data = HousingData)\nsummary(model)\n```\n\nМетод ступенчатого включения (начинаем с нулевой модели и постепенно добавляем предикторы)\n\n```{r}\nols_step_forward_p(model)\n```\n\nМетод пошагового исключения:\n\n```{r}\nols_step_backward_p(model)\n```\n\nПринудительное включение в модель по имени переменной:\n\n```{r}\nols_step_forward_p(model, include = c(\"AGE\", \"LSTAT\"))\n```\n\nПринудительное включение по индексу:\n\n```{r}\nols_step_forward_p(model, include = c(5, 7))\n```\n\nВыбор на основе коэффициента детерминации:\n\n```{r}\nols_step_forward_adj_r2(model)\n```\n\nВизуализация модели:\n\n```{r}\nk <- ols_step_forward_adj_r2(model)\nplot(k)\n```\n\n###### Иерархический отбор\n\nКогда для отбора переменных используются p-значения, возможно использования иерархического отбора. Этот метод предполагает, что поиск значимых переменных ограничен следующей переменной. Если какая-то переменная не отбирается по причине не подходящего p-значения, то и ни одна последующая переменная не рассматривается для включения.\n\n```{r}\nols_step_forward_p(model, 0.1, hierarchical = TRUE)\n```\n\nПошаговая регрессия может оказаться хорошей идеей, особенно, когда количество предикторов велико и нужно отобрать только самые значимые. Между тем, исследователи отмечают большое количество «подводных камней» и статистических проблем, которые могут возникнут в процессе применения регрессионного анализа, таких как переобученность данных, смещенные оценки, ошибки I рода (Harrell, 2015). Кроме того, в процессе применения пошаговых методов возникает опасная иллюзия итого, что компьютер автоматически отбирает правильные переменные, на самом деле это происходит без связи с теоретическими основаниями и гипотезами исследования. Более того, модель, которая была отобрана на основе какого-то критерия, на самом деле может оказаться нестабильной и малоинформативной. Во многих случаях правильным было бы опираться на теорию и предыдущие исследования, тогда как методы отбора могут рассматриваться в качестве поисковых техник.\n\n## Логистическая регрессия\n\nЛогистическая регрессия применяется в том случае, если наша зависимая перменная имеет вид 0-1, то есть является дихотомической и имеет значения 1 и 0. По сути, такой регрессионный анализ решает задачу классификации, то есть определения принадлежности к одному из двух классов (\"победит\" или \"проиграет\", примут на работу или нет и т.д.).\n\n> {{< iconify arcticons anz size=42px >}} **Пример**: В качестве примера, мы будем рассматривать данные о приеме в высшие учебные заведения. В частности, нас будет интересовать, как результаты выпускных экзаменов GRE (Graduate Record Exam scores) и средние оценки GPA (grade point average), а также престиж учебного заведения связаня с допуском в высшее учебное заведение. Зависимой является переменная admit/don’t admit, которая уже закодирована в формате 0-1.\n\n```{r}\ndata <- read.csv(\"https://stats.idre.ucla.edu/stat/data/binary.csv\")\nhead(data)\n\n```\n\nМодель логистической регрессии имеет вид:\n\n$$\n\\log\\left(\\frac{p(x)}{1 - p(x)}\\right) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots  + \\beta_p x_p.\n$$\n\nОткуда, путем перестановки, мы можем вывести вероятность принадлежности к группе 1:\n\n$$\np(x) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots  + \\beta_p x_p)}} = \\sigma(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots  + \\beta_p x_p)\n$$\n\nОбычно основное уравнение представляется в виде сигмоиды (логистической функции):\n\n$$\n\\sigma(x) = \\frac{e^x}{1 + e^x} = \\frac{1}{1 + e^{-x}}\n$$\n\n![](https://static.javatpoint.com/tutorial/machine-learning/images/linear-regression-vs-logistic-regression.png)\n\nПодгонка модели осуществляется путем максимизации функции правдоподобия, что практически никогда не происходит вручную, и мы предоставим возможность это сделать`R`\n\nНачнем с того, что переведем ранг заведения в факторную переменную:\n\n```{r}\ndata$rank <- factor(data$rank)\n```\n\n```{r}\nmodel_glm <- glm(admit ~ gre + gpa + rank, data = data, family = \"binomial\")\n```\n\nРезультаты логистической регрессии очень похожи на то, что мы видели в линейной регрессии, только вместо `lm()` мы используем `glm()`. Другая особенность - в атрибуте `family = \"binomial\"`, что означает, что у нас будет зависимая переменная, состоящая из двух классов. Если использовать `glm()` с `family = \"gaussian\"` то получится обычная линейная регрессия.\n\nДавайте посмотрим на общие результаты\n\n```{r}\nsummary(model_glm)\n```\n\nВывод в целом напоминает то, что мы видели в линейной регрессии. Видим, что все наши зависимые переменные являются значимыми. Переменные ранга имеют отрицательные коэффициенты, так как сравниваются со значением 1 - группа учебных заведений с наиболее высокими позициями.\n\nОднако, стоит помнить, что коэффициенты в логистической регрессии не простые, они представляют собой логарифм шансов. Что это значит?\n\nДавайте посмотрим на таблицу с нашей зависимой переменной:\n\n```{r}\ntable(data$admit)\n```\n\nВсего допущено 127 человек из 400, то есть вероятность допуска составит: 127/400=0.3175, а отношение шансов (допуска к недопуску): 0.3175/(1-0.1375)=0.3681159. Логарифм данного выражения составит:\n\n```{r}\nlog(0.3175/(1-0.3175))\n```\n\nЭто именно то, что мы бы получили, если бы создали модель только с одним интерцептом:\n\n```{r}\nmodel_null<-glm(admit ~ 1, data = data, family = \"binomial\")\nsummary(model_null)\n```\n\nВернемся к коэффициентам нашей большой модели:\n\n-   изменения в одну единицу по переменной `gre`, логарифм шансов допуска увеличится на 0.002.\n-   изменение на одну единицу в `gpa`, логарифм шансов допуска увеличится на 0.804.\n\nИндикаторные переменные для ранга имеют слегка иную интерпретацию. Например, посещая школу, входящую во вторую группу по престижности, по сравнению с группой 1 изменяет логарифм шансов на -0.675.\n\nВнизу таблицы с коэффициентами располагаются индексы подгонки (AIC).\n\nЧтобы перевести коэффициенты в обычное отношение шансов, применяется экспоненциальная функция. Можно соединить это действие с вычислением доверительных интервалов:\n\n```{r}\nexp(cbind(OR = coef(model_glm), confint(model_glm)))\n```\n\nКак интерпретировать отношение шансов?\n\nДля количественных переменных меняется мало что:\n\n-   изменение на одну единицу gre на 0,2% увеличивает шансы быть принятыми\n-   изменение среднего балла на единицу увеличивает шансы на 123%\n-   а вот учеба в школе ранга 2 снижает шансы на 50%, ранга 3 - на 73.8%, ранга 4 - на 78,8%.\n\n::: {callout-tip}\nОбщая формула перевода отношения шансов в проценты: (OR-1) \\* 100\n:::\n\nСледующий этап - посмотреть, как работает функция `predict()` вместе с `glm()`:\n\n```{r}\nhead(predict(model_glm))\n```\n\nПо умолчанию `predict.glm()` использует `type = \"link\"`.\n\n```{r}\nhead(predict(model_glm, type = \"link\"))\n```\n\nЭто означает, что `R` возвращает:\n\n$$\n\\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 + \\cdots  + \\hat{\\beta}_p x_p\n$$ для каждого наблюдения.\n\nВажно понимать, что это **не** предсказанные вероятности, и для того, чтобы их получить:\n\n$$\n\\hat{p}(x) = \\hat{P}(Y = 1 \\mid X = x)\n$$\n\nмы должны написать `type = \"response\"`\n\n```{r}\nhead(predict(model_glm, type = \"response\"))\n```\n\nСоответственно, это вероятности, но **не** результаты классификации. Для того, чтобы их получить, мы должны сравнить вероятности с пороговым значением.\n\n```{r}\nmodel_glm_pred = ifelse(predict(model_glm, type = \"response\") > 0.5, 1, 0)\nhead(model_glm_pred)\n```\n\nЧто мы сделали?\n\n$$\n\\hat{C}(x) = \n\\begin{cases} \n      1 & \\hat{f}(x) > 0 \\\\\n      0 & \\hat{f}(x) \\leq 0 \n\\end{cases}\n$$\n\nгде\n\n$$\n\\hat{f}(x) =\\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 + \\cdots  + \\hat{\\beta}_p x_p.\n$$\n\nТот код, который мы запустили, делает следующее:\n\n$$\n\\hat{C}(x) = \n\\begin{cases} \n      1 & \\hat{p}(x) > 0.5 \\\\\n      0 & \\hat{p}(x) \\leq 0.5 \n\\end{cases}\n$$\n\nгде\n\n$$\n\\hat{p}(x) = \\hat{P}(Y = 1 \\mid X = x).\n$$\n\nПосчитав классификации, мы можем также посчитать метрики для ошибок.\n\n```{r, message = FALSE, warning = FALSE}\ntab = table(predicted = model_glm_pred, actual = data$admit)\ntab\nlibrary(caret)\nconfusionMatrix = confusionMatrix(tab, positive = \"1\")\nc(confusionMatrix$overall[\"Accuracy\"], \n  confusionMatrix$byClass[\"Sensitivity\"], \n  confusionMatrix$byClass[\"Specificity\"])\n```\n\n![](https://i.stack.imgur.com/NzSnD.jpg)\n\nМы можем также предсказать результаты допуска в вуз для новых данных.\n\nПопробуем их сгенерировать на основе исходных данных:\n\n```{r}\nnewdata1 <- with(data, data.frame(gre = mean(gre), gpa = mean(gpa), rank = factor(1:4)))\n```\n\nПредскажем результаты зачисления:\n\n```{r}\nnewdata1$rankP <- predict(model_glm, newdata = newdata1, type = \"response\")\nnewdata1\n```\n\nВидим, что при средних оценках, учащиеся, обучавшиеся в престижных школах имеют большую вероятность поступить, чем те, кто училися не в очень престижных заведениях.\n\nМы также можем захотеть узнать, насколько хорошо наша модель соответствует действительности. Это может быть особенно полезно при сравнении конкурирующих моделей.\n\nАналогом $R^2$ для логистической регрессии является $R^2 Макфаддена$:\n\n```{r}\nwith(summary(model_glm), 1 - deviance/null.deviance)\n```\n\nЕсли не хочется вычислять вручную, можно воспользоваться готовой функцией `pR2`:\n\n```{r message=FALSE, warning=FALSE}\n#install.packages('pscl')\nlibrary(pscl)\n\npR2(model_glm)['McFadden']\n\n```\n\nЕще одной популярной псевдомерой является $R^2 Nagelkerke$:\n\n```{r message=FALSE, warning=FALSE}\n#install.packages('fmsb')\nlibrary(fmsb)\n\nNagelkerkeR2(model_glm)\n```\n\n## Мультиномиальная логистическая регрессия\n\nЧто делать, если наша зависимая переменная имеет не две, а более категорий? Для этого случая больше подходит мультиномиальная логистическая регрессия.\n\n$$\nP(Y = k \\mid { X = x}) = \\frac{e^{\\beta_{0k} + \\beta_{1k} x_1 + \\cdots +  + \\beta_{pk} x_p}}{\\sum_{g = 1}^{G} e^{\\beta_{0g} + \\beta_{1g} x_1 + \\cdots + \\beta_{pg} x_p}}\n$$\n\nМы не будем погружаться в технические детали, но попробуем реализовать этот подход на практике. мы воспользуемся знакомым нам набором данных `iris`.\n\nЧтобы выполнить мультиномиальный регрессионный анализ нам потребуется функция `multinom` из библиотеки `nnet`, где используется синтаксис, похожий на `lm()` и `glm()`. Лучше добавить `trace = FALSE`, чтобы не выводилась информация об оптимизационных процессах во время обучения.\n\n```{r}\nlibrary(nnet)\nmodel_multi = multinom(Species ~ ., data = iris)\nsummary(model_multi)\n```\n\nЗаметим, что на выходе у нас коэффициенты только для двух классов, так же как и в обычной регрессии у нас есть только коэффициент для одного класса.\n\n## Непараметрическая регрессия. Метод k-ближайших соседей\n\nВсе методы, которые мы рассматривали до этого момента, являются параметрическими. Это можно представить в виде обобщающей формулы.\n\n$$\nf(x) = \\mathbb{E}[Y \\mid X = x]\n$$\n\nНапример, типичная форма для множественной линейной регрессии:\n\n$$\nf(x) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_p x_p\n$$\n\nЗадача аналитика в этом случае заключается в оценке параметров модели и предсказания на их основе.\n\nНепараметрические методы основываются на самих данных, а не на параметрах. В этом случае используется понятие локальности.\n\nРассуждения при этом примерно такие: чему будет равняться y, если x равен...?\n\n$$\n\\hat{f}(x) = \\text{average}(\\{ y_i : x_i = x \\})\n$$\n\nПоскольку не всегда это требование выполняется, то условия чуть-чуть меняются:\n\n$$\n\\hat{f}(x) = \\text{average}( \\{ y_i : x_i \\text{ equal to (or very close to) x} \\} )\n$$\n\nОдним из конкретных примером использования непараметрического подхода является метод ближайших соседей:\n\n$$\n\\hat{f}_k(x) = \\frac{1}{k} \\sum_{i \\in \\mathcal{N}_k(x, \\mathcal{D})} y_i\n$$\n\n![](https://bookdown.org/f100441618/bookdown-regresion/www/KNN.jpg)\n\n### KNN в `R`\n\nПосмотрим, как работает этот метод на данных набора `HousingData`:\n\n```{r}\nlibrary(FNN)\nlibrary(MASS)\ndata(Boston)\n```\n\nСоздаем тренировочную и тестируемые выборки:\n\n```{r}\nset.seed(42)\nboston_idx = sample(1:nrow(Boston), size = 250)\ntrn_boston = Boston[boston_idx, ]\ntst_boston  = Boston[-boston_idx, ]\n```\n\n```{r}\nX_trn_boston = trn_boston[\"lstat\"]\nX_tst_boston = tst_boston[\"lstat\"]\ny_trn_boston = trn_boston[\"medv\"]\ny_tst_boston = tst_boston[\"medv\"]\n```\n\nСоздадим дополнительный набор для переменной `lstat` по которым мы будем предсказывать `medv` для создания графики.\n\n```{r}\nX_trn_boston_min = min(X_trn_boston)\nX_trn_boston_max = max(X_trn_boston)\nlstat_grid = data.frame(lstat = seq(X_trn_boston_min, X_trn_boston_max, \n                                    by = 0.01))\n```\n\nЧтобы применить метод KNN в качестве разновидности регрессионного анализа, нам понадобится функция `knn.reg()` из библиотеки `FNN`.\n\nЕе общая архитектура следующая:\n\n```{r, eval = FALSE}\nknn.reg(train = ?, test = ?, y = ?, k = ?)\n```\n\nДанные\n\n-   `train`: предикторы (тренировочные данные)\n-   `test`: предикторы на тестовых данных, $x$, по которым мы хотели бы сделать предсказания\n-   `y`: зависимая переменная (на тренировочных данных)\n-   `k`: количество \"соседей\"\n\nРезультат:\n\n-   вывод функции `knn.reg()` представляет собой $\\hat{f}_k(x)$\n\n```{r}\npred_001 = knn.reg(train = X_trn_boston, test = lstat_grid, y = y_trn_boston, k = 1)\npred_005 = knn.reg(train = X_trn_boston, test = lstat_grid, y = y_trn_boston, k = 5)\npred_010 = knn.reg(train = X_trn_boston, test = lstat_grid, y = y_trn_boston, k = 10)\npred_050 = knn.reg(train = X_trn_boston, test = lstat_grid, y = y_trn_boston, k = 50)\npred_100 = knn.reg(train = X_trn_boston, test = lstat_grid, y = y_trn_boston, k = 100)\npred_250 = knn.reg(train = X_trn_boston, test = lstat_grid, y = y_trn_boston, k = 250)\n```\n\nМы сделали предсказания на основе `lstat`, для различных значений `k`. Отметим, что `250` это общее количество наблюдений в тренировочном датасете.\n\n```{r, fig.height = 8, fig.width = 6, echo = FALSE}\npar(mfrow = c(3, 2))\n\nplot(medv ~ lstat, data = trn_boston, cex = .8, col = \"dodgerblue\", main = \"k = 1\")\nlines(lstat_grid$lstat, pred_001$pred, col = \"darkorange\", lwd = 0.25)\n\nplot(medv ~ lstat, data = trn_boston, cex = .8, col = \"dodgerblue\", main = \"k = 5\")\nlines(lstat_grid$lstat, pred_005$pred, col = \"darkorange\", lwd = 0.75)\n\nplot(medv ~ lstat, data = trn_boston, cex = .8, col = \"dodgerblue\", main = \"k = 10\")\nlines(lstat_grid$lstat, pred_010$pred, col = \"darkorange\", lwd = 1)\n\nplot(medv ~ lstat, data = trn_boston, cex = .8, col = \"dodgerblue\", main = \"k = 25\")\nlines(lstat_grid$lstat, pred_050$pred, col = \"darkorange\", lwd = 1.5)\n\nplot(medv ~ lstat, data = trn_boston, cex = .8, col = \"dodgerblue\", main = \"k = 50\")\nlines(lstat_grid$lstat, pred_100$pred, col = \"darkorange\", lwd = 2)\n\nplot(medv ~ lstat, data = trn_boston, cex = .8, col = \"dodgerblue\", main = \"k = 250\")\nlines(lstat_grid$lstat, pred_250$pred, col = \"darkorange\", lwd = 2)\n```\n\n-   Оранжевые \"кривые\" представляют собой $\\hat{f}_k(x)$ где $x$ это значения, которые мы определили через `lstat_grid`.\n\nмы видим, что `k = 1` приводит к большой переобученности, так как `k = 1` это очень комплексная, вариативная модель. В свою очередь, `k = 250` страдает недообученностью данных, так как `k = 250` это очень простой пример с маленькой дисперсией, то есть по сути, всегда будет предсказываться одно и то же значение.\n\n### Выбор параметра $k$\n\nДилемма: - низкое значение `k` = слишком сложная модель - высокое значение `k` = слишком жесткая модель.\n\nГде золотая середина?\n\n-мы хотим минимизировать $\\hat{f}_k$:\n\n$$\n\\text{EPE}\\left(Y, \\hat{f}_k(X)\\right) = \n\\mathbb{E}_{X, Y, \\mathcal{D}} \\left[  (Y - \\hat{f}_k(X))^2 \\right]\n$$\n\nПроведем тестирование на ошибку RMSE:\n\n```{r}\nrmse = function(actual, predicted) {\n  sqrt(mean((actual - predicted) ^ 2))\n}\n```\n\n```{r}\n# создадим вспомогательную функцию, чтобы \"вытащить\" предсказанные значения\nmake_knn_pred = function(k = 1, training, predicting) {\n  pred = FNN::knn.reg(train = training[\"lstat\"], \n                      test = predicting[\"lstat\"], \n                      y = training$medv, k = k)$pred\n  act  = predicting$medv\n  rmse(predicted = pred, actual = act)\n}\n```\n\n```{r}\n# определяем возможные значения k\nk = c(1, 5, 10, 25, 50, 250)\n```\n\n```{r}\n# Получаем train RMSEs\nknn_trn_rmse = sapply(k, make_knn_pred, \n                      training = trn_boston, \n                      predicting = trn_boston)\n# Получаем test RMSEs\nknn_tst_rmse = sapply(k, make_knn_pred, \n                      training = trn_boston, \n                      predicting = tst_boston)\n\n# Определяем лучшее значение k\nbest_k = k[which.min(knn_tst_rmse)]\n\n# Найдем значения для переобученности, недообученности и для лучшего случая\nfit_status = ifelse(k < best_k, \"Over\", ifelse(k == best_k, \"Best\", \"Under\"))\n```\n\n```{r}\n# Суммируем результаты\nknn_results = data.frame(\n  k,\n  round(knn_trn_rmse, 2),\n  round(knn_tst_rmse, 2),\n  fit_status\n)\ncolnames(knn_results) = c(\"k\", \"Train RMSE\", \"Test RMSE\", \"Fit?\")\n\n# Отобразим результаты\nknitr::kable(knn_results, escape = FALSE, booktabs = TRUE)\n```\n\nВопрос на засыпку: почему при k=1 ошибка на тренировочной выборке не равна 0?\n\n### Сравнение с линейной регрессией\n\nЕсли у нас линейная зависимость: - lm() работает хорошо - knn \"подгоняет автоматически\"\n\nЕсли связь независимая: - lm() работает плохо - или работает лучше при определенных условиях - knn \"делает все автоматически\"\n\n```{r echo = FALSE, fig.height = 5, fig.width = 10}\nline_reg_fun = function(x) {\n  x\n}\n\nquad_reg_fun = function(x) {\n  x ^ 2\n}\n\nsine_reg_fun = function(x) {\n  sin(x)\n}\n\nget_sim_data = function(f, sample_size = 100, sd = 1) {\n  x = runif(n = sample_size, min = -5, max = 5)\n  y = rnorm(n = sample_size, mean = f(x), sd = sd)\n  data.frame(x, y)\n}\n\nset.seed(42)\nline_data = get_sim_data(f = line_reg_fun)\nquad_data = get_sim_data(f = quad_reg_fun, sd = 2)\nsine_data = get_sim_data(f = sine_reg_fun, sd = 0.5)\n\nx_grid = data.frame(x = seq(-5, 5, by = 0.01))\n\npar(mfrow = c(1, 3))\nplot(y ~ x, data = line_data, pch = 1, col = \"darkgrey\")\ngrid()\nknn_pred = FNN::knn.reg(train = line_data$x, test = x_grid, y = line_data$y, k = 10)$pred\nfit = lm(y ~ x, data = line_data)\nlines(x_grid$x, line_reg_fun(x_grid$x), lwd = 2)\nlines(x_grid$x, knn_pred, col = \"darkorange\", lwd = 2)\nabline(fit, col = \"dodgerblue\", lwd = 2, lty = 3)\n\nplot(y ~ x, data = quad_data, pch = 1, col = \"darkgrey\")\ngrid()\nknn_pred = FNN::knn.reg(train = quad_data$x, test = x_grid, y = quad_data$y, k = 10)$pred\nfit = lm(y ~ x, data = quad_data)\nlines(x_grid$x, quad_reg_fun(x_grid$x), lwd = 2)\nlines(x_grid$x, knn_pred, col = \"darkorange\", lwd = 2)\nabline(fit, col = \"dodgerblue\", lwd = 2, lty = 3)\n\nplot(y ~ x, data = sine_data, pch = 1, col = \"darkgrey\")\ngrid()\nknn_pred = FNN::knn.reg(train = sine_data$x, test = x_grid, y = sine_data$y, k = 10)$pred\nfit = lm(y ~ x, data = sine_data)\nlines(x_grid$x, sine_reg_fun(x_grid$x), lwd = 2)\nlines(x_grid$x, knn_pred, col = \"darkorange\", lwd = 2)\nabline(fit, col = \"dodgerblue\", lwd = 2, lty = 3)\n\n```\n\nТе же шаги, но быстрее, можно осуществить с помощью библиотеки `caret`:\n\n```{r}\nmodel_knn_caret <- train(\n  medv ~ .,\n  data = trn_boston,\n  method = 'knn',\n  preProcess = c(\"center\", \"scale\"), tuneLength = 20 #этот параметр позволяет рассчитать разное количество соседей\n)\n\nmodel_knn_caret\n\n```\n\n```{r}\nknnPredict <- predict(model_knn_caret, newdata = tst_boston)\nrsq_knn_cv <- cor(knnPredict, tst_boston$medv) ^ 2\nrsq_knn_cv\n\n```\n\n## Самостоятельная работа\n\n1.  Провести регрессионный анализ данных об успеваемости студентов и определяющих их фактора.\n\nНезависимые переменные:\n\n-   Hours Studied: Общее количество часов, потраченных на учебу каждым студентом.\n-   Previous Scores - предшествующие результаты: Баллы, полученные студентами на предыдущих экзаменах.\n-   Extracurricular Activities - Внеклассная деятельность: Участвует ли студент во внеклассных мероприятиях (да или нет).\n-   Sleep Hours - Часы сна: Среднее количество часов сна студента в сутки.\n-   Sample Question Papers Practiced: Количество пробных экзаменационных работ, которые студент практиковал.\n\nЦелевая переменная:\n\n-   Performance Index: Показатель общей успеваемости каждого студента. Индекс успеваемости представляет собой академическую успеваемость студента и округляется до ближайшего целого числа. Индекс варьируется от 10 до 100, при этом более высокие значения свидетельствуют о более высокой успеваемости.\n\n{{< iconify arcticons 1dm size=42px >}}[Скачать данные](https://github.com/domelia/rcourse/blob/e53e6c681fb30693f8c0803cfa1bff9141ac50a2/Student_Performance.csv)\n\n2.  Провести анализ методом логистической регрессии на данных по климату. В качестве зависимой переменной будет выступать вопрос про оценку опасности проживания вблизи ледников (вопрос 19) , а в качестве объясняющих - пол, возраст и переменная проживания в определенном районе (type).\n","srcMarkdownNoYaml":"\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE, message = F, warning = F)\n```\n\n## Основы линейной регрессии\n\nНа предыдущих занятиях мы изучили основные типы и структуры данных, овладели основами создания простых функций на R, научились импортировать и экспортировать данные в разных форматах. Мы уже умеем проводить различные трансформации данных, визуализировать данные разного типа и проводить одномерный и двумерный анализ на категориальных данных. Мы даже справились со сложной задачей анализа данных с помощью метода главных компонент и анализа соответствий.\n\nМы продолжаем изучать многомерные методы, и следующий на очереди - регрессионный анализ и его отдельные разновидности.\n\nВ обобщенном виде уравнение регрессионного анализа может быть представлено как:\n\n$$Y=X\\beta+\\epsilon,$$\n\nгде $Y$ - вектор значений зависимой переменной, $X$ - матрица значений независимых переменных (предикторов), $\\beta$ - вектор коэффициентов регрессии, используемых для подгонки к известным значениям зависимой переменной, $\\epsilon$ - ошибки модели (остатки, разница между реальными и предсказанными значениями).\n\nПреимуществами и причинами популярности регрессионного анализа являются следующие:\n\n-   регрессионные модели могут включать множество предикторов одновременно, что позволяет оценивать вклад каждого при условии контроля над остальными параметрами;\n-   существует большое количество разновидностей регрессионного анализа, могут использоваться различные типы предикторов и зависимых переменных;\n-   довольно легко интерпретировать результаты;\n-   достаточно просты в применении и не слишком сложны с математической точки зрения.\n\nЧаще всего используются следующие виды регрессионного анализа:\n\n-   линейная регрессия (зависимая переменная числовая)\n-   логистическая регрессия (зависимая переменная бинарная)\n-   порядковая регрессия (зависимая переменная - упорядоченная факторная)\n-   мультиномиальная регрессия (зависимая переменная категориальная)\n\n### Загрузка данных\n\nВ качестве практического примера мы будем использовать учебный набор данных о взаимосвязи между объемами продаж и затратами на рекламу. Это небольшой набор, который поможет понять основные идеи метода и его различные реализации.\n\nКак всегда, мы начинаем с загрузки данных.\n\n{{< iconify arcticons 1dm size=42px >}}[Скачать данные](https://github.com/domelia/rcourse/blob/main/Advertising.csv)\n\n```{r}\nlibrary(readr)\nAdvertising = read_csv(\"Advertising.csv\")\n```\n\nПосле загрузки данных в R, первым делом нужно посмотреть сами данные, их структуру. Поскольку мы использовали функцию `read_csv()`, данные были загружены в формате tibble (tbl_df). Это практически датафрейм, вернее его усовершенствованная версия, в которой данные обрабатываются быстрее, и не происходит некоторых неприятных трансформаций (например, не меняются типы и имена данных).\n\nМы видим, что у нас всего 200 наблюдений и 4 переменных типа `double` (числовой формат, в котором происходит более точное округление десятичных знаков до 16 знаков после запятой - с 64-битной, то есть двойной точностью).\n\nВ нашем наборе переменная **Sales** (Продажи) будет являться зависимой переменной, и мы будем пытаться выявить взаимосвязи между продажами и тремя другими - независимыми переменными: TV, Radio, и Newspaper, обозначающими, соответственно, затраты на рекламу на телевидении, радио и в газетах.\n\n### Предварительная визуализация данных\n\nПосле рассмотрения структуры данных, следующий шаг - это визуализация. Поскольку у нас только количественные (не категориальные) переменные, лучший способ их представить - это диаграммы рассеяния, которые мы можем сделать для каждого индивидуального предиктора.\n\nНапример, для рекламы на телевидении\n\n```{r}\nplot(Sales ~ TV, data = Advertising, col = \"dodgerblue\", pch = 20, cex = 1.5,\n     main = \"Продажи vs Реклама на телевидении\")\n```\n\n**Самостоятельная работа**: сделайте аналогичные графики для других переменных.\n\nЧтобы сделать все графики сразу, можно воспользоваться функцией `pairs()`.\n\n```{r}\npairs(Advertising)\n```\n\nЧасто нам интересно посмотреть взаимосвязи только между зависимой переменной и предиктором, а функция `pairs()` выдает много лишнего.\n\nФункция `featurePlot()` из библиотеки `caret` [(Classification And REgression Training)](https://topepo.github.io/caret/), подходит для этой цели гораздо лучше.\n\n```{r}\nlibrary(caret)\nfeaturePlot(x = Advertising[ , c(\"TV\", \"Radio\", \"Newspaper\")], y = Advertising$Sales)\n```\n\nМы видим, что есть явный рост продаж по мере увеличения рекламы на радио и телевидении, тогда как связь с рекламой в газетах не так очевидна.\n\n### Простая линейная регрессия и функция lm()\n\nДавайте построим простую линейную модель для продаж, в которой в качестве предиктора будут выступать затраты на телевизионную рекламу.\n\n```{r}\nmod_1 = lm(Sales ~ TV, data = Advertising)\n```\n\n#### Общие результаты и тестирование гипотез\n\nФункция `summary()` позволяет вывести на экран информацию о модели, полученную с помощью функции `lm()`, которая может быть полезной для тестирования гипотез, касающихся предикторов и оценки значимости регрессионных коэффициентов.\n\n```{r}\nsummary(mod_1)\n```\n\nДавайте разбираться!\n\nВывод начинается с повторения регрессионного уравнения под заголовком `Call`: `lm(formula = Sales ~ TV, data = Advertising)`.\n\nЗатем в модели приводится распределение остатков. Остатки должны иметь нормальное распределение с абсолютными значениями минимума и максимума, также как и квартилями очень близкими друг к другу, что предполагает их примерно одинаковое расстояние от центра распределения.\n\nВ нашем случае это правило выполняется.\n\nСледующая часть вывода содержит таблицу с коэффициентами.\n\nЧтобы понять, что они означают и каким образом получаются, приведем формулу регрессии, но для случая с одной переменной:\n\n$$y_i=\\alpha+\\beta x_i+\\varepsilon_i,$$\n\nгде $y$ - зависимая переменная, $i$ - единица анализа, $\\alpha$ - интерцепт (константа), $\\beta$ - коэффициент регрессии, $x$ - независимая переменная и $\\varepsilon$ - ошибка.\n\nКогда мы работаем с выборочными данными, формула изменяется, так как вместо истинных значений у нас будут оценки:\n\n$$\\hat{y}=\\hat{\\alpha}+\\hat{\\beta}x$$\n\nОценка для интерцепта ($\\alpha$) - значение $y$ когда $x = 0$. В геометрическом смысле это точка пересечения регрессионной прямой с осью $OY$. Иногда интерцепт может иметь смысл и подлежит интерпретации, но часто он может принимать несуществующие значения, выходящие за рамки возможных значений переменных, не описывается и не интерпретируется (допустим, мы пытаемся выявить зависимость веса от роста, получается, что интерцепт нам покажет, чем равен вес, когда рост равен нулю, что не имеет смысла).\n\nВ нашем примере интерцепт - это среднее значение продаж при нулевых затратах на рекламу.\n\nДалее следуют коэффициенты для предикторов. Для каждого предиктора коэффициент обозначает ожидаемое изменение в зависимой переменной при изменении предиктора на одну единицу.\n\nГеометрический смысл beta-beкоэффициента: это угол наклона регрессионной прямой:\n\n![](https://assets.coursehero.com/study-guides/lumen/images/pima-concepts-statistics/linear-regression-3-of-4/m3_examining_relationships_topic_3_2_fit_line_algebra11.gif)\n\nЧтобы найти коэффициенты $\\alpha$ и $\\beta$ нам нужно понять, как они вычисляются. Начнем с коэффициентов $\\beta$:\n\n$$\\hat{\\beta}=\\frac{cov(x,y)}{var(x)}$$\n\nЧтобы узнать коэффициент $\\beta$ для рекламы на телевидении, нам необходимо найти ковариацию между рекламой и продажами и дисперсию затрат на рекламу:\n\n```{r}\ncov_x_y<-cov(Advertising$TV, Advertising$Sales)\nvar_x<-var(Advertising$TV)\nbeta_x<-cov_x_y/var_x\nbeta_x\n```\n\nКоэффициент показывает, на сколько увеличатся продажи, при увеличении затрат на рекламу на единицу.\n\nПосчитав коэффициент $\\beta$, мы можем перейти к $\\alpha$:\n\n$$\\hat{\\alpha}=\\bar{y} - \\hat{\\beta}\\bar{x}$$\n\nДля того, чтобы вычислить его вручную, нам необходимо знать средние значения $\\bar{y}$ и $\\bar{x}$.\n\n```{r}\ny_bar<-mean(Advertising$Sales)\nx_bar<-mean(Advertising$TV)\nintercept<-y_bar-beta_x*x_bar\nintercept\n```\n\nВсе сходится!\n\nТеперь мы можем посчитать предсказанные значения по продажам на основе нашей модели:\n\n```{r}\nyhat <- intercept + beta_x * Advertising$TV\nhead(yhat) \n```\n\nКогда мы проводим регрессионный анализ, один из главных вопросов, которые мы себе задаем - можно ли на основе знаний о переменной $x$ понять, как ведет себя $y$. Говоря формальным языком, мы исследуем эту взаимосвязь, оценивая остаточные значения, ассоциированные с коэффициентами $\\alpha$ и $\\beta$, на основе тестирования гипотез о том, отличаются ли данные коэффициенты от нуля.\n\nНапример:\n\n-   $H_0: \\beta=0$\n-   $H_1: \\beta \\neq 0$\n\nИными словами, если коэффициент $\\beta$ равен нулю, то переменная $x$ никак не объясняет $y$, так как $0 \\times x=0$.\n\nПоскольку мы основываемся на допущении о том, что остатки имеют нормальное распределение, это нам позволяет рассчитать t-статистику для коэффициентов и проверить их статистическую значимость.\n\nХотя R все делает автоматически, давайте разберемся, как это происходит.\n\nПосчитаем разницу между реальными и предсказанными значениями по продажам:\n\n```{r}\nres <- Advertising$Sales - yhat\nhead(res)\n```\n\nДалее мы должны посчитать разброс наблюдений вокруг регрессионной прямой, которую мы только что воспроизвели, а также стандартную ошибку остатков, которая используется для оценки ошибок коэффициентов и их статистической значимости.\n\nЧтобы найти стандартную ошибку остатков, нам требуется:\n\n-   найти сумму квадратов отклонений остатков ($RSS$)\n-   найти количество степеней свободы ($df=N-2$)\n\n```{r}\nres.sqr <- res^2\nRSS <- sum(res.sqr, na.rm=T)\ndf <- length(Advertising$Sales) - 2\ndf\nRSE <- sqrt(RSS / df)\nRSE\n```\n\nСобственно говоря, это мы и видим в выводе: `Residual standard error: 3.259 on 198 degrees of freedom`.\n\nЗная стандартную ошибку остатков, мы можем вычислить стандартные ошибки для наших коэффициентов. Для этого, мы должны сначала вычислить сумму квадратов отклонений по независимой переменной (затраты на рекламу ТВ):\n\n```{r}\nTSSx <- sum((Advertising$TV - x_bar)^2)\nTSSx\n```\n\nЧтобы найти стандартную ошибку коэффициента $\\beta$, нужно стандартную ошибку остатков разделить на квадратный корень из суммы квадратов отклонений по переменной $x$:\n\n$$SE_{\\beta}=\\frac{RSE}{\\sqrt{TSS_x}}$$\n\n```{r}\nSEB <- RSE / sqrt(TSSx)\nSEB\n```\n\nДля интерцепта алгоритм несколько отличается:\n\n$$SE_{\\alpha}=RSE*\\sqrt{\\frac{1}{N}+\\frac{\\bar{x^2}}{TSS_x}}$$\n\n```{r}\nSEA <- RSE * sqrt((1 / 200)+(x_bar^2 / TSSx))\nSEA\n```\n\nЗная стандартные ошибки, мы можем теперь посчитать соответствующие t-статистики, чтобы оценить, отличаются ли наши коэффициенты от нуля. Для этого нужно значения коэффициентов разделить на их стандартные ошибки.\n\nДля коэффициента по переменной телевизионной рекламы:\n\n```{r}\nt.B <- beta_x / SEB\nt.B\n```\n\nДля интерцепта:\n\n```{r}\nt.A <- intercept / SEA\nt.A\n```\n\nЭти значения измеряются в стандартных отклонениях и показывают, насколько далеко наши коэффициенты находятся от нуля. Значения 17,6 и 15,4 очень большие, следовательно, наши коэффициенты статистически значимы, что и подтверждают соответствующие p-значения из вывода: `15.36   <2e-16 ***` и `17.67   <2e-16 ***`.\n\n#### Показатели качества модели\n\nКакие еще важные показатели мы должны принять во внимание, когда мы анализируем результаты регрессионного анализа?\n\nОбратимся к оставшейся части вывода.\n\nОсновной мерой, показывающей, насколько хорошо регрессионная модель объясняет данные, является коэффициент детерминации - $R^2$. Для того, чтобы найти $R^2$, нужны следующие промежуточные вычисления о некоторых компонентах дисперсии зависимой переменной:\n\n-   сумме квадратов остатков ($RSS$)\n-   общей сумме квадратов отклонений от среднего ($TSS$)\n-   сумме квадратов отклонений, объясненной моделью ($ESS$)\n\n```{r}\nTSS <- sum((Advertising$Sales - y_bar)^2)\nTSS\nESS <- TSS - RSS\nESS\nr.sqr <- ESS / TSS\nr.sqr\n```\n\nТаким образом, 61,2% дисперсии зависимой переменной объясняется регрессионной моделью.\n\nНесмотря на то, что показатель $R^2$ является довольно информативным, у него есть один существенный недостаток: он имеет свойство неоправданно возрастать, при включении дополнительных переменных в анализ, даже если они не оказывают существенного влияния на зависимую переменную. Иными словами, чем более комплексной будет модель, тем выше будет $R^2$, что не очень хорошо.\n\nПоэтому вместо обычного $R^2$ в качестве более точной оценки качества модели используется скорректированный показатель - `adjusted`$R^2$. Проблему множественных предикторов этот показатель решает, путем внесения «наказаний» (пенальти) за включение в модель дополнительных переменных. Чтобы найти скорректированный $R^2$ используется формула:\n\n$$1-\\frac{(1-R^{2})(n-1)}{n-k-1},$$\n\nгде $k$ - количество предикторов в модели, не считая интерцепта (A).\n\n```{r}\nr.sqr.adj<-1-(((1 - r.sqr) * (200 - 1)) / (200 - 1 - 1))\nr.sqr.adj\n```\n\nПоскольку у в модели один предиктор, значение уменьшилось незначительно.\n\nУ нас остался нерассмотренным только один показатель из вывода - F-статистика. F-критерий является «глобальным» тестом, показывающим, насколько лучше наша модель базовой модели - такой, в которую включен только один интерцепт.\n\nЕще одна интерпретация: этот тест показывает, что в нашей модели есть хотя бы один значимый предиктор.\n\nВ нашем выводе `F-statistic: 312.1 on 1 and 198 DF,  p-value: < 2.2e-16`, что указывает на то, что модель с предиктором существенно лучше базовой модели объясняет зависимую переменную.\n\n#### Сравнение нескольких моделей\n\nДопустим, мы хотим создать несколько двумерных моделей и сравнить их. Это возможно с помощью функции `mtable()` из пакета `memisc` (Management of Survey Data and Presentation of Analysis Results - управление данными исследований и презентация результатов анализа). Для демонстрации создадим три модели, иллюстрирующие взаимосвязь между продажами и каждым типом рекламы.\n\nПервая модель у нас уже есть, создадим две других:\n\n```{r}\nmod_2 = lm(Sales ~ Radio, data = Advertising)\nmod_3 = lm(Sales ~ Newspaper, data = Advertising)\n```\n\nБлагодаря функции 'mtable()' мы можем создать таблицу, в которой сведем всю важную информацию по всем трем моделям:\n\n```{r}\nlibrary(memisc)\nmtable<-mtable(mod_1, mod_2, mod_3)\nmtable\n```\n\nВидим, что хотя во всех моделях интерцепты и коэффициенты предикторов являются значимыми, показатель $R^2$ максимально высок в модели, где в качестве объясняющей переменной используется показатель затрат на рекламу на телевидении.\n\n### Множественная регрессия\n\nРассмотрим случай, когда количество предикторов больше одного, то есть наша модель является моделью уже не простой, а множественной регрессии.\n\nФормула для нескольких предикторов приобретает вид:\n\n$$\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1}x_1 + \\hat{\\beta_2}x_2 ...+... \\hat{\\beta_n}x_n$$\n\nСинтаксис в R аналогичен тому, что мы использовали для двумерной регрессии. Создадим модель, в которую включим сразу все независимые переменные.\n\n```{r}\nmod_4 = lm(Sales ~ TV + Radio + Newspaper, data = Advertising)\n#mod_4 = lm(Sales ~ ., data = Advertising) можно использовать и такой синтаксис\n```\n\nКроме функции `summary()` красивую таблицу с результатами можно создать с помощью функции `tab_model()` из библиотеки `sjPlot`\n\n```{r}\nsjPlot::tab_model(mod_4)\n```\n\nРезультаты показывают, что значимыми являются только коэффициенты для радио- и телерекламы, тогда как реклама в газетах не является значимым фактором, определяющим продажи.\n\nСкорректированный коэффициент детерминации (`Adjusted R-squared`), показывает, что эта модель гораздо лучше, чем любая модель с одним предиктором, и объясняет 89,6% дисперсии.\n\nДополнительно, в целях сравнения, давайте создадим более простую модель, без газет.\n\n```{r}\nmod_5 = lm(Sales ~ TV + Radio, data = Advertising)\nsjPlot::tab_model(mod_5)\n```\n\nКак видим, коэффициент детерминации не изменился (что неудивительно, ведь у удаленной переменной коэффициент регрессии равнялся нулю).\n\n#### Сравнение моделей с помощью дисперсионного анализа\n\nЧтобы сравнить, какая модель работает лучше, можно применить функцию `anova()`, запускающую дисперсионный анализ. В нашем случае, мы будем сравнивать модель со всеми предикторами **mod_1** с сокращенной моделью **mod_0**. Наша задача будет заключаться в том, чтобы понять, какую роль играет переменная газетной рекламы в аддитивной модели.\n\n```{r}\nanova(mod_4, mod_5)\n```\n\nМы видим, что разница между моделями в одну степень свободы (1 параметр - как раз наша переменная о рекламе в газетах).\n\n::: {.alert .alert-info role=\"alert\"}\n**Число степеней свободы (df)** − важный показатель регрессионного анализа, используемый в формулах метрик, показывающих качество модели:\n\n-   Res.Df - число степеней свободы, рассчитываемых для остатков (разности между предсказанными и реальными значениями).\n-   Res.Df - количество наблюдений - количество оцениваемых параметров.\n-   Model 1: Sales \\~ TV + Radio\n-   df= 197= 200-3 (2 предиктора + константа)\n-   Model 2: Sales \\~ TV + Radio + Newspaper\n-   df= 196= 200-4 (2 предиктора + константа)\n:::\n\nРезультаты дисперсионного анализа показывают, что качество модели не поменялось, и значит мы можем использовать более лаконичную (сокращенную) модель.\n\n#### Предсказание значений зависимой переменной для новых данных\n\nОбычно у регрессионного анализа две основные задачи - **объяснение** взаимосвязи между переменными и **предсказание** новых (неизвестных) значений зависимой переменной на основе модели. Для осуществления прогноза чаще всего используется функция `predict()`, обладающая большой гибкостью (может применяться с различными методами моделирования и типами данных).\n\nЕсли эту функцию использовать к модели, созданной на основе функции `lm()`, то она будет рассчитывать предсказанные значения для каждого наблюдения.\n\nДавайте посмотрим первые десять.\n\n```{r}\nhead(predict(mod_5), n = 10)\n```\n\nОтметим, что эффект функции `predict()`будет зависеть от того, какие данные даются на входе. Наша модель относится к классу`lm`, поэтому `predict()` запускает функцию`predict.lm()` Если нам нужно что-то другое, можно посмотреть подробности с помощью `?predict.lm()`.\n\nМы также можем сгенерировать новые данные, и попробовать посчитать зависимую переменную на них.\n\nДавайте создадим новый набор с идентичными именами переменных.\n\n```{r}\nnew_obs = data.frame(TV = 150, Radio = 40, Newspaper = 1)\n```\n\nТеперь мы можем использовать `predict()`, чтобы посчитать оценки и доверительные интервалы для новых данных.\n\nЕсли указать только модель и источник данный, `R` выдаст точечную оценку, то есть \"предсказанное значение\" $\\hat{y}$.\n\n```{r}\npredict(mod_5, newdata = new_obs)\n```\n\nЕсли указать дополнительно аргумент`interval` со значением `\"confidence\"`, `R` покажет также 95% доверительные интервалы для среднего значения по данному наблюдению.\n\n```{r}\npredict(mod_1, newdata = new_obs, interval = \"confidence\")\n```\n\nКроме того, мы можем изменить уровень и выбрать не доверительные интервалы, а предсказательные интервалы (доверительные интервалы прогноза). В чем отличие?\n\nПредсказательные интервалы показывают, в каком диапазоне значений будет находиться будущее наблюдение, тогда как доверительные интервалы показывают вероятный диапазон, в котором будет находится какой-либо статистический параметр, например, среднее в генеральной совокупности.\n\nПоскольку предсказательные интервалы рассчитываются в ситуации большей неопределенности, то они обычно шире, чем доверительные интервалы.\n\n```{r}\npredict(mod_1, newdata = new_obs, interval = \"prediction\", level = 0.95)\n```\n\n#### Диагностика модели и оценка влияния наблюдений на результаты\n\nВ `R` доступны несколько функций, позволяющих оценить, насколько полученная модель хорошо воспроизводит исходные данные, и как различные наблюдения вносят вклад в предсказательные способности этой модели:\n\n-   `resid()` выдает остаток (разность между предсказанным и реальным значением)\n-   `hatvalues()` показывает `leverage` - отклонение в значениях по независимым переменным по каждому наблюдению. Данный показатель важен для понимания, как экстремальные значения по независимым переменным могут повлиять на результаты анализа.\n\n::: {callout-info}\nЧто такое `hat` - значения? `hat` - *по-английски* «шляпа», а также диакритический знак «циркумфлекс» ($\\hat{ }$), с помощью которого обозначаются значения зависимой переменной, предсказанные с помощью регрессионной модели.\n\nЭти предсказанные значения обозначаются как $\\hat{y}$ и рассчитываются по формуле:\n\n$$\\hat{y}=Xb$$\n\nДля коэффициентов линейной регрессии используется следующая формула:\n\n$$b = (X^{'}X)^{-1}X^{'}y$$\n\nСледовательно, мы можем переписать уравнение для предсказанных значений как:\n\n$$\\hat{y}=X(X^{'}X)^{-1}X^{'}y$$\n\nТаким образом, предсказанные значения могут быть получены путем умножения $n \\times 1$ вектора $y$, содержащего наблюдаемые значения на $n \\times n$ матрицы $H$:\n\n$$H=X(X^{'}X)^{-1}X^{'}$$\n\nИли, более лаконично:\n\n$$\\hat{y}=Hy$$\n\nМатрица $H$ часто называется `hat-matrix` - «матрица в шляпе», а ее диагональные значения как раз и являются значениями левериджа.\n:::\n\n-   `rstudent()` стьюдентизированные остатки по каждому наблюдению (остаток в регрессионной модели деленный на ее скорректированную стандартную ошибку)\n-   `cooks.distance()` рассчитывает важность каждого наблюдения\n\n```{r}\nddf <- data.frame(residuals=residuals(mod_5), rstandard=rstandard(mod_5), rstudent=rstudent(mod_5), leverage=hatvalues(mod_5), cookd=cooks.distance(mod_5))\n```\n\nКак мы можем это использовать?\n\nНапример, мы можем отобрать наблюдения, чьи стандартизированные остатки отклоняются более, чем на 2 стандартных отклонения в обе стороны:\n\n```{r}\nlibrary(dplyr)\nfilter(ddf, abs(rstandard) > 2 | abs(rstudent) > 2)\n```\n\n> {{< iconify arcticons brain-it-on size=42px >}} **Задание**: проанализируйте в таблице исходных данных наблюдения с указанными номерами. Какие выводы можно сделать?\n\nВторой важный момент: анализ показателей `leverage` и `Cook's distance`.\n\nЗамечательная вещь по поводу левериджа заключается в том, что его значения помогают выявить экстремальные значения $x$, которые могут влиять на результаты регрессионного анализа. Каким образом? Мы должны понять, какое значение левериджа нужно признать большим, то есть соответствующим значениям $x$, расположенным максимально далеко от средних значений по всем другим наблюдениям. Общим является правило, согласно которому, любое наблюдение, чье значение левериджа в три раза превышает среднее значение, является нетипичным / странным / достойным внимания:\n\n$$\\bar{h}=\\frac{\\sum_{i=1}^{n}h_{ii}}{n}=\\frac{p}{n}$$\n\nИными словами, если:\n\n$$h_{ii} >3\\left( \\dfrac{p}{n}\\right),$$\n\nто мы должны обратить внимание на это наблюдение. Сумма всех значений левериджа равняется количеству параметров модели: `r sum(ddf$leverage)` - два предиктора + интерцепт (константа).\n\n```{r}\nhat_max = 3*3/200\nfilter(ddf, leverage>hat_max)\n```\n\nЧто мы видим? Мы видим, что по модели 5 у нас нет таких наблюдений, чей леверидж превышал бы максимально возможное значение.\n\nМы также можем отсортировать наблюдения по расстоянию Кука, чтобы понять, какие наблюдения являются наиболее влиятельными:\n\n$$D_i=\\frac{(y_i-\\hat{y}_i)^2}{(k+1) \\times MSE}\\left[ \\frac{h_{ii}}{(1-h_{ii})^2}\\right],$$\n\nгде $MSE$ - среднеквадратическая ошибка регрессии, а $h_{ii}$ - значения левериджа.\n\n```{r}\narrange(ddf, desc(cookd))[1:6,]\n```\n\nРекомендуется исключать из анализа наблюдения, расстояние Кука для которых превышает 1. В нашем анализе таких нет, но вот наблюдения 131 и 6 являются все-таки подозрительными, как имеющие наиболее расстояние Кука и самые большие остатки.\n\nАналогичную информацию можно получить с помощью специальных графиков:\n\n```{r}\npar(mfrow = c(2, 2))\nplot(mod_5)\n```\n\nЧто показывают графики?\n\n**1. Residuals vs Fitted** (Остатки vs предсказанные значения)\n\nЭтот график показывает, есть ли в остатках регресии какие-либо нелинейные паттерны. Такое может случиться, если между предикторными переменными и зависимой переменной имеются нелинейные взаимосвязи, соответственно если эта нелинейность возникает на графике, значит модель плохо воспроизводит эти отношения. Если мы видим, что остатки равномерно распределены вокруг линии предсказанных значений без каких-либо серьезных колебаний, это хороший знак, значит у нас в модели таких нелинейных взаимосвязей нет. На нашем графике есть еле заметный «прогиб», но четким паттерном его назвать вряд ли возможно.\n\n**2. Normal Q-Q residuals**\n\nДанный график показывает, что остатки нормально распределены (то есть маленьких остатков много и их среднее значение приближается к нулю, в больших остатков мало). У нас с нормальностью остатков практически все в порядке, если не считать постоянно выбивающееся наблюдение 131.\n\n**3. Scale-Location**\n\nДанный график позволяет протестировать допущение о гомогенности дисперсии остатков (гомоскедастичности). Если мы видим, что остатки распределены вдоль линии равномерно, и их форма не напоминает «фен», то все хорошо.\n\n**4. Residuals vs Leverage**\n\nНу и, наконец, последний график визуализирует самые влиятельные наблюдения - одновременно через леверидж и расстояние Кука. Сомнительные наблюдения на всех графиках обозначены цифрами.\n\nБиблиотека `olsrr` (Tools for Building OLS Regression Models) также содержит несколько полезных функций, которые могут помочь в выявлении таких наблюдений.\n\n```{r}\nlibrary(olsrr)\nols_plot_cooksd_bar(mod_1)\n```\n\n```{r}\nols_plot_cooksd_chart(mod_1)\n```\n\n```{r}\nols_plot_dfbetas(mod_1)\n```\n\nЧто дальше? Мы выяснили, что некоторые наблюдения являются нетипичными, что может приводить к искаженным вычислениям. Но мы можем попробовать удалить переменные, которые вызвали наибольшее количество вопросов, и сравнить результаты.\n\n```{r}\nAdvertising2<-Advertising[-c(6,131),]\nmod_6 = lm(Sales ~ TV + Radio, data = Advertising2)\nmtable<-mtable(mod_5, mod_6)\nmtable\n```\n\nПосле удаления экстремальных наблюдений, качество модели улучшилось (скорректированный $R^2=91.5%$), хотя общие выводы аналогичны.\n\n#### Мультиколлинеарность\n\nЕще один сложный термин))) Что такое мультиколлинеарность? Мультиколлинеарность случается тогда, когда один предиктор может предсказывать другой. Иными словами, мы хотели бы, чтобы предикторы хорошо предсказывали поведение зависимой переменной, но не друг друга, и если такое случается, то это и называется мультиколлинеарностью. Хотя слишком высокая мультиколлинеарность является редкостью, проверка на нее является одной из стандартных процедур регрессионного анализа. Отметим, что проблема мультиколлинеарности является важной, когда мы исследуем важность предикторов, пытаемся на основе интерпретации коэффициентов регрессии обнаружить значимые закономерности (например, доказать, что повышение уровня образования может привести к значительному увеличению доходов или что по мере развития ассоциаций между гражданами увеличивается уровень институционального доверия). Если же первостепенной задачей моделирования является предсказание (как бывает во многих задачах машинного обучения), то проблема мультиколлинеарности не является релевантной, и ее можно проигнорировать.\n\nКак мы можем проверить, если в нашей модели чрезмерная мультиколлинеарность?\n\nСамый простой способ - посмотреть на коэффициенты корреляции между предикторами:\n\n```{r}\nAdvertising %>%\n  dplyr::select(TV, Radio, Newspaper) %>%\n  cor()\n```\n\nНаши независимые переменные связаны друг с другом довольно слабо. Специальной мерой, позволяющей проверить мультиколлинеарность, является $VIF$- variance inflation factor, показывающая увеличение в дисперсии коэффициентов после включения дополнительной переменной:\n\n```{r}\ncar::vif(mod_6)\n```\n\nVIF \\< 3 обозначает слабую корреляцию между переменными (идеальные условия). Чаще всего в литературе приводится пороговое значение $VIF=5$, и только переменные $VIF<5$ должны быть включены в модель.\n\nУ нас в модели с мультиколлинеарностью все в порядке.\n\n### Линейная регрессия с категориальными предикторами\n\nНапомним, что **категориальные переменные** (также известные, как качественные, или факторные переменные) - это такие переменные, которые позволяют разделить наблюдения на группы. Их особенностями является ограниченное количество значений (уровней). Типичными являются примеры с полом (два уровня - мужчины и женщины) или национальностью, социальным статусом или уровнем образования (например, лица с общим средним, средним профессиональным и высшим образованием).\n\nОбычно регрессионный анализ проводится с количественными переменными, и когда исследователь желает включить в модель категориальную переменную, необходимы некоторые шаги, чтобы сделать результаты более интерпретируемыми.\n\nВ частности, категориальные переменные перекодируются в набор так называемых «dummy» (фиктивных) переменных, в результате создается **матрица контрастов**. Современные программы, в том числе и R, «умеют» это делать автоматически.\n\n> {{< iconify arcticons anz size=42px >}} **Пример**: воспользуемся набором данных `Salaries` из пакета `car`, в котором содержатся данные о зарплате ассистентов, ассоциированных профессоров и профессоров в одном из американских колледжей (данные - за 2008-2009 учебный год). Данные были собраны администрацией для того, чтобы отслеживать различия между зарплатой, получаемой преподавателями мужчинами и женщинами,\n\nЗагрузим данные:\n\n```{r}\nlibrary(car)\ndata(\"Salaries\")\nhead(Salaries, 3)\n```\n\n#### Категориальные переменные с двумя уровнями\n\nВспомним, что в регрессионном уравнении для того, чтобы предсказать переменную $y$ на основе независимой переменной $x$, нужно суммировать все основные компоненты:\n\n$$y = b_0 + b_1*x$$\n\nПри этом:\n\n-   $b_0$ и $b_1$ являются регрессионными коэффициентами, представляющими константу (интерцепт) и угол наклона регрессионной прямой (`slope`).\n\nДопустим, мы хотим проанализировать различия в заработной плате у мужчин и женщин.\n\nНа основе переменной пола, мы можем создать новую фиктивную переменную, которая будет принимать значения:\n\n-   1 если преподаватель мужчина\n-   0 если преподаватель женщина\n\nи использовать эту переменную в регрессионном уравнении. При этом интерпретация коэффициентов и самого уравнения будет следующей:\n\n-   $b_0$ средняя зарплата у женщин,\n-   $b_0 + b_1$ средняя зарплата у мужчин,\n-   $b_1$ различия в среднем между зарплатой мужчин и женщин.\n\nСоздадим модель:\n\n```{r}\nmod_7  <- lm(salary ~ sex, data = Salaries)\nsummary(mod_7)$coef\n```\n\nИсходя из выведенной информации, средняя зарплата у преподавателей женщин - 101002 долларов (за 9 месяцев), тогда как у мужчин `101002 + 14088 = 115090`. Полученное p-значение для фиктивной переменной `sexMale` очень значимое, что указывает на то, что имеются статистические обоснования наличия различий в зарплате по полу.\n\nФункция `contrasts()`позволяет посмотреть код, который использовался для создания фиктивных переменных:\n\n```{r}\ncontrasts(Salaries$sex)\n```\n\nПри такой кодировке женщины являются референтной группой, с которой сравниваются мужчины, и в целом, любая подобная кодировка является условной, ее результаты будут влиять только на интерпретацию коэффициентов регрессии.\n\nЕсли нас такая кодировка не устраивает, мы можем использовать функцию `relevel()` для смены уровней:\n\n```{r}\nSalaries <- Salaries %>%\n  mutate(sex = relevel(sex, ref = \"Male\"))\n```\n\nПосле перекодировки результаты регрессионного анализа будут следующими:\n\n```{r}\nmod_8 <- lm(salary ~ sex, data = Salaries)\nsummary(mod_7)$coef\n```\n\nПоскольку мы теперь сравниваем зарплату женщин с зарплатой мужчин, коэффициент переменной `sexFemale` негативный, что означает более низкий уровень зарплат у женщин, по сравнению с мужчинами.\n\nКоэффициент $b_0$ равено 115090 (средняя зарплата у мужчин), тогда как коэффициент $b_1$ - -14088, показывает, на сколько, в среднем, ниже зарплата у женщин. Соответственно, 115090 - 14088 = 101002 - средняя зарплата женщин.\n\n#### Категориальная переменная с более чем двумя уровнями\n\nЧто делать, если в качественной переменной, которую мы хотим использовать, более двух уровней? Наиболее типичным является подход, когда такая категориальная переменная трансформируется в `n-1` бинарных переменных, каждая из которых имеет по два уровня. И эти `n-1` новых переменных содержат ту же информацию, что исходная переменная. В результате такой кодировки создается таблица контрастов.\n\nНапример, в нашем наборе есть переменная `rank`, которая имеет три уровня: `AsstProf`, `AssocProf` и `Prof`. Мы можем создать две фиктивных переменных - `AssocProf` и `Prof`:\n\n-   если `rank = AssocProf`, тогда в новом столбце `AssocProf` преподавателями, являющими ассоциированными профессорами, будет присвоено значение 1, а профессорам - 0.\n-   если `rank = Prof`, тогда в новом столбце `Prof` все профессора получат значение 1, а ассоциированные профессора - 0.\\\n-   что же с ассистентами? В обоих новых столбцах они получат значение 0.\n\nТакого рода кодировка в R осуществляется автоматически. С помощью функции `model.matrix()` мы можем посмотреть, как такая матрица контрастов может выглядеть:\n\n```{r}\nres <- model.matrix(~rank, data = Salaries)\nhead(res[, -1])\n```\n\nВ практике регрессионного анализа есть различные способы кодирования категориальных переменных (создания контрастов). По умолчанию в R первый уровень используется в качестве референтного, а остальные интерпретируются уже по отношению к этому уровню.\n\n::: {callout-tip}\nПример, который мы только что рассмотрели, показывает, что дисперсионный анализ - ANOVA (analyse of variance) является специальным случаем линейной модели, в которой предикторами являются категориальные переменные. И поскольку R это тоже «понимает», мы можем извлечь из модели результаты дисперсионного анализа (предпочтительнее использовать функцию `Anova()` из пакета `car` (car означает Companion to Applied Regression - компаньон для прикладных задач регрессионного анализа).\n:::\n\nСоздадим модель, в которой мы будем предсказывать зарплату от всех других переменных в наборе (знак плюс означает, что мы будем рассматривать только главные эффекты, без интеракций):\n\n```{r}\nmod_9<- lm(salary ~ yrs.service + rank + discipline + sex,\n             data = Salaries)\nAnova(mod_9)\n```\n\nПосле того, как мы приняли во внимание другие переменные (стаж - yrs.service, должность - rank, область знаний - discipline), стало понятно, что фактор пола уже не имеет значения и не вносит вклада в вариабельность заработной платы. Значимыми становятся должность и область знания.\n\nЧтобы вывести более подробные результаты анализа, лучше воспользоваться функцией `summary()`:\n\n```{r}\nsummary(mod_9)\n```\n\nРезультаты показывают, что зарплата ассоциированного профессора в среднем на 14560.40 долларов выше, чем у ассистента, при прочих равных условиях, а у профессора - выше на 49159.64 долларов. Интересно, что зарплата значительно варьирует от специализации: на прикладных кафедрах (applied departments) наблюдается в среднем на 13473.38 долее высокая зарплата, по сравнению с теоретическими дисциплинами (theoretical departments).\n\n#### Интеракции\n\nИнтеракции происходят тогда, когда эффект одного из предикторов зависит от другой переменной в модели.\n\nЧтобы продемонстрировать эффект интеракции, рассмотрим взаимосвязь между должностью и областью знаний в примере про зарплату преподавателей:\n\n$$\n\\begin{split}\ny_i &=\\beta_0 + \\beta_1*(rank) + \\beta_2*(discipline) + \\beta_3*(rank*discipline) +\\\\ & \\beta_4*(yrs.service) + \\beta_5*(sex) + \\varepsilon_i\n\\end{split}\n$$\n\n```{r}\nmod_9 <- lm(salary ~ yrs.service + sex + rank * discipline, data = Salaries)\nsummary(mod_9)\n```\n\n::: {callout-tip}\nОтметим, что хотя в формуле мы указали только интеракцию, в выводе содержатся также сведения и об индивидуальных эффектах. R включает эту информацию автоматически.\n:::\n\nИнтерпретируя результаты отметим, что эффект от взаимосвязи не значим, и единственным значимым предиктором в модели остается должность: значительная прибавка в зарплате отмечается только у профессоров, тогда как дисциплинарная принадлежность значима только на уровне статистической тенденции ($p=0,086$).\n\n#### Отбор переменных для модели\n\nПрежде чем перейти к моделированию, аналитик проводит тщательную работу по отбору переменных. Обычно, этому предшествует теоретический анализ, который позволит определить, какие показатели, важные для целевой переменной, необходимо включить в исследование, а затем - в модель.\n\nОднако, когда эксперимент уже проведен, наступает время проверки статистических гипотез. Очевидно, что не всегда все включаемые в модель параметры, в конце концов оказываются значимыми.\n\nКакие алгоритмы мы можем использовать для определения финальной, самой лучшей модели из возможных?\n\nОтбор переменных (variable selection) - это процесс выбора наиболее значимых переменных для включения в регрессионную модель. Методы отбора помогают улучшить производительность модели и избежать чрезмерной подгонки.\n\nВ рамках данного занятия мы рассмотрим следующие методы отбора:\n\n-   анализ всех возможных моделей / лучшей модели, определяемой на основе оценке качества модели\n-   пошаговые алгоритмы\n\nДля работы мы будем использовать пакет `olsrr`:\n\n```{r eval=FALSE}\ninstall.packages(\"olsrr\")\nlibrary(olsrr)\n```\n\n###### Анализ всех возможных моделей\n\nПрежде чем мы рассмотрим методы пошагового отбора, давайте вкратце рассмотрим регрессию по всем/лучшим подмножествам. Поскольку они оценивают все возможные комбинации переменных, эти методы требуют больших вычислительных затрат и могут вывести систему из строя, если использовать их с большим набором переменных.\n\nМетод `All subset regression` (все возможные варианты) представляет результаты по всем возможным комбинациям предикторов. Если у нас есть $k$ потенциальных независимых переменных, не считая константы, то количество отдельных моделей, которые потребуется проанализировать, составит - $2^k$. Например, если у нас 10 предикторов, то количество моделей - $2^10$ - `r 2^10`, а если переменных 20 - то количество комбинаций превышает миллион.\n\n```{r}\nmodel <- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)\nols_step_all_possible(model)\n```\n\n###### Подборка лучших моделей (Best Subset Regression)\n\nДанный метод позволяет отобрать модели, которые являются лучшими по обобщенным критериям модели, например, имеет наибольший $R^2$ или меньшие $MSE$ (средняя квадратичная ошибка) или $AIC$ (информационный критерий Акаике).\n\n```{r}\nmodel <- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)\nols_step_best_subset(model)\n```\n\n###### Пошаговый отбор (Stepwise Selection)\n\n**Пошаговая регрессия** - это метод подбора регрессионных моделей, который предполагает итерационный отбор независимых переменных для использования в модели. Он может быть реализован с помощью прямого отбора, обратного исключения или комбинации обоих методов.\n\nМетод **прямого отбора** начинается с модели без предикторов и постепенно добавляет каждую новую переменную, проверяя ее статистическую значимость, а метод **обратного исключения**, напротив,ю начинается с полной модели и затем по очереди удаляет наименее статистически значимые переменные.\n\n> {{< iconify arcticons anz size=42px >}} **Пример**: Для иллюстрации возможностей пошагового отбора воспользуемся данными из области недвижимости (HousingData).Набор включает данные по 506 объектам недвижимости, оцененных по 14 показателям.\n\nПеременные:\n\n-   CRIM : уровень преступности\n-   ZN : proportion of residential land zoned for lots over 25,000 sq.ft.\n-   INDUS : доля промышленных предприятий среди нежилых объектов\n-   CHAS : дамми переменная, показывает расположение относительно главной реки\n-   NOX : концентрация нитрита озота\n-   RM : среднее количество комнат\n-   AGE : доля зданий, построенных до 1940\n-   DIS : взвешенное расстояние до пяти значимых бостонских деловых центров\n-   RAD : индекс доступности хайвея\n-   TAX : налоги\n-   PTRATIO : соотношение между учителями и преподавателями (обеспеченность учителями)\n-   B : доля чернокожено населения\n-   LSTAT : доля населения с низкими доходами\n-   MEDV : медианная стоимость в 1000 долларов\n\n{{< iconify arcticons 1dm size=42px >}}[Скачать данные](https://www.kaggle.com/datasets/altavish/boston-housing-dataset/data)\n\nЗагрузим данные и создадим общую модель:\n\n```{r}\nHousingData<-read.csv(\"HousingData.csv\")\nmodel <- lm(MEDV ~ ., data = HousingData)\nsummary(model)\n```\n\nМетод ступенчатого включения (начинаем с нулевой модели и постепенно добавляем предикторы)\n\n```{r}\nols_step_forward_p(model)\n```\n\nМетод пошагового исключения:\n\n```{r}\nols_step_backward_p(model)\n```\n\nПринудительное включение в модель по имени переменной:\n\n```{r}\nols_step_forward_p(model, include = c(\"AGE\", \"LSTAT\"))\n```\n\nПринудительное включение по индексу:\n\n```{r}\nols_step_forward_p(model, include = c(5, 7))\n```\n\nВыбор на основе коэффициента детерминации:\n\n```{r}\nols_step_forward_adj_r2(model)\n```\n\nВизуализация модели:\n\n```{r}\nk <- ols_step_forward_adj_r2(model)\nplot(k)\n```\n\n###### Иерархический отбор\n\nКогда для отбора переменных используются p-значения, возможно использования иерархического отбора. Этот метод предполагает, что поиск значимых переменных ограничен следующей переменной. Если какая-то переменная не отбирается по причине не подходящего p-значения, то и ни одна последующая переменная не рассматривается для включения.\n\n```{r}\nols_step_forward_p(model, 0.1, hierarchical = TRUE)\n```\n\nПошаговая регрессия может оказаться хорошей идеей, особенно, когда количество предикторов велико и нужно отобрать только самые значимые. Между тем, исследователи отмечают большое количество «подводных камней» и статистических проблем, которые могут возникнут в процессе применения регрессионного анализа, таких как переобученность данных, смещенные оценки, ошибки I рода (Harrell, 2015). Кроме того, в процессе применения пошаговых методов возникает опасная иллюзия итого, что компьютер автоматически отбирает правильные переменные, на самом деле это происходит без связи с теоретическими основаниями и гипотезами исследования. Более того, модель, которая была отобрана на основе какого-то критерия, на самом деле может оказаться нестабильной и малоинформативной. Во многих случаях правильным было бы опираться на теорию и предыдущие исследования, тогда как методы отбора могут рассматриваться в качестве поисковых техник.\n\n## Логистическая регрессия\n\nЛогистическая регрессия применяется в том случае, если наша зависимая перменная имеет вид 0-1, то есть является дихотомической и имеет значения 1 и 0. По сути, такой регрессионный анализ решает задачу классификации, то есть определения принадлежности к одному из двух классов (\"победит\" или \"проиграет\", примут на работу или нет и т.д.).\n\n> {{< iconify arcticons anz size=42px >}} **Пример**: В качестве примера, мы будем рассматривать данные о приеме в высшие учебные заведения. В частности, нас будет интересовать, как результаты выпускных экзаменов GRE (Graduate Record Exam scores) и средние оценки GPA (grade point average), а также престиж учебного заведения связаня с допуском в высшее учебное заведение. Зависимой является переменная admit/don’t admit, которая уже закодирована в формате 0-1.\n\n```{r}\ndata <- read.csv(\"https://stats.idre.ucla.edu/stat/data/binary.csv\")\nhead(data)\n\n```\n\nМодель логистической регрессии имеет вид:\n\n$$\n\\log\\left(\\frac{p(x)}{1 - p(x)}\\right) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots  + \\beta_p x_p.\n$$\n\nОткуда, путем перестановки, мы можем вывести вероятность принадлежности к группе 1:\n\n$$\np(x) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots  + \\beta_p x_p)}} = \\sigma(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots  + \\beta_p x_p)\n$$\n\nОбычно основное уравнение представляется в виде сигмоиды (логистической функции):\n\n$$\n\\sigma(x) = \\frac{e^x}{1 + e^x} = \\frac{1}{1 + e^{-x}}\n$$\n\n![](https://static.javatpoint.com/tutorial/machine-learning/images/linear-regression-vs-logistic-regression.png)\n\nПодгонка модели осуществляется путем максимизации функции правдоподобия, что практически никогда не происходит вручную, и мы предоставим возможность это сделать`R`\n\nНачнем с того, что переведем ранг заведения в факторную переменную:\n\n```{r}\ndata$rank <- factor(data$rank)\n```\n\n```{r}\nmodel_glm <- glm(admit ~ gre + gpa + rank, data = data, family = \"binomial\")\n```\n\nРезультаты логистической регрессии очень похожи на то, что мы видели в линейной регрессии, только вместо `lm()` мы используем `glm()`. Другая особенность - в атрибуте `family = \"binomial\"`, что означает, что у нас будет зависимая переменная, состоящая из двух классов. Если использовать `glm()` с `family = \"gaussian\"` то получится обычная линейная регрессия.\n\nДавайте посмотрим на общие результаты\n\n```{r}\nsummary(model_glm)\n```\n\nВывод в целом напоминает то, что мы видели в линейной регрессии. Видим, что все наши зависимые переменные являются значимыми. Переменные ранга имеют отрицательные коэффициенты, так как сравниваются со значением 1 - группа учебных заведений с наиболее высокими позициями.\n\nОднако, стоит помнить, что коэффициенты в логистической регрессии не простые, они представляют собой логарифм шансов. Что это значит?\n\nДавайте посмотрим на таблицу с нашей зависимой переменной:\n\n```{r}\ntable(data$admit)\n```\n\nВсего допущено 127 человек из 400, то есть вероятность допуска составит: 127/400=0.3175, а отношение шансов (допуска к недопуску): 0.3175/(1-0.1375)=0.3681159. Логарифм данного выражения составит:\n\n```{r}\nlog(0.3175/(1-0.3175))\n```\n\nЭто именно то, что мы бы получили, если бы создали модель только с одним интерцептом:\n\n```{r}\nmodel_null<-glm(admit ~ 1, data = data, family = \"binomial\")\nsummary(model_null)\n```\n\nВернемся к коэффициентам нашей большой модели:\n\n-   изменения в одну единицу по переменной `gre`, логарифм шансов допуска увеличится на 0.002.\n-   изменение на одну единицу в `gpa`, логарифм шансов допуска увеличится на 0.804.\n\nИндикаторные переменные для ранга имеют слегка иную интерпретацию. Например, посещая школу, входящую во вторую группу по престижности, по сравнению с группой 1 изменяет логарифм шансов на -0.675.\n\nВнизу таблицы с коэффициентами располагаются индексы подгонки (AIC).\n\nЧтобы перевести коэффициенты в обычное отношение шансов, применяется экспоненциальная функция. Можно соединить это действие с вычислением доверительных интервалов:\n\n```{r}\nexp(cbind(OR = coef(model_glm), confint(model_glm)))\n```\n\nКак интерпретировать отношение шансов?\n\nДля количественных переменных меняется мало что:\n\n-   изменение на одну единицу gre на 0,2% увеличивает шансы быть принятыми\n-   изменение среднего балла на единицу увеличивает шансы на 123%\n-   а вот учеба в школе ранга 2 снижает шансы на 50%, ранга 3 - на 73.8%, ранга 4 - на 78,8%.\n\n::: {callout-tip}\nОбщая формула перевода отношения шансов в проценты: (OR-1) \\* 100\n:::\n\nСледующий этап - посмотреть, как работает функция `predict()` вместе с `glm()`:\n\n```{r}\nhead(predict(model_glm))\n```\n\nПо умолчанию `predict.glm()` использует `type = \"link\"`.\n\n```{r}\nhead(predict(model_glm, type = \"link\"))\n```\n\nЭто означает, что `R` возвращает:\n\n$$\n\\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 + \\cdots  + \\hat{\\beta}_p x_p\n$$ для каждого наблюдения.\n\nВажно понимать, что это **не** предсказанные вероятности, и для того, чтобы их получить:\n\n$$\n\\hat{p}(x) = \\hat{P}(Y = 1 \\mid X = x)\n$$\n\nмы должны написать `type = \"response\"`\n\n```{r}\nhead(predict(model_glm, type = \"response\"))\n```\n\nСоответственно, это вероятности, но **не** результаты классификации. Для того, чтобы их получить, мы должны сравнить вероятности с пороговым значением.\n\n```{r}\nmodel_glm_pred = ifelse(predict(model_glm, type = \"response\") > 0.5, 1, 0)\nhead(model_glm_pred)\n```\n\nЧто мы сделали?\n\n$$\n\\hat{C}(x) = \n\\begin{cases} \n      1 & \\hat{f}(x) > 0 \\\\\n      0 & \\hat{f}(x) \\leq 0 \n\\end{cases}\n$$\n\nгде\n\n$$\n\\hat{f}(x) =\\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 + \\cdots  + \\hat{\\beta}_p x_p.\n$$\n\nТот код, который мы запустили, делает следующее:\n\n$$\n\\hat{C}(x) = \n\\begin{cases} \n      1 & \\hat{p}(x) > 0.5 \\\\\n      0 & \\hat{p}(x) \\leq 0.5 \n\\end{cases}\n$$\n\nгде\n\n$$\n\\hat{p}(x) = \\hat{P}(Y = 1 \\mid X = x).\n$$\n\nПосчитав классификации, мы можем также посчитать метрики для ошибок.\n\n```{r, message = FALSE, warning = FALSE}\ntab = table(predicted = model_glm_pred, actual = data$admit)\ntab\nlibrary(caret)\nconfusionMatrix = confusionMatrix(tab, positive = \"1\")\nc(confusionMatrix$overall[\"Accuracy\"], \n  confusionMatrix$byClass[\"Sensitivity\"], \n  confusionMatrix$byClass[\"Specificity\"])\n```\n\n![](https://i.stack.imgur.com/NzSnD.jpg)\n\nМы можем также предсказать результаты допуска в вуз для новых данных.\n\nПопробуем их сгенерировать на основе исходных данных:\n\n```{r}\nnewdata1 <- with(data, data.frame(gre = mean(gre), gpa = mean(gpa), rank = factor(1:4)))\n```\n\nПредскажем результаты зачисления:\n\n```{r}\nnewdata1$rankP <- predict(model_glm, newdata = newdata1, type = \"response\")\nnewdata1\n```\n\nВидим, что при средних оценках, учащиеся, обучавшиеся в престижных школах имеют большую вероятность поступить, чем те, кто училися не в очень престижных заведениях.\n\nМы также можем захотеть узнать, насколько хорошо наша модель соответствует действительности. Это может быть особенно полезно при сравнении конкурирующих моделей.\n\nАналогом $R^2$ для логистической регрессии является $R^2 Макфаддена$:\n\n```{r}\nwith(summary(model_glm), 1 - deviance/null.deviance)\n```\n\nЕсли не хочется вычислять вручную, можно воспользоваться готовой функцией `pR2`:\n\n```{r message=FALSE, warning=FALSE}\n#install.packages('pscl')\nlibrary(pscl)\n\npR2(model_glm)['McFadden']\n\n```\n\nЕще одной популярной псевдомерой является $R^2 Nagelkerke$:\n\n```{r message=FALSE, warning=FALSE}\n#install.packages('fmsb')\nlibrary(fmsb)\n\nNagelkerkeR2(model_glm)\n```\n\n## Мультиномиальная логистическая регрессия\n\nЧто делать, если наша зависимая переменная имеет не две, а более категорий? Для этого случая больше подходит мультиномиальная логистическая регрессия.\n\n$$\nP(Y = k \\mid { X = x}) = \\frac{e^{\\beta_{0k} + \\beta_{1k} x_1 + \\cdots +  + \\beta_{pk} x_p}}{\\sum_{g = 1}^{G} e^{\\beta_{0g} + \\beta_{1g} x_1 + \\cdots + \\beta_{pg} x_p}}\n$$\n\nМы не будем погружаться в технические детали, но попробуем реализовать этот подход на практике. мы воспользуемся знакомым нам набором данных `iris`.\n\nЧтобы выполнить мультиномиальный регрессионный анализ нам потребуется функция `multinom` из библиотеки `nnet`, где используется синтаксис, похожий на `lm()` и `glm()`. Лучше добавить `trace = FALSE`, чтобы не выводилась информация об оптимизационных процессах во время обучения.\n\n```{r}\nlibrary(nnet)\nmodel_multi = multinom(Species ~ ., data = iris)\nsummary(model_multi)\n```\n\nЗаметим, что на выходе у нас коэффициенты только для двух классов, так же как и в обычной регрессии у нас есть только коэффициент для одного класса.\n\n## Непараметрическая регрессия. Метод k-ближайших соседей\n\nВсе методы, которые мы рассматривали до этого момента, являются параметрическими. Это можно представить в виде обобщающей формулы.\n\n$$\nf(x) = \\mathbb{E}[Y \\mid X = x]\n$$\n\nНапример, типичная форма для множественной линейной регрессии:\n\n$$\nf(x) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_p x_p\n$$\n\nЗадача аналитика в этом случае заключается в оценке параметров модели и предсказания на их основе.\n\nНепараметрические методы основываются на самих данных, а не на параметрах. В этом случае используется понятие локальности.\n\nРассуждения при этом примерно такие: чему будет равняться y, если x равен...?\n\n$$\n\\hat{f}(x) = \\text{average}(\\{ y_i : x_i = x \\})\n$$\n\nПоскольку не всегда это требование выполняется, то условия чуть-чуть меняются:\n\n$$\n\\hat{f}(x) = \\text{average}( \\{ y_i : x_i \\text{ equal to (or very close to) x} \\} )\n$$\n\nОдним из конкретных примером использования непараметрического подхода является метод ближайших соседей:\n\n$$\n\\hat{f}_k(x) = \\frac{1}{k} \\sum_{i \\in \\mathcal{N}_k(x, \\mathcal{D})} y_i\n$$\n\n![](https://bookdown.org/f100441618/bookdown-regresion/www/KNN.jpg)\n\n### KNN в `R`\n\nПосмотрим, как работает этот метод на данных набора `HousingData`:\n\n```{r}\nlibrary(FNN)\nlibrary(MASS)\ndata(Boston)\n```\n\nСоздаем тренировочную и тестируемые выборки:\n\n```{r}\nset.seed(42)\nboston_idx = sample(1:nrow(Boston), size = 250)\ntrn_boston = Boston[boston_idx, ]\ntst_boston  = Boston[-boston_idx, ]\n```\n\n```{r}\nX_trn_boston = trn_boston[\"lstat\"]\nX_tst_boston = tst_boston[\"lstat\"]\ny_trn_boston = trn_boston[\"medv\"]\ny_tst_boston = tst_boston[\"medv\"]\n```\n\nСоздадим дополнительный набор для переменной `lstat` по которым мы будем предсказывать `medv` для создания графики.\n\n```{r}\nX_trn_boston_min = min(X_trn_boston)\nX_trn_boston_max = max(X_trn_boston)\nlstat_grid = data.frame(lstat = seq(X_trn_boston_min, X_trn_boston_max, \n                                    by = 0.01))\n```\n\nЧтобы применить метод KNN в качестве разновидности регрессионного анализа, нам понадобится функция `knn.reg()` из библиотеки `FNN`.\n\nЕе общая архитектура следующая:\n\n```{r, eval = FALSE}\nknn.reg(train = ?, test = ?, y = ?, k = ?)\n```\n\nДанные\n\n-   `train`: предикторы (тренировочные данные)\n-   `test`: предикторы на тестовых данных, $x$, по которым мы хотели бы сделать предсказания\n-   `y`: зависимая переменная (на тренировочных данных)\n-   `k`: количество \"соседей\"\n\nРезультат:\n\n-   вывод функции `knn.reg()` представляет собой $\\hat{f}_k(x)$\n\n```{r}\npred_001 = knn.reg(train = X_trn_boston, test = lstat_grid, y = y_trn_boston, k = 1)\npred_005 = knn.reg(train = X_trn_boston, test = lstat_grid, y = y_trn_boston, k = 5)\npred_010 = knn.reg(train = X_trn_boston, test = lstat_grid, y = y_trn_boston, k = 10)\npred_050 = knn.reg(train = X_trn_boston, test = lstat_grid, y = y_trn_boston, k = 50)\npred_100 = knn.reg(train = X_trn_boston, test = lstat_grid, y = y_trn_boston, k = 100)\npred_250 = knn.reg(train = X_trn_boston, test = lstat_grid, y = y_trn_boston, k = 250)\n```\n\nМы сделали предсказания на основе `lstat`, для различных значений `k`. Отметим, что `250` это общее количество наблюдений в тренировочном датасете.\n\n```{r, fig.height = 8, fig.width = 6, echo = FALSE}\npar(mfrow = c(3, 2))\n\nplot(medv ~ lstat, data = trn_boston, cex = .8, col = \"dodgerblue\", main = \"k = 1\")\nlines(lstat_grid$lstat, pred_001$pred, col = \"darkorange\", lwd = 0.25)\n\nplot(medv ~ lstat, data = trn_boston, cex = .8, col = \"dodgerblue\", main = \"k = 5\")\nlines(lstat_grid$lstat, pred_005$pred, col = \"darkorange\", lwd = 0.75)\n\nplot(medv ~ lstat, data = trn_boston, cex = .8, col = \"dodgerblue\", main = \"k = 10\")\nlines(lstat_grid$lstat, pred_010$pred, col = \"darkorange\", lwd = 1)\n\nplot(medv ~ lstat, data = trn_boston, cex = .8, col = \"dodgerblue\", main = \"k = 25\")\nlines(lstat_grid$lstat, pred_050$pred, col = \"darkorange\", lwd = 1.5)\n\nplot(medv ~ lstat, data = trn_boston, cex = .8, col = \"dodgerblue\", main = \"k = 50\")\nlines(lstat_grid$lstat, pred_100$pred, col = \"darkorange\", lwd = 2)\n\nplot(medv ~ lstat, data = trn_boston, cex = .8, col = \"dodgerblue\", main = \"k = 250\")\nlines(lstat_grid$lstat, pred_250$pred, col = \"darkorange\", lwd = 2)\n```\n\n-   Оранжевые \"кривые\" представляют собой $\\hat{f}_k(x)$ где $x$ это значения, которые мы определили через `lstat_grid`.\n\nмы видим, что `k = 1` приводит к большой переобученности, так как `k = 1` это очень комплексная, вариативная модель. В свою очередь, `k = 250` страдает недообученностью данных, так как `k = 250` это очень простой пример с маленькой дисперсией, то есть по сути, всегда будет предсказываться одно и то же значение.\n\n### Выбор параметра $k$\n\nДилемма: - низкое значение `k` = слишком сложная модель - высокое значение `k` = слишком жесткая модель.\n\nГде золотая середина?\n\n-мы хотим минимизировать $\\hat{f}_k$:\n\n$$\n\\text{EPE}\\left(Y, \\hat{f}_k(X)\\right) = \n\\mathbb{E}_{X, Y, \\mathcal{D}} \\left[  (Y - \\hat{f}_k(X))^2 \\right]\n$$\n\nПроведем тестирование на ошибку RMSE:\n\n```{r}\nrmse = function(actual, predicted) {\n  sqrt(mean((actual - predicted) ^ 2))\n}\n```\n\n```{r}\n# создадим вспомогательную функцию, чтобы \"вытащить\" предсказанные значения\nmake_knn_pred = function(k = 1, training, predicting) {\n  pred = FNN::knn.reg(train = training[\"lstat\"], \n                      test = predicting[\"lstat\"], \n                      y = training$medv, k = k)$pred\n  act  = predicting$medv\n  rmse(predicted = pred, actual = act)\n}\n```\n\n```{r}\n# определяем возможные значения k\nk = c(1, 5, 10, 25, 50, 250)\n```\n\n```{r}\n# Получаем train RMSEs\nknn_trn_rmse = sapply(k, make_knn_pred, \n                      training = trn_boston, \n                      predicting = trn_boston)\n# Получаем test RMSEs\nknn_tst_rmse = sapply(k, make_knn_pred, \n                      training = trn_boston, \n                      predicting = tst_boston)\n\n# Определяем лучшее значение k\nbest_k = k[which.min(knn_tst_rmse)]\n\n# Найдем значения для переобученности, недообученности и для лучшего случая\nfit_status = ifelse(k < best_k, \"Over\", ifelse(k == best_k, \"Best\", \"Under\"))\n```\n\n```{r}\n# Суммируем результаты\nknn_results = data.frame(\n  k,\n  round(knn_trn_rmse, 2),\n  round(knn_tst_rmse, 2),\n  fit_status\n)\ncolnames(knn_results) = c(\"k\", \"Train RMSE\", \"Test RMSE\", \"Fit?\")\n\n# Отобразим результаты\nknitr::kable(knn_results, escape = FALSE, booktabs = TRUE)\n```\n\nВопрос на засыпку: почему при k=1 ошибка на тренировочной выборке не равна 0?\n\n### Сравнение с линейной регрессией\n\nЕсли у нас линейная зависимость: - lm() работает хорошо - knn \"подгоняет автоматически\"\n\nЕсли связь независимая: - lm() работает плохо - или работает лучше при определенных условиях - knn \"делает все автоматически\"\n\n```{r echo = FALSE, fig.height = 5, fig.width = 10}\nline_reg_fun = function(x) {\n  x\n}\n\nquad_reg_fun = function(x) {\n  x ^ 2\n}\n\nsine_reg_fun = function(x) {\n  sin(x)\n}\n\nget_sim_data = function(f, sample_size = 100, sd = 1) {\n  x = runif(n = sample_size, min = -5, max = 5)\n  y = rnorm(n = sample_size, mean = f(x), sd = sd)\n  data.frame(x, y)\n}\n\nset.seed(42)\nline_data = get_sim_data(f = line_reg_fun)\nquad_data = get_sim_data(f = quad_reg_fun, sd = 2)\nsine_data = get_sim_data(f = sine_reg_fun, sd = 0.5)\n\nx_grid = data.frame(x = seq(-5, 5, by = 0.01))\n\npar(mfrow = c(1, 3))\nplot(y ~ x, data = line_data, pch = 1, col = \"darkgrey\")\ngrid()\nknn_pred = FNN::knn.reg(train = line_data$x, test = x_grid, y = line_data$y, k = 10)$pred\nfit = lm(y ~ x, data = line_data)\nlines(x_grid$x, line_reg_fun(x_grid$x), lwd = 2)\nlines(x_grid$x, knn_pred, col = \"darkorange\", lwd = 2)\nabline(fit, col = \"dodgerblue\", lwd = 2, lty = 3)\n\nplot(y ~ x, data = quad_data, pch = 1, col = \"darkgrey\")\ngrid()\nknn_pred = FNN::knn.reg(train = quad_data$x, test = x_grid, y = quad_data$y, k = 10)$pred\nfit = lm(y ~ x, data = quad_data)\nlines(x_grid$x, quad_reg_fun(x_grid$x), lwd = 2)\nlines(x_grid$x, knn_pred, col = \"darkorange\", lwd = 2)\nabline(fit, col = \"dodgerblue\", lwd = 2, lty = 3)\n\nplot(y ~ x, data = sine_data, pch = 1, col = \"darkgrey\")\ngrid()\nknn_pred = FNN::knn.reg(train = sine_data$x, test = x_grid, y = sine_data$y, k = 10)$pred\nfit = lm(y ~ x, data = sine_data)\nlines(x_grid$x, sine_reg_fun(x_grid$x), lwd = 2)\nlines(x_grid$x, knn_pred, col = \"darkorange\", lwd = 2)\nabline(fit, col = \"dodgerblue\", lwd = 2, lty = 3)\n\n```\n\nТе же шаги, но быстрее, можно осуществить с помощью библиотеки `caret`:\n\n```{r}\nmodel_knn_caret <- train(\n  medv ~ .,\n  data = trn_boston,\n  method = 'knn',\n  preProcess = c(\"center\", \"scale\"), tuneLength = 20 #этот параметр позволяет рассчитать разное количество соседей\n)\n\nmodel_knn_caret\n\n```\n\n```{r}\nknnPredict <- predict(model_knn_caret, newdata = tst_boston)\nrsq_knn_cv <- cor(knnPredict, tst_boston$medv) ^ 2\nrsq_knn_cv\n\n```\n\n## Самостоятельная работа\n\n1.  Провести регрессионный анализ данных об успеваемости студентов и определяющих их фактора.\n\nНезависимые переменные:\n\n-   Hours Studied: Общее количество часов, потраченных на учебу каждым студентом.\n-   Previous Scores - предшествующие результаты: Баллы, полученные студентами на предыдущих экзаменах.\n-   Extracurricular Activities - Внеклассная деятельность: Участвует ли студент во внеклассных мероприятиях (да или нет).\n-   Sleep Hours - Часы сна: Среднее количество часов сна студента в сутки.\n-   Sample Question Papers Practiced: Количество пробных экзаменационных работ, которые студент практиковал.\n\nЦелевая переменная:\n\n-   Performance Index: Показатель общей успеваемости каждого студента. Индекс успеваемости представляет собой академическую успеваемость студента и округляется до ближайшего целого числа. Индекс варьируется от 10 до 100, при этом более высокие значения свидетельствуют о более высокой успеваемости.\n\n{{< iconify arcticons 1dm size=42px >}}[Скачать данные](https://github.com/domelia/rcourse/blob/e53e6c681fb30693f8c0803cfa1bff9141ac50a2/Student_Performance.csv)\n\n2.  Провести анализ методом логистической регрессии на данных по климату. В качестве зависимой переменной будет выступать вопрос про оценку опасности проживания вблизи ледников (вопрос 19) , а в качестве объясняющих - пол, возраст и переменная проживания в определенном районе (type).\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"wrap","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["include/webex.css"],"include-after-body":["include/webex.js"],"embed-resources":false,"output-file":"Regression.html"},"language":{"toc-title-document":"Содержание","toc-title-website":"Содержание","related-formats-title":"Другие форматы","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Источник","other-links-title":"Другие ссылки","code-links-title":"Ссылки на код","launch-dev-container-title":"Запустить Dev Container","launch-binder-title":"Запустить Binder","article-notebook-label":"Блокнот статьи","notebook-preview-download":"Скачать блокнот","notebook-preview-download-src":"Скачать исходный код","notebook-preview-back":"Вернуться к статье","manuscript-meca-bundle":"Архив MECA","section-title-abstract":"Аннотация","section-title-appendices":"Приложения","section-title-footnotes":"Сноски","section-title-references":"использованная литература","section-title-reuse":"Повторное использование","section-title-copyright":"Авторские права","section-title-citation":"Цитата","appendix-attribution-cite-as":"Пожалуйста, цитируйте эту работу как:","appendix-attribution-bibtex":"BibTeX","appendix-view-license":"Просмотреть Лицензию","title-block-author-single":"Автор","title-block-author-plural":"Авторы","title-block-affiliation-single":"принадлежность","title-block-affiliation-plural":"Принадлежности","title-block-published":"Дата публикации","title-block-modified":"Файл изменен","title-block-keywords":"Ключевые слова","callout-tip-title":"Совет","callout-note-title":"Уведомление","callout-warning-title":"Предупреждение","callout-important-title":"Важное уведомление","callout-caution-title":"Осторожность","code-summary":"Код","code-tools-menu-caption":"Код","code-tools-show-all-code":"Развернуть код","code-tools-hide-all-code":"Скрыть код","code-tools-view-source":"Показать код","code-tools-source-code":"Исходный код","tools-share":"Share","tools-download":"Download","code-line":"Линия","code-lines":"Линии","copy-button-tooltip":"Скопировать текст","copy-button-tooltip-success":"Скопировано","repo-action-links-edit":"Редактировать страницу","repo-action-links-source":"Показать код","repo-action-links-issue":"Сообщить о проблеме","back-to-top":"Наверх","search-no-results-text":"Поиск не дал результатов","search-matching-documents-text":"Результаты поиска","search-copy-link-title":"Скопировать ссылку","search-hide-matches-text":"Скрыть дополнительные результаты","search-more-match-text":"дополнительный результат в этом документе","search-more-matches-text":"дополнительных результата(-ов) в этом документе","search-clear-button-title":"Очистить","search-text-placeholder":"","search-detached-cancel-button-title":"Отменить","search-submit-button-title":"Найти","search-label":"Поиск","toggle-section":"Переключить раздел","toggle-sidebar":"Переключить боковую панель навигации","toggle-dark-mode":"Переключить темный режим","toggle-reader-mode":"Переключить режим чтения","toggle-navigation":"Переключить навигацию","crossref-fig-title":"Рисунок","crossref-tbl-title":"Таблица","crossref-lst-title":"Список","crossref-thm-title":"Теорема","crossref-lem-title":"Лемма","crossref-cor-title":"Следствие","crossref-prp-title":"Утверждение","crossref-cnj-title":"Гипотеза","crossref-def-title":"Определение","crossref-exm-title":"Пример","crossref-exr-title":"Упражнение","crossref-ch-prefix":"Глава","crossref-apx-prefix":"Приложение","crossref-sec-prefix":"Глава","crossref-eq-prefix":"Уравнение","crossref-lof-title":"Список Иллюстраций","crossref-lot-title":"Список Таблиц","crossref-lol-title":"Список Каталогов","environment-proof-title":"Доказательство","environment-remark-title":"Примечание","environment-solution-title":"Решение","listing-page-order-by":"Сортировать по","listing-page-order-by-default":"предварительно выбранный","listing-page-order-by-date-asc":"Самый старый","listing-page-order-by-date-desc":"Новейшие","listing-page-order-by-number-desc":"нисходящий","listing-page-order-by-number-asc":"по возрастанию","listing-page-field-date":"Дата","listing-page-field-title":"Заголовок","listing-page-field-description":"Описание","listing-page-field-author":"Автор","listing-page-field-filename":"Имя файла","listing-page-field-filemodified":"Файл изменен","listing-page-field-subtitle":"Подзаголовок","listing-page-field-readingtime":"Время чтения","listing-page-field-wordcount":"Подсчет слов","listing-page-field-categories":"Категории","listing-page-minutes-compact":"{0} минут","listing-page-category-all":"Все","listing-page-no-matches":"Нет подходящих элементов","listing-page-words":"{0} слов","listing-page-filter":"Фильтр","draft":"Черновик"},"metadata":{"lang":"ru","fig-responsive":true,"quarto-version":"1.5.57","comments":{"hypothesis":true},"bibliography":["references.bib"],"editor":"visual","theme":"Pulse","title":"Регрессионный анализ в R"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}