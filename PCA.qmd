---
title: "Основы многомерного анализа: снижение размерности"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Общие цели занятия и возможности библиотеки`factoextra`

**Многомерный анализ данных** представляет собой сложную, но очень интересную задачу, которая часто встречается в деятельности аналитика данных. Задачи анализа взаимосвязей между переменными, их структурирование в виде кластеров, факторов, компонент и представление в визуально интерпретируемом виде часто встречаются, как в фундаментальных, так и прикладных исследованиях.

Многомерный анализ основан на принципе многомерной статистики, который предполагает наблюдение и анализ более чем одной переменной за один раз. Обычно многомерный анализ используется для решения ситуаций, когда в ходе научного эксперимента выполняются несколько измерений, и взаимосвязь между этими измерениями и их структурами имеет важное значение. 

Многомерный анализ охватывает большой репертуар методов, включая факторный анализ, кластерный анализ, регрессионный анализ, метод главных компонент и другие, поэтому эта обширная тема, в которой мы изучим только некоторые направления анализа.

Многомерный анализ включает в себя:

- модели, основанные на многомерном нормальном распределении и генерализованные модели, а также распределений
- измерение и анализ взаимосвязей между отдельными переменными
- расчеты вероятностей на многомерных данных
- Изучение структур и паттернов в данных

Для подобных целей на языке R разработано множество библиотек, но определенными преимуществами обладает одна из них - `factoextra`. Ее цель заключается в извлечении результатов и визуализации **эксплораторного** многомерного анализа данных, включая:

**Метод главных компонент - Principal Component Analysis (PCA)** - анализирующий количественные (непрерывные) переменные путем сокращения размерности данных без потери важной информации (например, представление множества шкал психологических тестов в виде нескольких компонент или представление множества показателей социально-экономического развития регионов в виде нескольких интегральных показателей).

**Корреспондентский анализ (анализ соответствий) - Correspondence Analysis (CA)**, выступающий расширением метода главных компонент для анализа больших таблиц сопряженности, образуемых на основе двух качественных переменных (категориальных данных)(например, продукты и их характеристики, метод хорошо подходит для вопросов с множественным выбором).

**Множественный корреспондентский анализ, множественный анализ соответствий - Multiple Correspondence Analysis (MCA)**, являющийся адаптацией метода CA к таблице данных, содержащих более двух категориальных переменных (например, взаимосвязь между занятостью, национальностью и полом).

**Множественный факторный анализ - Multiple Factor Analysis (MFA)** - предназначен для работы с данными, где переменные организованы в группы, блоки или наборы (качественных и / или количественных) переменныех. MFA позволяет обнаружить общие структуры, присутствующие во всех или некоторых наборах данных. Выполняется в два этапа. На первом этапе на каждом наборе данных проводится анализ главных компонент, а затем исходные данные в каждом наборе «нормализуются» путем деления каждого элемента на квадратный корень из собственного значения, полученного из МГК на данном наборе. Затем, нормализованные данные объединяются в единую матрицу, над которой проводится общий анализ главных компонент, а индивидуальные наборы данных проектируются на результаты этого анализа, чтобы найти общие зависимости и расхождения [@abdi2007multiple]

**Иерархический множественный факторный анализ - Hierarchical Multiple Factor Analysis (HMFA)**:  расширение метода MFA, подходящий для ситуации, где данные организованы в иерархические структуры.

**Факторный анализ смешанных данных - Factor Analysis of Mixed Data (FAMD)** - особый случай MFA, предназначенный для анализа данных, содержащих как количественные, так и качественные переменные.

<div class="alert alert-info" role="alert">
**Важно!** Сам анализ проводится с помощью других библиотек, библиотека factoextra используется для извлечения результатов и визуализации.
</div>


Существует большое количество пакетов, в которых можно сделать анализ главных компонент, например, `FactoMineR`, `ade4`, `stats`, `ca`, `MASS` и `ExPosition`.

Результаты будут представлены по-разному, в зависимости от того, какие функции будут использованы. Помочь в интерпретации и визуализации полученных результатов многомерного анализа, например, кластерного анализа или результатов снижения размерности, нам как раз и поможет библиотека `factoextra`.


В рамках данного занятия мы будем использовать библиотеку `FactoMineR` [@le2008factominer] для расчетов, а библиотеку `factoextra` для извлечения и анализа результатов.

С какими методами может работать библиотека `factoextra`:

![](https://cran.r-project.org/web/packages/factoextra/readme/tools/factoextra-r-package.png)

### Как сделать правильный выбор?

Выбор метода для анализа будет зависеть от формата данных и их структуры:

Так, например, если у нас количественные данные, то нам подойдет метод главных компонент, а если качественные, то в зависимости от их количества - анализ соответствий - простой или множественный.

В случае, если данные разной природы, то нам лучше обратиться к смешанным методам - иерархическому множественному факторному анализу или факторному анализу для смешанных данных.
.
![](https://cran.r-project.org/web/packages/factoextra/readme/tools/multivariate-analysis-factoextra.png)

### Установка и загрузка библиотек

Устанавливаем библиотеки:

```{r, eval=FALSE, message=FALSE, warning=FALSE}
install.packages("factoextra")
install.packages("FactoMineR")
```

Загружаем:

```{r}
library("factoextra")
library("FactoMineR")
```


## Метод главных компонент (Principal component analysis)
Метод главных компонент (PCA) позволяет суммировать и визуализировать информацию, представленную в наборе данных, где некоторые индивиды / наблюдения описываются через большое количество взаимосвязанных друг с другом количественных переменных. Каждая переменная может быть представлена в качестве отдельного измерения.
Если переменных больше трех, то визуализация результатов становится затруднительным.

Метод главных компонент извлекает информацию из многомерной таблицы данных $n \times k$ 
и представляет ее в виде новых переменных, чье количество меньше количества исходных переменных.

Это новые переменные и называются **главные компоненты**, они являются линейной комбинацией исходных переменных. Количество главных компонент всегда меньше или равно количеству исходных переменных.

Информация набора данных соответствует дисперсии переменных, которые в него входят. 

Цель МГК заключается в идентификации главных компонент, описывающих максимум дисперсии переменных. Снижение размерности заключается в определении ведущих направлений (векторов), или главных компонент, показывающих изменение данных. 

Метод МГК предполагает, что эти главные направления описывают большую часть дисперсии, поэтому являются более важными.

:::{#fig-proj}

![](https://i.stack.imgur.com/Q7HIP.gif)
Геометрический смысл нахождения главной компоненты. 
:::

Другими словами, МГК сокращает размерность данных, путем выделения двух или трех самых важных компонент, которые мы может визуализировать графически и минимальной потерей информации.

Таким образом, главными задачами метода главных компонент являются:

- выявление скрытой структуры в наборе данных,
- снизить размерность данных, путем удаления статистического шума и избыточных переменных
- идентифицировать взаимосвязанные переменные


На практике этот метод реализуется двумя возможными подходами:

- путем разложения исходной матрицы взаимосвязей (ковариаций или корреляций) на произведение матриц собственных значений и собственных векторов
- путем сингулярного разложения данной матрицы.

### Собственные значения и собственные вектора

Допустим наши данные представлены в матрице $\mathbf X$ размера $n \times p$, где $n$ – количество наблюдений, а $p$ – количество признаков. Наши данные должны быть центрированы, то есть в каждом столбце из каждого элемента должно быть вычтено среднее значение, так чтобы среднее значение по каждому признаку равнялось нулю.

Тогда $p \times p$ матрица ковариаций $\mathbf C$ может быть представлена как:

$$\mathbf C = \mathbf X^\top \mathbf X/(n-1)$$


Поскольку эта матрица симметричная, она может быть диагонализирована, то есть представлена в виде произведения трех матриц, в результате чего находится соответствующая диагональной матрицы собственных значений:

$$\mathbf C = \mathbf V \mathbf L \mathbf V^\top,$$

где $\mathbf V$ - это матрица собственных векторов,а $\mathbf L$ - это диагональная матрица, на диагонали которой находятся собственные значения $λ_i$, упорядоченные по убыванию:

$$\mathbf L = \begin{bmatrix}
\lambda_1 &  & & \\ 
 & \lambda_2 & & \\ 
 & & \lambda_{...} & \\
 & & & \lambda_p  \\
\end{bmatrix}$$


Собственные вектора называются **главными осями** или **главными направлениями**. 

Проекции данных на главные оси собственно и называются **главными компонентами**, представляющими собой новые, трансформированные переменные. 

Главная компонента $j$ находится в $j$ столбце матрицы $\mathbf {XV}$. 

Координаты  $i-ой$ точки данных в новом пространстве главных компонент заданы $i-й$ строкой
$\mathbf XV$.

Для нахождения собственных значений решается характеристическое уравнение:

$$|\mathbf C-\mathbf \Lambda \mathbf I|\vec{x}=0,$$

предполагающее вычисление детерминанта $|\mathbf C-\mathbf \Lambda \mathbf I|$ и корней уравнения степени $n$, в результате которого находятся $\lambda_1, \lambda_2, ... \lambda_p$.

Далее, путем подстановки собственных значений в исходное уравнение, для каждого $\lambda$ находится собственный вектор. 

Фундаментальным для метода главных компонент является уравнение:

$$\mathbf C=\mathbf A \mathbf {A}',$$

где $\mathbf A$ – матрица нагрузок, а $\mathbf {A}'$ - транспонированная ей матрица.

Нагрузки получаются путем умножения собственных векторов на квадрат из собственных значений:

$$\mathbf A= \mathbf V \sqrt{\mathbf \Lambda}$$

Таким образом, главную компоненту можно представить как линейную комбинацию исходных данных и компонентных нагрузок. 

Пример с первой компонентой:

$$PC_k = a_{k1}X_1 + a_{k2}X_2 + … + a_{kp}X_p,$$

где a$_ij$ является нагрузкой переменной $x_j$ по компоненте $PC_i$,  $x_j$ – $j-ая$ переменная матрицы признаков $\mathbf X$.

### Сингулярное разложение

Когда мы осуществляем сингулярное разложение матрицы взаимосвязей, у нас получается несколько иная картина:

$$\mathbf X = \mathbf U \mathbf S \mathbf V^\top,$$

где $\mathbf U$ - являются унитарной матрицей (столбцы которой называются левыми сингулярными векторами), $\mathbf S$ - диагональная матрица сингулярных значений $s_i$, а столбцы матрицы $\mathbf V$ называются правыми сингулярными векторами.

Правые сингулярные вектора – это и есть собственные вектора $A^\top A$, то есть между методами есть тесная взаимосвязь, и $\lambda_i = s_i^2/(n-1)$.


> {{< iconify arcticons anz size=42px >}} **Пример**: Рассмотрим пример с данными соревнований по десятиборью, в которые входят следующие дисциплины:

- "X100m" - юез на 100 метров
- "Long.jump"     - прыжки в длину
- "Shot.put" - толкание ядра
- "High.jump" - прыжки в высоту
- "X400m" - бег на 400 метров
- "X110m.hurdle" - бег с препятсвиями
- "Discus" - метание диска
- "Pole.vault" - прыжок с шестом
- "Javeline" - метание копья
- "X1500m"  - без на 1500 метров

![](https://sportsmatik.com/uploads/matik-sports-corner/matik-know-how/decathlon_1499492047_83444.jpg)

```{r}
data(decathlon2)
head(decathlon2)
```

![](images/PCA/pic3.png)

Итак, наши данные описывают выполнение атлетами испытаний в двух типах спортивных соревнований (Desctar и OlympicG). Набор содержит информацию о 27 спортсменах и 13 переменных.

**Активные спортсмены**  (голубой цвел, строки 1:23) : Данные, которые будут использованы для проведения анализа методом главных компонент.
**Дополнительные спортсмены** (синий цвет, строки 24:27) : Координаты по этим спортсменам будут использованы для предсказания параметров с помощью МГК по информации полученной по активным спортсменам / переменным
**Активные переменные** (розовый цвет, столбцы 1:10) : переменные, используемые в МГК
**Дополнительные переменные**: используются для предсказания, включая:

**Дополнительные количественные переменные** (красные): Столбцы 11 и 12 соответствуют рангу и баллам.

**Дополнительные качественные переменные** (зеленые): данные по соревнованиям (категориальная переменная).

Начнем с разделения наших данных на активные и дополнительные части:

```{r}
decathlon2.active <- decathlon2[1:23, 1:10]
head(decathlon2.active[, 1:6], 4)
```


### Стандартизация данных

Когда проводится анализ главных компонент, переменные часто стандартизируются. Эта процедура особенно рекомендуется тогда, когда переменные измеряются в разных единицах (килограммы, сантиметры и пр.). 

Главная цель стандартизации - сделать переменные сопоставимыми. Обычно стандартизация происходит таким образом, чтобы переменная имела 1) стандартное отклонение равным 1, и 2) среднее значение 0.


Типичная формула для стандартизации:

$$\frac{x_i−mean(x)}{sd(x)}$$

Где $mean(x)$
 это среднее значение, а $sd(x)$
 стандартное отклонение (SD).

Вы уже знакомы с функцией `scale()`, которая может быть использована для стандартизации.

Однако, стандартизация может быть осуществлена сразу в процессе анализа (это действие по умолчанию).

Проведем анализ главных компонент на активных данных по декатлону:

```{r}
res.pca <- PCA(decathlon2.active, graph = FALSE)
```

На выходе функции мы получаем следующие результаты:

```{r}
print(res.pca)
```

### Визуализация и интерпретация 

Мы будем использовать библиотеку `factoextra`, чтобы разобраться с результатами анализа.

Полезные функции:

- get_eigenvalue(res.pca): - извлекает собственные значения / дисперсию главных компонент 
- fviz_eig(res.pca): визуализация собственных значений
- get_pca_ind(res.pca), get_pca_var(res.pca) - извлекает результаты для наблюдений и переменных 
- fviz_pca_ind(res.pca), fviz_pca_var(res.pca) - визуализирует результаты по наблюдениям и переменным
- fviz_pca_biplot(res.pca) - создает двойной график (биплот) по индивидам и переменным



### Собственные значения / Дисперсия

Собственные значения показывают, какую долю дисперсии переменных представляет каждая компонента. Собственные значения больше для первых компонент и меньше - для последующих.

Собственные значения могут быть использованы для определения количества главных компонент, которые достойны рассмотрения. 

Для получения информации о собственных значниях и дисперсии можно использовать следующий код:

```{r}
eig.val <- get_eigenvalue(res.pca)
eig.val
```

Сумма всех собственных значений равняется 10, то есть общему количеству переменных.

Доля дисперсии, объясняемой каждым собственным значением, представлена во второй колонке.

Например, если 4.124 разделить на 10 получится 0.4124, или около 41.24% изменчивости, объясненной первой компонентой. 

Кумулятивный процент представлен в последнем столбце

Видим, что четыре первых компоненты объясняют 80% дисперсии.


Собственные значения могут быть использованы для определения количества компонент: (Kaiser 1961):

- По правилу Кайзера собственное значение > 1 обозначает, что эта главная компонента описывает дисперсию хотя бы одной переменной.


Мы можем также ограничить число компонент теми, которые описывают какую-то определенную долю дисперсии (например, 70%).

Единого правила не существует!

В нашем анализе первые три компоненты объясняют 72% дисперсии, это достаточно приемлемый результат.

Альтернативный метод заключается в рассмотрении диаграммы рассеяния собственных значений, упорядоченных по убыванию (правило локтя или каменистой осыпи). 

Отбирается количество компонент выше точки, фиксирующей спад собственных значений, которые становятся очень близки друг другу (Jollife 2002, Peres-Neto, Jackson, and Somers (2005)).

Попробуем сделать такой график:

```{r}
fviz_eig(res.pca, addlabels = TRUE, ylim = c(0, 50))
```

### Результаты. Работаем с переменными

Самый простой способ извлечь информацию о переменных, это воспользоваться функцией `get_pca_var()`.

Эта функция предоставляет список матриц, содержащих результаты для активных переменных (координаты, корреляцию между переменными и осями, квадрат косинуса и вклады).

```{r}
var <- get_pca_var(res.pca)
var
```

Рассмотрим эти параметры подробнее

- var$coord: координаты, используемые для создания графика (проекции, нагрузки)
- var$cos2: квадрат косинуса - качество представленности переменных в факторном пространстве Рассчитывается как квадрат координат: var.cos2 = var.coord * var.coord.
- var$contrib: вклыды, показывает вклад переменной в главную компоненту (в процентах): contains the contributions (in percentage) of the variables to the principal components.  (var.cos2 * 100) / (total cos2 of the component). Чем важнее переменная для этой компоненты, тем выше у нее вклад.

Мы можем создать графики, основываясь либо на:
1) их качестве (cos2) или
2) на их вкладах в главные компоненты 

Представим результаты:

```{r, eval=FALSE}
# Координаты
head(var$coord)
# Cos2: качество анализа
head(var$cos2)
# Вклады в компоненты
head(var$contrib)

```

Рассмотрим, как визуализировать результаты анализа по отдельным наблюдениям и переменным и как сделать выводы о взаимосвязах между ними. 


### Корреляционный круг

Корерляция между переменной и главной компонентой используется в качестве координаты переменной на графике. 

Представление переменных зависит от графика наблюдений6 наблюдения представлены своими проекциями, тогда как переменные - корреляциями (Abdi, Williams 2010).

```{r}
# Координаты переменных
head(var$coord, 4)
```


Представим переменные в виде графика:

```{r}
fviz_pca_var(res.pca, col.var = "black")
```


Такой график показывает взаимосвязи между всеми переменными. Он может быть интерпретирован следующим образом:

Переменные с положительной корреляцией сгруппированы вместе.
Переменные, имеющие отрицательную корреляцию, находятся на разных сторонах графика (квадрантах).
Расстояние между переменными и началом координат показывает качество переменных в факторном пространстве.

Переменные, расположенные как можно дальше от начала координат, представлены лучше.

### Качество представленности переменных 

Качество представленности переменных в факторном пространстве называется `cos2` (квадрат косинуса, квадрат координаты). 

Их можно «добыть» следующим образом:

```{r}
head(var$cos2, 4)
```

Мы можем визуализировать этот показатель с помощью очень интересного графика из библиотеки `corrplot` (не забываем устанавливать):

```{r}
library("corrplot")
corrplot(var$cos2, is.corr=FALSE)
```


Можно сделать и столбчатую диаграмму (по первым двум компонентам:

```{r}
fviz_cos2(res.pca, choice = "var", axes = 1:2)
```

Отметим, что:
- высокие значения cos2 обозначают, что переменная хорошо описывается главной компонентой. В этом случае переменная располагается ближе к окружности на графике.
- низкие значения cos2 обозначают, что переменные на очень хорошо представлены полученной компонентной структурой. В этом случае переменная - ближе к центру.

Сумма всех квадратов косинуса по всем компонентам равна 1 - то есть 100% дисперсии.


Сделаем цветной график, так, чтобы:
- переменные с низкими значениями cos2 будут окрашены в голубой цвет
- переменные со средними значениями cos2 будут иметь оранжевый цвет
- переменные с высокими значениями cos2 будут иметь красный цвет

```{r}
fviz_pca_var(res.pca, col.var = "cos2",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), 
             repel = TRUE # Avoid text overlapping
             )
```

  
  Можно представить не с помощью цвета, а с помощью прозрачности:

```{r}
fviz_pca_var(res.pca, alpha.var = "cos2")
```

### Оценка вклада переменной в компоненту

Вклад переменной оценку дисперсии данной главной компоненты представлен в процентном соотношении.

Переменные, которые коррелируют и с первой компонентой PC1  (Dim.1) и со второй компонентой (PC2, Dim.2) являются самыми главными в описании измечивости данных в наборе.

Переменные, которые не коррелируют ни с одной компонентой или коррелируют с последними компонентами не являются важными и могут быть исключены из анализа.

Посмотрим вклад переменных:

```{r}
head(var$contrib, 4)
```

Чем выше вклад, тем больше переменная "вкладывает" в компоненту.


Воспользуемся функцией `corrplot`, чтобы это визуализировать:

```{r}
library("corrplot")
corrplot(var$contrib, is.corr=FALSE)
```
    

Функция  `fviz_contrib()` может быть использована для того, чтобы построить столбчатую диаграмму по данному показателю:

```{r}
# Вклад переменных в PC1
fviz_contrib(res.pca, choice = "var", axes = 1, top = 10)
# Вклад переменных в PC2
fviz_contrib(res.pca, choice = "var", axes = 2, top = 10)
```


Общий вклад переменных в обе компоненты:

```{r}
fviz_contrib(res.pca, choice = "var", axes = 1:2, top = 10)
```

Красная линия показывает некоторый средний вклад. Если бы вклад всех переменных был бы одинаковый, то каждая вносила бы по  1/10 = 10%. Соответственно, переменная, превышающая данный уровень, может считаться более важной, а менее - незначимой.


Самые важные переменные в нашем анализе - X100m, Long.jump (прыжок в длину) и Pole.vault (прыжок с шестом).

Вклад переменных может быть визуализирован следующим образом:

```{r}
fviz_pca_var(res.pca, col.var = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07")
             )
```


### Анализ по группам

Мы можем также провести анализ по отдельным группам и использовать данные о группировке в визулизации.

Так как у нас нет группирующей переменной, давайте ее создадим.

Мы разобьем переменные на три кластера, используя метод `kmeans`, и затем информацию о кластере включим в график в качестве группирующей переменной.

``` {r}
# Сначала - кластерный анализ, создаем 3 кластера на основе координат

set.seed(123)
res.km <- kmeans(var$coord, centers = 3, nstart = 25)
grp <- as.factor(res.km$cluster)

# Распределяем переменные по кластерам

fviz_pca_var(res.pca, col.var = grp, 
             palette = c("#0073C2FF", "#EFC000FF", "#868686FF"),
             legend.title = "Cluster")
```

### Описание отдельных измерений

В предыдущем разделе мы проанализировали переменные с позиции их вклада в главные компоненты.

Возникает вопросы, что же эти компоненты из себя представляют.

Функция`dimdesc()` может быть использована для анализа отдельных измерений:

```{r}
res.desc <- dimdesc(res.pca, axes = c(1,2), proba = 0.05)
# Описание измерения 1
res.desc$Dim.1
```

В результатах выше, $quanti означает результаты по количественным переменным, переменные отсортированы по значению p-value.

### Анализ отдельных наблюдений

Результаты по отдельным наблюдениям могут быть получены с помощью аналогичной функции  `get_pca_ind()`. Так же, как и `get_pca_var()`, функция `get_pca_ind()` позволяет получить список матриц с результатами по наблюдениям (координаты, корреляции с осями, квадрат косинуса и вклады)

```{r}
ind <- get_pca_ind(res.pca)
ind
```
Посмотрим результаты по отдельным показателям

```{r}
# Координаты наблюдений
head(ind$coord)
# Качество
head(ind$cos2)
# Вклады
head(ind$contrib)
```


### Графики качества и вкладов

Функция `fviz_pca_ind()` помогает получить график по наблюдениям (спортсменам):

```{r}
fviz_pca_ind(res.pca)
```

Как и в случае с переменными, мы можем раскрасить наблюдения в зависимости от значений cos2:

```{r}
fviz_pca_ind(res.pca, col.ind = "cos2", 
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE # Avoid text overlapping (slow if many points)
             )
```

Отметим, что наблюдения с похожими значениями на графике располагаются близко друг к другу.


Столбчатый график по наблюдениям:

```{r}

fviz_cos2(res.pca, choice = "ind")
```

Визуализация вклада спортсменов в первые две компоненты:

```{r}
fviz_contrib(res.pca, choice = "ind", axes = 1:2)
```



### Анализ по группам

Чтобы показать разбивку по группам, давайте воспользуемся известным нам набором по ирисам.

Переменная вида “Species” будет использована в качестве группирующей переменной:

```{r}
iris.pca <- PCA(iris[,-5], graph = FALSE)
```

Создаем график:

```{r}
fviz_pca_ind(iris.pca,
             geom.ind = "point", # показываем только точки, не метки
             col.ind = iris$Species, # группирующая переменная
             palette = c("#00AFBB", "#E7B800", "#FC4E07"),
             addEllipses = TRUE, # эллипсы концентрации
             legend.title = "Группы"
             )
```

### Создаем биплот

Это, конечно, самый интересный момент. Чтобы сделать простой биплот по переменным и наблюдениям, можно воспользоваться следующим кодом:

```{r}
fviz_pca_biplot(res.pca, repel = TRUE,
                col.var = "#2E9FDF", # Цвет переменных
                col.ind = "#696969"  # Цвет наблюдений
                )
```


Отметим, что такой график целесообразен, если у нас мало переменных и наблюдений, в противном случае он может быть нечитаемым.

Отметим также, что координаты переменных и наблюдений рассчитываются не в одном пространстве. Иными словами, в биплоте для нас больше важны направления переменных, чем абсолютные значения расстояний на графике.

Грубо говоря, смысл биплота заключается в следующем:

- наблюдение, которое находится с той же стороны, что и переменная, имеет большие значения именно по данной переменной;
- наблюдение, которое находится с другой стороны от переменной, имеет по ней маленькие значения.

Сделаем график по ирисам:

```{r}
fviz_pca_biplot(iris.pca, 
                col.ind = iris$Species, palette = "jco", 
                addEllipses = TRUE, label = "var",
                col.var = "black", repel = TRUE,
                legend.title = "Species") 
                
```

В следующем графике мы хотим раскрасить как наблюдения, так и переменные по группам. 

Чтобы разобраться с цветами, мы воспользуемся вспомогательными функциями  `fill_palette()` и `color_palette()` [ggpubr].

```{r}
fviz_pca_biplot(iris.pca, 
                # Наблюдения по группам
                geom.ind = "point",
                pointshape = 21,
                pointsize = 2.5,
                fill.ind = iris$Species,
                col.ind = "black",
                # Цвет переменных по группам
                col.var = factor(c("sepal", "sepal", "petal", "petal")),
                
                legend.title = list(fill = "Species", color = "Clusters"),
                repel = TRUE        # чтобы не было пересечений
             )+
  ggpubr::fill_palette("jco")+      # цвет для наблюдений
  ggpubr::color_palette("npg")      # цвет для переменных
```

В следующем графике наблюдения раскрашены по группам, а переменные - по вкладу в главные компоненты (градиентная заливка).

```{r}
fviz_pca_biplot(iris.pca, 
                # Individuals
                geom.ind = "point",
                fill.ind = iris$Species, col.ind = "black",
                pointshape = 21, pointsize = 2,
                palette = "jco",
                addEllipses = TRUE,
                # Variables
                alpha.var ="contrib", col.var = "contrib",
                gradient.cols = "RdYlBu",
                
                legend.title = list(fill = "Species", color = "Contrib",
                                    alpha = "Contrib")
                )
```

### Дополнительные элементы

Помните, мы разделили наш набор на две части - активные и дополнительные переменные?

У нас есть две количественные дополнительные переменные (`quanti.sup`, колонки 11:12), и одна дополнительная качественная переменная (`quali.sup`, колонка 13) и четыре дополнительных спортсмена (`ind.sup`, строки 24:27).

Дополнительные элементы не используются в основном анализе, а их координаты предсказываются исходя из информации, полученной в результате анализа, полученного на активных данных.

Как такой анализ может быть проведен?

```{r}
res.pca <- PCA(decathlon2, ind.sup = 24:27, 
               quanti.sup = 11:12, quali.sup = 13, graph=FALSE)

```

### Количественные переменные

Результаты предсказания (координаты, корреляции и квадрат косинуса) для дополнительных переменных:

```{r}
res.pca$quanti.sup
```

Визуализация переменных (и активных, и дополнительных)

```{r}
fviz_pca_var(res.pca)
```

### Данные по наблюдениям

Предсказанные результаты по наблюдениям:

```{r}
res.pca$ind.sup
```

Визуализация наблюдений (активных и дополнительных). Мы можем представить также качественные переменные (`quali.sup`), а их координаты можно извлечь через `res.pca$quali.supp$coord`.


```{r}
p <- fviz_pca_ind(res.pca, col.ind.sup = "blue", repel = TRUE)
p <- fviz_add(p, res.pca$quali.sup$coord, color = "red")
p
```


### Данные по качественным переменным

Дополнительные качественные переменные могут также быть использованы для группировки. Это может помочь с интерпретацией данных. 

Результаты по качественным переменным:

```{r}
res.pca$quali
```

Чтобы использовать эту переменную в качестве группирующей, нужно использовать аргумент `habillage` (по-французски это "одевание"), в который вносится индекс этой переменной. 

```{r}
fviz_pca_ind(res.pca, habillage = 13,
             addEllipses =TRUE, ellipse.type = "confidence",
             palette = "jco", repel = TRUE) 


```

### Фильтр результатов

Если у нас много наблюдений или переменных, то можно визуализировать только некоторые из них через аргументы `select.ind` и `select.var`.

Можно сделать сортировку по имени, по косинусу или вкладу.

Примеры:
```{r}
# Отбор переменных с cos2 >= 0.6
fviz_pca_var(res.pca, select.var = list(cos2 = 0.6))
# Top 5 активных переменных с самым высоким cos2
fviz_pca_var(res.pca, select.var= list(cos2 = 5))
# Отбор по именам
name <- list(name = c("Long.jump", "High.jump", "X100m"))
fviz_pca_var(res.pca, select.var = name)
# top 5 самых важных наблюдений и переменных
fviz_pca_biplot(res.pca, select.ind = list(contrib = 5), 
               select.var = list(contrib = 5),
               ggtheme = theme_minimal())

```

### Самостоятельная работа

1. Обратиться к набору данных психосемантического исследования по религии (см. файл в приложении)
2. Сделать анализ главных компонент. Вывести результаты по собственным значениям и дисперсии (вместе с графиками), по координатам (корреляциям) и вкладам - по дескрипторам и ролям
3. Сделать визуализацию - отдельно по дескрипторам, по ролям и биплот.

### Источники

::: {#refs}
:::
